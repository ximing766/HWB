{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sc_special\n",
    "import tensorflow as tf \n",
    "from \ttensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras import optimizers,metrics,layers,Sequential\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time1</th>\n",
       "      <th>Time2</th>\n",
       "      <th>localation</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>w_speed</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 0:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.8890</td>\n",
       "      <td>304.016</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>66.7409</td>\n",
       "      <td>4.16382</td>\n",
       "      <td>162.577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94818</td>\n",
       "      <td>428.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.401510</td>\n",
       "      <td>20.9208</td>\n",
       "      <td>8.17336</td>\n",
       "      <td>5.27729</td>\n",
       "      <td>8.78723</td>\n",
       "      <td>0.124491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 1:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.8736</td>\n",
       "      <td>303.739</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>62.1551</td>\n",
       "      <td>4.65267</td>\n",
       "      <td>171.978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.14987</td>\n",
       "      <td>427.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.448340</td>\n",
       "      <td>14.8144</td>\n",
       "      <td>6.49054</td>\n",
       "      <td>4.33106</td>\n",
       "      <td>12.74530</td>\n",
       "      <td>0.109056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 2:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.6471</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>64.1760</td>\n",
       "      <td>4.10031</td>\n",
       "      <td>172.013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.01616</td>\n",
       "      <td>427.428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.271610</td>\n",
       "      <td>13.9154</td>\n",
       "      <td>6.86679</td>\n",
       "      <td>4.40045</td>\n",
       "      <td>12.22960</td>\n",
       "      <td>0.105957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 3:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.4555</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>68.7958</td>\n",
       "      <td>2.44317</td>\n",
       "      <td>168.135</td>\n",
       "      <td>0.047224</td>\n",
       "      <td>...</td>\n",
       "      <td>1.89003</td>\n",
       "      <td>442.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467429</td>\n",
       "      <td>11.1535</td>\n",
       "      <td>5.25900</td>\n",
       "      <td>3.35261</td>\n",
       "      <td>13.78000</td>\n",
       "      <td>0.101764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 4:00</td>\n",
       "      <td>A</td>\n",
       "      <td>28.5189</td>\n",
       "      <td>302.987</td>\n",
       "      <td>0.019881</td>\n",
       "      <td>76.5791</td>\n",
       "      <td>2.57759</td>\n",
       "      <td>207.884</td>\n",
       "      <td>8.260020</td>\n",
       "      <td>...</td>\n",
       "      <td>6.53753</td>\n",
       "      <td>458.394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574856</td>\n",
       "      <td>13.9989</td>\n",
       "      <td>6.05979</td>\n",
       "      <td>3.59303</td>\n",
       "      <td>9.96333</td>\n",
       "      <td>0.104536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time1           Time2 localation  temp_2m   temp_0        BS     humi  \\\n",
       "0  2020/7/23  2020/7/23 0:00          A  29.8890  304.016  0.018871  66.7409   \n",
       "1  2020/7/23  2020/7/23 1:00          A  29.8736  303.739  0.017556  62.1551   \n",
       "2  2020/7/23  2020/7/23 2:00          A  29.6471  303.419  0.017874  64.1760   \n",
       "3  2020/7/23  2020/7/23 3:00          A  29.4555  303.419  0.018935  68.7958   \n",
       "4  2020/7/23  2020/7/23 4:00          A  28.5189  302.987  0.019881  76.5791   \n",
       "\n",
       "   w_speed    w_dir      rain  ...     QRTL     CBFS  DBFS  sunFS       SO2  \\\n",
       "0  4.16382  162.577  0.000000  ...  0.94818  428.278   0.0    0.0  2.401510   \n",
       "1  4.65267  171.978  0.000000  ...  1.14987  427.531   0.0    0.0  1.448340   \n",
       "2  4.10031  172.013  0.000000  ...  1.01616  427.428   0.0    0.0  1.271610   \n",
       "3  2.44317  168.135  0.047224  ...  1.89003  442.472   0.0    0.0  0.467429   \n",
       "4  2.57759  207.884  8.260020  ...  6.53753  458.394   0.0    0.0  0.574856   \n",
       "\n",
       "       NO2     PM10    PM2.5        O3        CO  \n",
       "0  20.9208  8.17336  5.27729   8.78723  0.124491  \n",
       "1  14.8144  6.49054  4.33106  12.74530  0.109056  \n",
       "2  13.9154  6.86679  4.40045  12.22960  0.105957  \n",
       "3  11.1535  5.25900  3.35261  13.78000  0.101764  \n",
       "4  13.9989  6.05979  3.59303   9.96333  0.104536  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('A-A1-A2-A3.csv',sep=',',header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data)):\n",
    "    if data['Time2'][i].split('/')[2].split(' ')[0] != data['Time1'][i].split('/')[2]:\n",
    "        data.drop(index=[i],inplace = True)\n",
    "data.to_csv('NewThree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33888"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO21</th>\n",
       "      <th>NO21</th>\n",
       "      <th>PM101</th>\n",
       "      <th>PM2.51</th>\n",
       "      <th>O31</th>\n",
       "      <th>CO1</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>...</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.8890</td>\n",
       "      <td>304.016</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>66.7409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94818</td>\n",
       "      <td>428.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.401510</td>\n",
       "      <td>20.9208</td>\n",
       "      <td>8.17336</td>\n",
       "      <td>5.27729</td>\n",
       "      <td>8.78723</td>\n",
       "      <td>0.124491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.8736</td>\n",
       "      <td>303.739</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>62.1551</td>\n",
       "      <td>...</td>\n",
       "      <td>1.14987</td>\n",
       "      <td>427.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.448340</td>\n",
       "      <td>14.8144</td>\n",
       "      <td>6.49054</td>\n",
       "      <td>4.33106</td>\n",
       "      <td>12.74530</td>\n",
       "      <td>0.109056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.6471</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>64.1760</td>\n",
       "      <td>...</td>\n",
       "      <td>1.01616</td>\n",
       "      <td>427.428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.271610</td>\n",
       "      <td>13.9154</td>\n",
       "      <td>6.86679</td>\n",
       "      <td>4.40045</td>\n",
       "      <td>12.22960</td>\n",
       "      <td>0.105957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.4555</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>68.7958</td>\n",
       "      <td>...</td>\n",
       "      <td>1.89003</td>\n",
       "      <td>442.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467429</td>\n",
       "      <td>11.1535</td>\n",
       "      <td>5.25900</td>\n",
       "      <td>3.35261</td>\n",
       "      <td>13.78000</td>\n",
       "      <td>0.101764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.7843</td>\n",
       "      <td>302.383</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>72.2798</td>\n",
       "      <td>...</td>\n",
       "      <td>3.81680</td>\n",
       "      <td>429.953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.764980</td>\n",
       "      <td>21.7531</td>\n",
       "      <td>13.43040</td>\n",
       "      <td>7.64338</td>\n",
       "      <td>3.38709</td>\n",
       "      <td>0.144370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SO21  NO21  PM101  PM2.51  O31  CO1  temp_2m   temp_0        BS     humi  \\\n",
       "0     4    12      3       3   23  0.4  29.8890  304.016  0.018871  66.7409   \n",
       "1     3    11      3       1   24  0.4  29.8736  303.739  0.017556  62.1551   \n",
       "2     4    13     14       2   20  0.4  29.6471  303.419  0.017874  64.1760   \n",
       "3     4    12     12       1   20  0.4  29.4555  303.419  0.018935  68.7958   \n",
       "4     5    19      6       5   11  0.5  28.7843  302.383  0.019065  72.2798   \n",
       "\n",
       "   ...     QRTL     CBFS  DBFS  sunFS       SO2      NO2      PM10    PM2.5  \\\n",
       "0  ...  0.94818  428.278   0.0    0.0  2.401510  20.9208   8.17336  5.27729   \n",
       "1  ...  1.14987  427.531   0.0    0.0  1.448340  14.8144   6.49054  4.33106   \n",
       "2  ...  1.01616  427.428   0.0    0.0  1.271610  13.9154   6.86679  4.40045   \n",
       "3  ...  1.89003  442.472   0.0    0.0  0.467429  11.1535   5.25900  3.35261   \n",
       "4  ...  3.81680  429.953   0.0    0.0  1.764980  21.7531  13.43040  7.64338   \n",
       "\n",
       "         O3        CO  \n",
       "0   8.78723  0.124491  \n",
       "1  12.74530  0.109056  \n",
       "2  12.22960  0.105957  \n",
       "3  13.78000  0.101764  \n",
       "4   3.38709  0.144370  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('four.csv',sep=',',header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4. , 12. ,  3. ,  3. , 23. ,  0.4],\n",
       "       [ 3. , 11. ,  3. ,  1. , 24. ,  0.4],\n",
       "       [ 4. , 13. , 14. ,  2. , 20. ,  0.4],\n",
       "       ...,\n",
       "       [ 7. , 16. ,  9. ,  2. ,  9. ,  0.4],\n",
       "       [ 4. , 16. ,  7. ,  2. ,  9. ,  0.4],\n",
       "       [ 4. ,  7. , 14. ,  5. , 23. ,  0.4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollution1 = np.array(data.iloc[:,0:6])\n",
    "pollution2 = np.array(data.iloc[:,-6:])\n",
    "pollution1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存污染物相减后的数据，在将之前处理的预测环境数据手动添加进去\n",
    "pol = pd.DataFrame(pollution1-pollution2,columns = ['SO2','NO2','PM10','PM2.5','O3','CO'])\n",
    "pol.head()\n",
    "pol.to_csv('DataFour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入处理好的数据，若已处理好代码从这里开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>...</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloud</th>\n",
       "      <th>high</th>\n",
       "      <th>press</th>\n",
       "      <th>GRTL</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.598490</td>\n",
       "      <td>-8.9208</td>\n",
       "      <td>-5.17336</td>\n",
       "      <td>-2.27729</td>\n",
       "      <td>14.21277</td>\n",
       "      <td>0.275509</td>\n",
       "      <td>29.8890</td>\n",
       "      <td>304.016</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>66.7409</td>\n",
       "      <td>...</td>\n",
       "      <td>162.577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038201</td>\n",
       "      <td>769.903</td>\n",
       "      <td>100.665</td>\n",
       "      <td>20.13770</td>\n",
       "      <td>0.94818</td>\n",
       "      <td>428.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.551660</td>\n",
       "      <td>-3.8144</td>\n",
       "      <td>-3.49054</td>\n",
       "      <td>-3.33106</td>\n",
       "      <td>11.25470</td>\n",
       "      <td>0.290944</td>\n",
       "      <td>29.8736</td>\n",
       "      <td>303.739</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>62.1551</td>\n",
       "      <td>...</td>\n",
       "      <td>171.978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>682.085</td>\n",
       "      <td>100.671</td>\n",
       "      <td>15.83330</td>\n",
       "      <td>1.14987</td>\n",
       "      <td>427.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.728390</td>\n",
       "      <td>-0.9154</td>\n",
       "      <td>7.13321</td>\n",
       "      <td>-2.40045</td>\n",
       "      <td>7.77040</td>\n",
       "      <td>0.294043</td>\n",
       "      <td>29.6471</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>64.1760</td>\n",
       "      <td>...</td>\n",
       "      <td>172.013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065039</td>\n",
       "      <td>627.078</td>\n",
       "      <td>100.678</td>\n",
       "      <td>12.33630</td>\n",
       "      <td>1.01616</td>\n",
       "      <td>427.428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.532571</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>6.74100</td>\n",
       "      <td>-2.35261</td>\n",
       "      <td>6.22000</td>\n",
       "      <td>0.298236</td>\n",
       "      <td>29.4555</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>68.7958</td>\n",
       "      <td>...</td>\n",
       "      <td>168.135</td>\n",
       "      <td>0.047224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>636.497</td>\n",
       "      <td>100.675</td>\n",
       "      <td>11.12150</td>\n",
       "      <td>1.89003</td>\n",
       "      <td>442.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.235020</td>\n",
       "      <td>-2.7531</td>\n",
       "      <td>-7.43040</td>\n",
       "      <td>-2.64338</td>\n",
       "      <td>7.61291</td>\n",
       "      <td>0.355630</td>\n",
       "      <td>28.7843</td>\n",
       "      <td>302.383</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>72.2798</td>\n",
       "      <td>...</td>\n",
       "      <td>196.949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378593</td>\n",
       "      <td>378.039</td>\n",
       "      <td>100.772</td>\n",
       "      <td>6.92296</td>\n",
       "      <td>3.81680</td>\n",
       "      <td>429.953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SO2     NO2     PM10    PM2.5        O3        CO  temp_2m   temp_0  \\\n",
       "0  1.598490 -8.9208 -5.17336 -2.27729  14.21277  0.275509  29.8890  304.016   \n",
       "1  1.551660 -3.8144 -3.49054 -3.33106  11.25470  0.290944  29.8736  303.739   \n",
       "2  2.728390 -0.9154  7.13321 -2.40045   7.77040  0.294043  29.6471  303.419   \n",
       "3  3.532571  0.8465  6.74100 -2.35261   6.22000  0.298236  29.4555  303.419   \n",
       "4  3.235020 -2.7531 -7.43040 -2.64338   7.61291  0.355630  28.7843  302.383   \n",
       "\n",
       "         BS     humi  ...    w_dir      rain     cloud     high    press  \\\n",
       "0  0.018871  66.7409  ...  162.577  0.000000  0.038201  769.903  100.665   \n",
       "1  0.017556  62.1551  ...  171.978  0.000000  0.058892  682.085  100.671   \n",
       "2  0.017874  64.1760  ...  172.013  0.000000  0.065039  627.078  100.678   \n",
       "3  0.018935  68.7958  ...  168.135  0.047224  1.000000  636.497  100.675   \n",
       "4  0.019065  72.2798  ...  196.949  0.000000  0.378593  378.039  100.772   \n",
       "\n",
       "       GRTL     QRTL     CBFS  DBFS  sunFS  \n",
       "0  20.13770  0.94818  428.278   0.0    0.0  \n",
       "1  15.83330  1.14987  427.531   0.0    0.0  \n",
       "2  12.33630  1.01616  427.428   0.0    0.0  \n",
       "3  11.12150  1.89003  442.472   0.0    0.0  \n",
       "4   6.92296  3.81680  429.953   0.0    0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据的各个Y是差值（实测值 - 一次预测值）\n",
    "data = pd.read_csv('DataFour.csv',sep=',',header=0)\n",
    "#data = data.sample(frac = 1) #打乱数据\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32498"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>...</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloud</th>\n",
       "      <th>high</th>\n",
       "      <th>press</th>\n",
       "      <th>GRTL</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.094233</td>\n",
       "      <td>-15.939938</td>\n",
       "      <td>17.465385</td>\n",
       "      <td>-2.504328</td>\n",
       "      <td>17.786499</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>24.916934</td>\n",
       "      <td>301.778926</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>59.221886</td>\n",
       "      <td>...</td>\n",
       "      <td>124.234901</td>\n",
       "      <td>0.399914</td>\n",
       "      <td>0.465743</td>\n",
       "      <td>676.290666</td>\n",
       "      <td>101.253019</td>\n",
       "      <td>92.886432</td>\n",
       "      <td>16.606632</td>\n",
       "      <td>395.988812</td>\n",
       "      <td>208.787297</td>\n",
       "      <td>250.884365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>7.479913</td>\n",
       "      <td>29.151128</td>\n",
       "      <td>37.532859</td>\n",
       "      <td>30.707256</td>\n",
       "      <td>35.559385</td>\n",
       "      <td>0.262027</td>\n",
       "      <td>5.980101</td>\n",
       "      <td>8.561029</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>16.636423</td>\n",
       "      <td>...</td>\n",
       "      <td>82.821606</td>\n",
       "      <td>2.035541</td>\n",
       "      <td>0.328914</td>\n",
       "      <td>454.716321</td>\n",
       "      <td>0.646240</td>\n",
       "      <td>107.334489</td>\n",
       "      <td>19.380946</td>\n",
       "      <td>50.285896</td>\n",
       "      <td>273.216779</td>\n",
       "      <td>328.304546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-103.061000</td>\n",
       "      <td>-429.778000</td>\n",
       "      <td>-148.909000</td>\n",
       "      <td>-124.171000</td>\n",
       "      <td>-264.976000</td>\n",
       "      <td>-2.033030</td>\n",
       "      <td>2.584230</td>\n",
       "      <td>275.045000</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>9.069630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.575400</td>\n",
       "      <td>99.713600</td>\n",
       "      <td>-23.560700</td>\n",
       "      <td>-1.390840</td>\n",
       "      <td>229.260000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-3.115165</td>\n",
       "      <td>-30.841725</td>\n",
       "      <td>1.845700</td>\n",
       "      <td>-12.536525</td>\n",
       "      <td>-0.000526</td>\n",
       "      <td>0.283547</td>\n",
       "      <td>21.241975</td>\n",
       "      <td>296.019000</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>47.172650</td>\n",
       "      <td>...</td>\n",
       "      <td>51.667050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152484</td>\n",
       "      <td>326.816250</td>\n",
       "      <td>100.698000</td>\n",
       "      <td>10.046575</td>\n",
       "      <td>1.089850</td>\n",
       "      <td>361.367000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.401255</td>\n",
       "      <td>-12.325500</td>\n",
       "      <td>15.626820</td>\n",
       "      <td>0.382690</td>\n",
       "      <td>13.973738</td>\n",
       "      <td>0.397692</td>\n",
       "      <td>25.933200</td>\n",
       "      <td>301.836500</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>59.219050</td>\n",
       "      <td>...</td>\n",
       "      <td>129.652000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454778</td>\n",
       "      <td>550.314000</td>\n",
       "      <td>101.252000</td>\n",
       "      <td>37.869700</td>\n",
       "      <td>6.393675</td>\n",
       "      <td>409.094000</td>\n",
       "      <td>24.071450</td>\n",
       "      <td>28.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>4.356280</td>\n",
       "      <td>1.052725</td>\n",
       "      <td>30.490700</td>\n",
       "      <td>8.474420</td>\n",
       "      <td>33.257918</td>\n",
       "      <td>0.547795</td>\n",
       "      <td>29.218950</td>\n",
       "      <td>307.314000</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>73.053650</td>\n",
       "      <td>...</td>\n",
       "      <td>180.807000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782031</td>\n",
       "      <td>973.546000</td>\n",
       "      <td>101.741000</td>\n",
       "      <td>166.994250</td>\n",
       "      <td>29.836850</td>\n",
       "      <td>436.387000</td>\n",
       "      <td>390.929500</td>\n",
       "      <td>469.751250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>58.261800</td>\n",
       "      <td>172.880700</td>\n",
       "      <td>947.092100</td>\n",
       "      <td>975.544690</td>\n",
       "      <td>269.230060</td>\n",
       "      <td>11.433869</td>\n",
       "      <td>37.532300</td>\n",
       "      <td>323.987000</td>\n",
       "      <td>0.023053</td>\n",
       "      <td>100.703000</td>\n",
       "      <td>...</td>\n",
       "      <td>359.997000</td>\n",
       "      <td>98.740700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2314.100000</td>\n",
       "      <td>102.997000</td>\n",
       "      <td>554.955000</td>\n",
       "      <td>77.052600</td>\n",
       "      <td>491.671000</td>\n",
       "      <td>893.002000</td>\n",
       "      <td>1073.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                SO2           NO2          PM10         PM2.5            O3  \\\n",
       "count  32498.000000  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean       0.094233    -15.939938     17.465385     -2.504328     17.786499   \n",
       "std        7.479913     29.151128     37.532859     30.707256     35.559385   \n",
       "min     -103.061000   -429.778000   -148.909000   -124.171000   -264.976000   \n",
       "25%       -3.115165    -30.841725      1.845700    -12.536525     -0.000526   \n",
       "50%        1.401255    -12.325500     15.626820      0.382690     13.973738   \n",
       "75%        4.356280      1.052725     30.490700      8.474420     33.257918   \n",
       "max       58.261800    172.880700    947.092100    975.544690    269.230060   \n",
       "\n",
       "                 CO       temp_2m        temp_0            BS          humi  \\\n",
       "count  32498.000000  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean       0.421860     24.916934    301.778926      0.013047     59.221886   \n",
       "std        0.262027      5.980101      8.561029      0.005260     16.636423   \n",
       "min       -2.033030      2.584230    275.045000      0.000647      9.069630   \n",
       "25%        0.283547     21.241975    296.019000      0.009172     47.172650   \n",
       "50%        0.397692     25.933200    301.836500      0.013393     59.219050   \n",
       "75%        0.547795     29.218950    307.314000      0.017922     73.053650   \n",
       "max       11.433869     37.532300    323.987000      0.023053    100.703000   \n",
       "\n",
       "       ...         w_dir          rain         cloud          high  \\\n",
       "count  ...  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean   ...    124.234901      0.399914      0.465743    676.290666   \n",
       "std    ...     82.821606      2.035541      0.328914    454.716321   \n",
       "min    ...      0.004486      0.000000      0.000000     19.575400   \n",
       "25%    ...     51.667050      0.000000      0.152484    326.816250   \n",
       "50%    ...    129.652000      0.000000      0.454778    550.314000   \n",
       "75%    ...    180.807000      0.000000      0.782031    973.546000   \n",
       "max    ...    359.997000     98.740700      1.000000   2314.100000   \n",
       "\n",
       "              press          GRTL          QRTL          CBFS          DBFS  \\\n",
       "count  32498.000000  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean     101.253019     92.886432     16.606632    395.988812    208.787297   \n",
       "std        0.646240    107.334489     19.380946     50.285896    273.216779   \n",
       "min       99.713600    -23.560700     -1.390840    229.260000      0.000000   \n",
       "25%      100.698000     10.046575      1.089850    361.367000      0.000000   \n",
       "50%      101.252000     37.869700      6.393675    409.094000     24.071450   \n",
       "75%      101.741000    166.994250     29.836850    436.387000    390.929500   \n",
       "max      102.997000    554.955000     77.052600    491.671000    893.002000   \n",
       "\n",
       "              sunFS  \n",
       "count  32498.000000  \n",
       "mean     250.884365  \n",
       "std      328.304546  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%       28.924900  \n",
       "75%      469.751250  \n",
       "max     1073.060000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32498"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(0,len(data)):\n",
    "    if  (data['SO2'][i] <= -15)|(data['SO2'][i]>=15):\n",
    "        index.append(i)\n",
    "    elif (data['NO2'][i] <= -25) | (data['NO2'][i] >=25):\n",
    "        index.append(i)\n",
    "    elif (data['PM10'][i] <= -25) | (data['PM10'][i] >= 25):\n",
    "        index.append(i)\n",
    "    elif (data['PM2.5'][i] <= -20) | (data['PM2.5'][i] >= 20):\n",
    "        index.append(i)\n",
    "    elif (data['O3'][i] <= -25) | (data['O3'][i] >=25):\n",
    "        index.append(i)\n",
    "    elif (data['CO'][i] >=3):\n",
    "        index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24320"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8177, 21)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(index=index,inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常值删除结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4 #模型数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各污染物浓度分别建模，自变量X不变（所有环节情况）\n",
    "SO2X = data.iloc[:8000,[6,7,9,10,11,12,14,16,18]] \n",
    "SO2TestX = data.iloc[8000:,[6,7,9,10,11,12,14,16,18]]\n",
    "NO2X = data.iloc[:8000,[6,7,9,10,11,12,14,18]]\n",
    "NO2TestX =data.iloc[8000:,[6,7,9,10,11,12,14,18]]\n",
    "PM10X =data.iloc[:8000,[6,7,10,12,14,16,18]]\n",
    "PM10TestX =data.iloc[8000:,[6,7,10,12,14,16,18]]\n",
    "PM2_5X =data.iloc[:8000,[6,7,9,10,11,12,18]]\n",
    "PM2_5TestX =data.iloc[8000:,[6,7,9,10,11,12,18]]\n",
    "O3X =data.iloc[:8000,[6,7,10,12,14,16,18]]\n",
    "O3TestX =data.iloc[8000:,[6,7,10,12,14,16,18]]\n",
    "COX =data.iloc[:8000,[6,7,9,10,12,14]]\n",
    "COTestX =data.iloc[8000:,[6,7,9,10,12,14]]\n",
    "n_train = SO2X.shape[0]\n",
    "\n",
    "SO2Y = data.iloc[:8000,0]\n",
    "SO2TestY = data.iloc[8000:,0]\n",
    "NO2Y = data.iloc[:8000,1]\n",
    "NO2TestY = data.iloc[8000:,1]\n",
    "PM10Y = data.iloc[:8000,2]\n",
    "PM10TestY = data.iloc[8000:,2]\n",
    "PM2_5Y = data.iloc[:8000,3]\n",
    "PM2_5TestY = data.iloc[8000:,3]\n",
    "O3Y = data.iloc[:8000,4]\n",
    "O3TestY = data.iloc[8000:,4]\n",
    "COY = data.iloc[:8000,5]\n",
    "COTestY = data.iloc[8000:,5]\n",
    "\n",
    "inputnums = [SO2X.shape[1],NO2X.shape[1],PM10X.shape[1],PM2_5X.shape[1],O3X.shape[1],COX.shape[1]]  #!!!!!!!!!!!!!!!!!!输入层结构\n",
    "inputnum = inputnums[0]\n",
    "outputnum = 1\n",
    "hiddnum1 = 15\n",
    "hiddennum = hiddnum1 \n",
    "numsum = inputnum*hiddennum+hiddennum+hiddennum*outputnum+outputnum #6*10+10+10*3+3=103\n",
    "\n",
    "LossArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#零均值处理\n",
    "\n",
    "SO2max = np.max(np.abs(SO2Y),axis=0)\n",
    "SO2mean = np.mean(SO2Y,axis=0)\n",
    "NO2max = np.max(np.abs(NO2Y),axis=0)\n",
    "NO2mean = np.mean(NO2Y,axis=0)\n",
    "PM10max = np.max(np.abs(PM10Y),axis=0)\n",
    "PM10mean = np.mean(PM10Y,axis=0)\n",
    "PM2_5max = np.max(np.abs(PM2_5Y),axis=0)\n",
    "PM2_5mean = np.mean(PM2_5Y,axis=0)\n",
    "O3max = np.max(np.abs(O3Y),axis=0)\n",
    "O3mean = np.mean(O3Y,axis=0)\n",
    "\n",
    "def data_progress(X,TestX):\n",
    "    for i in range(0,X.shape[1]):\n",
    "        X.iloc[:,i] -= np.mean(X,axis=0)[i]\n",
    "        X.iloc[:,i] /= np.max(np.abs(X),axis=0)[i]\n",
    "        TestX.iloc[:,i] -= np.mean(TestX,axis=0)[i]\n",
    "        TestX.iloc[:,i] /= np.max(np.abs(TestX),axis=0)[i]\n",
    "        \n",
    "    return 0\n",
    "def data_progressY(Y,TestY):\n",
    "    Y -= np.mean(Y,axis=0)\n",
    "    Y /= np.max(np.abs(Y),axis=0)\n",
    "    TestY -= np.mean(TestY,axis=0)\n",
    "    TestY /= np.max(np.abs(TestY),axis=0)\n",
    "    return 1\n",
    "    \n",
    "data_progress(SO2X,SO2TestX)\n",
    "data_progressY(SO2Y,SO2TestY)\n",
    "data_progress(NO2X,NO2TestX)\n",
    "data_progressY(NO2Y,NO2TestY)\n",
    "data_progress(PM10X,PM10TestX)\n",
    "data_progressY(PM10Y,PM10TestY)\n",
    "data_progress(PM2_5X,PM2_5TestX)\n",
    "data_progressY(PM2_5Y,PM2_5TestY)\n",
    "data_progress(O3X,O3TestX)\n",
    "data_progressY(O3Y,O3TestY)\n",
    "data_progress(COX,COTestX)\n",
    "data_progressY(COY,COTestY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.97744 2.9475783538887463 24.9993 -4.679705722500016 24.9965 11.227338726375043 19.9902 0.6359887460000033 24.99999156 5.7181293480728375\n"
     ]
    }
   ],
   "source": [
    "print(SO2max,SO2mean,NO2max,NO2mean,PM10max,PM10mean,PM2_5max,PM2_5mean,O3max,O3mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#n：布谷鸟数  m：布谷鸟的维度\n",
    "def cuckoo_search(n, m, lower_boundary, upper_boundary, iter_num = 10,pa = 0.25, beta = 1.5, step_size = 0.1,alpha=0.77,xn=0.33):\n",
    "\n",
    "    num=1\n",
    "    # get initial nests' locations \n",
    "    nests,best_nest,best_fitness,lossness = generate_nests(n, m,alpha,xn, lower_boundary, upper_boundary) #alpha,xn用于tent初始化阈值和初值设置\n",
    "    \n",
    "    # get the best nest and record it\n",
    "    LossArr.append(best_fitness) #全局变量，保存每代最佳适应度值\n",
    "    \n",
    "\n",
    "    print('\\r\\n BEST_LOSSNESS IS %.2f : \\r\\n',best_fitness)\n",
    "\n",
    "    for _ in range(iter_num):\n",
    "        \n",
    "        print('\\r\\n******************************************************第 %d 代开始迭代优化************************************************************\\r\\n'%num)\n",
    "        nests = update_nests(lower_boundary, upper_boundary, nests, best_nest, lossness, step_size,best_fitness)\n",
    "        nests = abandon_nests(nests, lower_boundary, upper_boundary, pa)\n",
    "        \n",
    "        print('\\r\\n*****************************************************第 %d 次迭代，计算适应度********************************************************\\r\\n'%num)\n",
    "        lossness,_ = calc_fitness( nests)\n",
    "        print('\\r\\n*********************************************************第 %d 次迭代结束************************************************************\\r\\n'%num)\n",
    "        \n",
    "        min_loss_index = np.argmin(lossness)\n",
    "        min_loss = lossness[min_loss_index]\n",
    "        min_nestloss = nests[min_loss_index]\n",
    "        LossArr.append(min_loss)\n",
    "            \n",
    "        if min_loss < best_fitness  : #and  min_loss_fit > best_two_fitness\n",
    "            best_nest = min_nestloss\n",
    "            best_fitness = min_loss\n",
    "            print('\\r\\n******')\n",
    "            print('\\r\\n 第 %d 次迭代最优Loss是 %.2f : \\r\\n'%(num,best_fitness))\n",
    "            print('\\r\\n******\\r\\n')\n",
    "        num+=1\n",
    "\n",
    "    return (best_nest, best_fitness)\n",
    "\n",
    "# #随机生成nest\n",
    "#     lower_boundary = np.array(lower_boundary)\n",
    "#     upper_boundary = np.array(upper_boundary)\n",
    "#     nests = np.empty((n, m))\n",
    "\n",
    "#     for each_nest in range(n):\n",
    "#         nests[each_nest] = lower_boundary + np.array([np.random.rand() for _ in range(m)]) * (upper_boundary - lower_boundary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nests(n, m,alpha,xn,upper_boundary , lower_boundary):\n",
    "\n",
    "#Tent混沌反向初始化\n",
    "\n",
    "# #混沌初始化\n",
    "    nests = np.empty((n, m))\n",
    "    sig_nest = np.empty(m)\n",
    "    alpha = alpha\n",
    "    xn = xn\n",
    "    for i in range(0,n):   #*2 值域为【-1,1】 *6 值域为[-3,3]\n",
    "        for j in range(0,m):\n",
    "            if 0<=xn<alpha:\n",
    "                xn = xn/alpha\n",
    "                sig_nest[j]=(xn-0.5)*6\n",
    "            elif alpha <= xn <= 1:\n",
    "                xn = (1-xn)/(1-alpha)\n",
    "                sig_nest[j] = (xn-0.5)*6\n",
    "            nests[i] = sig_nest\n",
    "            \n",
    "# #反向初始化            \n",
    "    renests = -1 * nests   #定义：Pi = ai + bi - pi  生成反向nests\n",
    "    \n",
    "# #拼接两个初始化nests\n",
    "    nests = np.vstack((nests,renests))  #拼接nests和renests 准备计算适应度选择最优的n个nest\n",
    "    \n",
    "#计算适应度\n",
    "    lossness,_ = calc_fitness( nests) \n",
    "    \n",
    "# #根据loss值排序\n",
    "    arrIndex = np.argsort(lossness)   #获得排序数组  从小到大\n",
    "    lossness = lossness[arrIndex]     #将lossness数组按照从小到大排序\n",
    "    nests = nests[arrIndex]    #将nests也按照相同序列进行排序，保证和lossness对齐\n",
    "    \n",
    "#删除多余的n组nest，这里从最底下开始一个个删，因为已经排好序了，所以删除的为效果最差的\n",
    "    for i in range(n):\n",
    "        nests = np.delete(nests,-1,0)\n",
    "        lossness = np.delete(lossness,-1,0)\n",
    "#现在的nests是按照loss排序的，第一个loss最小\n",
    "\n",
    "    return nests,nests[0],lossness[0],lossness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nests(lower_boundary, upper_boundary, nests, best_nest, lossness, step_coefficient,bestfitness):\n",
    "\n",
    "    lower_boundary = np.array(lower_boundary)\n",
    "    upper_boundary = np.array(upper_boundary)\n",
    "    n, m = nests.shape\n",
    "    # 莱维飞行生成步长：较长时间的短步长和较短时间的长步长\n",
    "    #生成的步长的shape也是n*m，保证所有元素得到更新\n",
    "    #这个步长通过step_size来控制大小\n",
    "    steps = levy_flight(n, m, 1.5)\n",
    "    new_nests = nests.copy()\n",
    "\n",
    "    for each_nest in range(n):\n",
    "        # coefficient 0.01 is to avoid levy flights becoming too aggresive\n",
    "        # and (nest[each_nest] - best_nest) 保留了最佳nest，但会导致局部最优\n",
    "        step_size = step_coefficient * steps[each_nest] # * (nests[each_nest] - best_nest)\n",
    "        step_direction = np.random.rand(m) #0-1均匀概率分布生成方向\n",
    "        #新的布谷鸟在原来的基础上加上步长和方向\n",
    "        new_nests[each_nest] += step_size * step_direction  \n",
    "        # apply boundary condtions\n",
    "        new_nests[each_nest][new_nests[each_nest] < lower_boundary] = lower_boundary[new_nests[each_nest] < lower_boundary]\n",
    "        new_nests[each_nest][new_nests[each_nest] > upper_boundary] = upper_boundary[new_nests[each_nest] > upper_boundary]\n",
    "\n",
    "    new_losses,new_nests = calc_fitness(new_nests)\n",
    "    #适应度更好的才更新过去\n",
    "    \n",
    "    nests[new_losses < lossness] = new_nests[new_losses < lossness] \n",
    "    return nests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abandon_nests(nests, lower_boundary, upper_boundary, pa):\n",
    "\n",
    "    lower_boundary = np.array(lower_boundary)\n",
    "    upper_boundary = np.array(upper_boundary)\n",
    "    n, m = nests.shape\n",
    "    for each_nest in range(n):  \n",
    "        #pa概率抛弃，抛弃后重新生成\n",
    "        if (np.random.rand() < pa):\n",
    "            #局部随机行走生成步长\n",
    "            #随机两个种群相差*一个0-1的随机数\n",
    "            step_size = np.random.rand() * (nests[np.random.randint(0, n)] - nests[np.random.randint(0, n)])\n",
    "            nests[each_nest] += step_size\n",
    "            # apply boundary condtions\n",
    "            nests[each_nest][nests[each_nest] < lower_boundary] = lower_boundary[nests[each_nest] < lower_boundary]\n",
    "            nests[each_nest][nests[each_nest] > upper_boundary] = upper_boundary[nests[each_nest] > upper_boundary]\n",
    "    \n",
    "    return nests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levy_flight(n, m, beta):\n",
    "\n",
    "    sigma_u = (sc_special.gamma(1+beta)*np.sin(np.pi*beta/2)/(sc_special.gamma((1+beta)/2)*beta*(2**((beta-1)/2))))**(1/beta)\n",
    "    sigma_v = 1\n",
    "\n",
    "    u =  np.random.normal(0, sigma_u, (n, m))\n",
    "    v = np.random.normal(0, sigma_v, (n, m))\n",
    "\n",
    "    steps = u/((np.abs(v))**(1/beta))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fitness(nests):\n",
    "    \n",
    "    n, m = nests.shape\n",
    "    lossness = np.empty(n)\n",
    "    new_nests = nests\n",
    "    for Sig_nest in range(n):\n",
    "        chrom = nests[Sig_nest]\n",
    "        #分解模型参数\n",
    "        w1 = chrom[:inputnum*hiddennum]\n",
    "        w1 = w1.reshape(inputnum,hiddennum)\n",
    "        b1 = chrom[inputnum*hiddennum:inputnum*hiddennum+hiddennum]\n",
    "        w2 = chrom[inputnum*hiddennum+hiddennum:inputnum*hiddennum+hiddennum+hiddennum*outputnum]\n",
    "        w2 = w2.reshape(hiddennum,outputnum)\n",
    "        b2 = chrom[inputnum*hiddennum+hiddennum+hiddennum*outputnum:]\n",
    "\n",
    "        WB_l1 = (w1,b1)\n",
    "        WB_l2 = (w2,b2)\n",
    "        #创建模型，并赋予参数\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(hiddennum,activation='relu',name='l1'),\n",
    "            keras.layers.Dense(outputnum,name='l2')\n",
    "        ])\n",
    "        model.build(input_shape=[None,inputnum])\n",
    "        \n",
    "        #model.summary()\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='mse',)\n",
    "        \n",
    "        layer1 = model.get_layer('l1')\n",
    "        layer2 = model.get_layer('l2')\n",
    "        layer1.set_weights(WB_l1)\n",
    "        layer2.set_weights(WB_l2)\n",
    "        \n",
    "        #训练模型\n",
    "        #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！下面两句输入数据集\n",
    "        model.fit(SO2X,SO2Y,epochs=1)\n",
    "        loss = model.evaluate(SO2X,SO2Y)  #这里是用验证集评估，其实应该用测试集吧 evaluate产生两个结果，前面是损失，后面是准确率\n",
    "        lossness[Sig_nest] = loss\n",
    "        \n",
    "        \n",
    "        (k1,y1) = layer1.get_weights()  #获取训练后的神经网络权值，并赋值给c\n",
    "        (k2,y2) = layer2.get_weights()\n",
    "        c=k1.reshape(1,-1).tolist()[0] + y1.reshape(1,-1).tolist()[0] + k2.reshape(1,-1).tolist()[0] + y2.reshape(1,-1).tolist()[0]\n",
    "        new_nests[Sig_nest] = c\n",
    "        \n",
    "        \n",
    "#         if(acc>bestfitness): #在计算适应度的过程当中，发现有更好的适应度就把参数保存\n",
    "#             #model.save_weights('my_model_fun.h5')\n",
    "#             bestfitness = acc\n",
    "    return lossness,new_nests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 个CS算法开始\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 29.4839\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.8198\n",
      "250/250 [==============================] - 1s 926us/step - loss: 16.3935\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.6954\n",
      "250/250 [==============================] - 0s 735us/step - loss: 20.5580\n",
      "250/250 [==============================] - 0s 658us/step - loss: 1.1126\n",
      "250/250 [==============================] - 0s 791us/step - loss: 32.9880\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.9494\n",
      "250/250 [==============================] - 0s 899us/step - loss: 32.0154\n",
      "250/250 [==============================] - 0s 666us/step - loss: 1.7376\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 55.8616\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.7105\n",
      "250/250 [==============================] - 0s 847us/step - loss: 28.6391\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.9899\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 14.0845\n",
      "250/250 [==============================] - 0s 689us/step - loss: 0.2269\n",
      "250/250 [==============================] - 0s 783us/step - loss: 41.1217\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.7577\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 74.3868\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.2684\n",
      "250/250 [==============================] - 0s 939us/step - loss: 72.7739\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.8479\n",
      "250/250 [==============================] - 1s 871us/step - loss: 10.9867\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.6346\n",
      "250/250 [==============================] - 0s 907us/step - loss: 26.4603\n",
      "250/250 [==============================] - 0s 622us/step - loss: 1.0366\n",
      "250/250 [==============================] - 0s 779us/step - loss: 27.2625\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.3388\n",
      "250/250 [==============================] - 0s 743us/step - loss: 12.0368\n",
      "250/250 [==============================] - 0s 847us/step - loss: 1.3427\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 2.0677\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0858\n",
      "250/250 [==============================] - 1s 987us/step - loss: 8.6276\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.4169\n",
      "250/250 [==============================] - 0s 710us/step - loss: 42.0397\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.5952\n",
      "250/250 [==============================] - 0s 718us/step - loss: 3.3947\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.2237\n",
      "250/250 [==============================] - 0s 773us/step - loss: 16.1717\n",
      "250/250 [==============================] - 0s 639us/step - loss: 0.6786\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.08581571280956268\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.6562\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0921\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.4713\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.1054\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.7978\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0984\n",
      "250/250 [==============================] - 0s 726us/step - loss: 1.1931\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1281\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.1534\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.1393\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.7269\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.1717\n",
      "250/250 [==============================] - 0s 745us/step - loss: 1.6177\n",
      "250/250 [==============================] - 0s 564us/step - loss: 0.1955\n",
      "250/250 [==============================] - 0s 739us/step - loss: 1.4708\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.3055\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.2498\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0622\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.4354\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1343\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 734us/step - loss: 1.7093\n",
      "250/250 [==============================] - 0s 596us/step - loss: 0.1866\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.1086\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0749\n",
      "250/250 [==============================] - 0s 719us/step - loss: 6.1830\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.1254\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0961\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0607\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.1108\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0654\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.1294\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0678\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1589\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0767\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.2437\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.1009\n",
      "250/250 [==============================] - 0s 746us/step - loss: 0.0661\n",
      "250/250 [==============================] - 0s 679us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.1092\n",
      "250/250 [==============================] - 0s 647us/step - loss: 0.0680\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.06 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.4616\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1154\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1336\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0627\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.8598\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0722\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.4605\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0555\n",
      "250/250 [==============================] - 0s 826us/step - loss: 10.6460\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0716\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.2045\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0551\n",
      "250/250 [==============================] - 0s 868us/step - loss: 2.7439\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1106\n",
      "250/250 [==============================] - 0s 722us/step - loss: 2.2845\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.1238\n",
      "250/250 [==============================] - 0s 722us/step - loss: 1.4601\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0768\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.2430\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0612\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 747us/step - loss: 1.3348\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.1328\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 593us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0690\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0485\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0617\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0593\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.0718\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0523\n",
      "250/250 [==============================] - 0s 694us/step - loss: 9.2706\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.5123\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0734\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0563\n",
      "250/250 [==============================] - 0s 717us/step - loss: 0.0950\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0639\n",
      "250/250 [==============================] - 0s 891us/step - loss: 8.8599\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.4183\n",
      "250/250 [==============================] - 1s 753us/step - loss: 1.0172\n",
      "250/250 [==============================] - 0s 592us/step - loss: 0.0766\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.05 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.6295\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.1230\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.4689\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0705\n",
      "250/250 [==============================] - 0s 746us/step - loss: 0.2296\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0562\n",
      "250/250 [==============================] - 0s 718us/step - loss: 5.5469\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0847\n",
      "250/250 [==============================] - 0s 749us/step - loss: 1.0564\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0757\n",
      "250/250 [==============================] - 0s 775us/step - loss: 3.5556\n",
      "250/250 [==============================] - 0s 620us/step - loss: 0.2587\n",
      "250/250 [==============================] - 0s 747us/step - loss: 1.4910\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0571\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.3256\n",
      "250/250 [==============================] - 0s 628us/step - loss: 0.0627\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.7571\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0785\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.2232\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0499\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.1016\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0672\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.4666\n",
      "250/250 [==============================] - 0s 589us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0534\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0458\n",
      "250/250 [==============================] - 0s 728us/step - loss: 0.0582\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0651\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0585\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.2039\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0888\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0842\n",
      "250/250 [==============================] - 0s 770us/step - loss: 0.0516\n",
      "250/250 [==============================] - 0s 935us/step - loss: 7.4482\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.4868\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0668\n",
      "250/250 [==============================] - 0s 845us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0506\n",
      "250/250 [==============================] - 0s 677us/step - loss: 0.0482\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 3 次迭代最优Loss是 0.05 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.4811\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0597\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1782\n",
      "250/250 [==============================] - 0s 577us/step - loss: 0.0534\n",
      "250/250 [==============================] - 0s 719us/step - loss: 1.5393\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0628\n",
      "250/250 [==============================] - 0s 764us/step - loss: 1.2180\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0609\n",
      "250/250 [==============================] - 0s 743us/step - loss: 1.0010\n",
      "250/250 [==============================] - 0s 720us/step - loss: 0.0957\n",
      "250/250 [==============================] - 0s 762us/step - loss: 1.8522\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0965\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.4739\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0529\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.5241\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0992\n",
      "250/250 [==============================] - 0s 700us/step - loss: 1.0877\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0699\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.1487\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0464\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0667\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0500\n",
      "250/250 [==============================] - 0s 732us/step - loss: 0.0538\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0486\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0441\n",
      "250/250 [==============================] - 0s 840us/step - loss: 0.0560\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0615\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0580\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0854\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0781\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0592\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0532\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0950\n",
      "250/250 [==============================] - 0s 624us/step - loss: 0.0590\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0527\n",
      "250/250 [==============================] - 0s 568us/step - loss: 0.0455\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0441\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 4 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 740us/step - loss: 3.9121\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0765\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.8828\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0584\n",
      "250/250 [==============================] - 1s 739us/step - loss: 0.4842\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0472\n",
      "250/250 [==============================] - 0s 735us/step - loss: 5.5497\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0515\n",
      "250/250 [==============================] - 0s 702us/step - loss: 1.1172\n",
      "250/250 [==============================] - 0s 568us/step - loss: 0.0616\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.7345\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0731\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.9927\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0526\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.2848\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 714us/step - loss: 4.0317\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0830\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.2788\n",
      "250/250 [==============================] - 0s 605us/step - loss: 0.0471\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 771us/step - loss: 35.8246\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.4623\n",
      "250/250 [==============================] - 0s 892us/step - loss: 0.6647\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0661\n",
      "250/250 [==============================] - 0s 770us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0556\n",
      "250/250 [==============================] - 0s 701us/step - loss: 0.0595\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0593\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0731\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0562\n",
      "250/250 [==============================] - 0s 770us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0553\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0555\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0450\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0469\n",
      "250/250 [==============================] - 0s 669us/step - loss: 0.0454\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 825us/step - loss: 0.8600\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0958\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.4783\n",
      "250/250 [==============================] - 0s 685us/step - loss: 0.0552\n",
      "250/250 [==============================] - 0s 787us/step - loss: 7.0582\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0521\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.3841\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0478\n",
      "250/250 [==============================] - 0s 795us/step - loss: 2.0327\n",
      "250/250 [==============================] - 0s 605us/step - loss: 0.0744\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.3552\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0588\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.5193\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0648\n",
      "250/250 [==============================] - 0s 763us/step - loss: 2.0381\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0663\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 2.9709\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.1115\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 2.1073\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.0641\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0850\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.0610\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0634\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0496\n",
      "250/250 [==============================] - 0s 849us/step - loss: 0.0471\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 747us/step - loss: 1.6651\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0936\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 639us/step - loss: 0.0492\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0565\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0618\n",
      "250/250 [==============================] - 0s 853us/step - loss: 0.0538\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0540\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0584\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 599us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0472\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0454\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.4270\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0710\n",
      "250/250 [==============================] - 0s 775us/step - loss: 2.1388\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0779\n",
      "250/250 [==============================] - 0s 746us/step - loss: 1.1094\n",
      "250/250 [==============================] - 0s 632us/step - loss: 0.0544\n",
      "250/250 [==============================] - 0s 823us/step - loss: 1.0894\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.0860\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.3960\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0526\n",
      "250/250 [==============================] - 1s 833us/step - loss: 2.1484\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1264\n",
      "250/250 [==============================] - 0s 779us/step - loss: 17.5315\n",
      "250/250 [==============================] - 0s 716us/step - loss: 0.1899\n",
      "250/250 [==============================] - 0s 751us/step - loss: 1.4745\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0647\n",
      "250/250 [==============================] - 0s 808us/step - loss: 0.2470\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0920\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0436\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 865us/step - loss: 139.2531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 670us/step - loss: 1.3746\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 2.3707\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0642\n",
      "250/250 [==============================] - 0s 722us/step - loss: 4.6089\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0916\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0807\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0644\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0582\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0524\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0494\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0473\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0489\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.6206\n",
      "250/250 [==============================] - 0s 591us/step - loss: 0.0552\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0475\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0454\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 7 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 717us/step - loss: 9.1742\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.4861\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.2956\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0665\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.2783\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0587\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.3361\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0612\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.8308\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0685\n",
      "250/250 [==============================] - 0s 783us/step - loss: 1.1349\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.1200\n",
      "250/250 [==============================] - 0s 718us/step - loss: 2.4045\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0468\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.1776\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0433\n",
      "250/250 [==============================] - 0s 745us/step - loss: 0.4421\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0464\n",
      "250/250 [==============================] - 0s 754us/step - loss: 0.3075\n",
      "250/250 [==============================] - 0s 625us/step - loss: 0.0461\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.3007\n",
      "250/250 [==============================] - 0s 577us/step - loss: 0.1296\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0738\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0503\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0574\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 750us/step - loss: 33.6010\n",
      "250/250 [==============================] - 0s 582us/step - loss: 1.0838\n",
      "250/250 [==============================] - 0s 713us/step - loss: 0.0629\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0578\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0485\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0431\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0564\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0612\n",
      "250/250 [==============================] - 0s 727us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 585us/step - loss: 0.0579\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0479\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0460\n",
      "250/250 [==============================] - 0s 725us/step - loss: 0.0450\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0416\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 8 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 710us/step - loss: 1.6550\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1843\n",
      "250/250 [==============================] - 0s 775us/step - loss: 1.5814\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0691\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.1896\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0456\n",
      "250/250 [==============================] - 0s 721us/step - loss: 1.2339\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.3625\n",
      "250/250 [==============================] - 0s 743us/step - loss: 5.6719\n",
      "250/250 [==============================] - 0s 649us/step - loss: 0.0940\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.2489\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0534\n",
      "250/250 [==============================] - 0s 732us/step - loss: 0.3350\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0562\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.2808\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0472\n",
      "250/250 [==============================] - 1s 759us/step - loss: 0.2370\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0441\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.1098\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0461\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.4310\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.1383\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0641\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0467\n",
      "250/250 [==============================] - 0s 584us/step - loss: 0.0493\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.3251\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.1718\n",
      "250/250 [==============================] - 0s 720us/step - loss: 0.0579\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0484\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0569\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0560\n",
      "250/250 [==============================] - 0s 660us/step - loss: 0.0523\n",
      "250/250 [==============================] - 0s 718us/step - loss: 2.0294\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.0716\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 665us/step - loss: 0.0421\n",
      "250/250 [==============================] - 0s 745us/step - loss: 429.1611\n",
      "250/250 [==============================] - 0s 593us/step - loss: 10.4231\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 718us/step - loss: 0.1810\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0631\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1739\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.3070\n",
      "250/250 [==============================] - 0s 651us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 734us/step - loss: 2.1088\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1979\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.2283\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 754us/step - loss: 0.3581\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0466\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.1794\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0501\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.4890\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0566\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.1314\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0467\n",
      "250/250 [==============================] - 0s 823us/step - loss: 5.3771\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.6656\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0646\n",
      "250/250 [==============================] - 0s 595us/step - loss: 0.0485\n",
      "250/250 [==============================] - 0s 794us/step - loss: 27.4219\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.7875\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0441\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0420\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.1622\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1220\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0557\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 818us/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 719us/step - loss: 0.0438\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0588\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0450\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0730\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0506\n",
      "250/250 [==============================] - 0s 946us/step - loss: 0.0455\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0416\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.4580\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1837\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "第 2 个CS算法开始\n",
      "250/250 [==============================] - 0s 847us/step - loss: 14.9651\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.8018\n",
      "250/250 [==============================] - 0s 771us/step - loss: 4.8349\n",
      "250/250 [==============================] - 0s 631us/step - loss: 0.3255\n",
      "250/250 [==============================] - 0s 935us/step - loss: 2.3692\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.1699\n",
      "250/250 [==============================] - 0s 795us/step - loss: 5.7760\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.2529\n",
      "250/250 [==============================] - 0s 791us/step - loss: 1.5165\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.1809\n",
      "250/250 [==============================] - 1s 959us/step - loss: 55.2223\n",
      "250/250 [==============================] - 0s 746us/step - loss: 0.7393\n",
      "250/250 [==============================] - 0s 786us/step - loss: 29.4420\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.4703\n",
      "250/250 [==============================] - 0s 722us/step - loss: 27.5085\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.9506\n",
      "250/250 [==============================] - 0s 774us/step - loss: 112.0870\n",
      "250/250 [==============================] - 0s 602us/step - loss: 1.1346\n",
      "250/250 [==============================] - 0s 783us/step - loss: 22.4109\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.9429\n",
      "250/250 [==============================] - 0s 807us/step - loss: 16.8124\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.9039\n",
      "250/250 [==============================] - 1s 777us/step - loss: 44.0359\n",
      "250/250 [==============================] - 0s 714us/step - loss: 1.2507\n",
      "250/250 [==============================] - 0s 801us/step - loss: 5.6334\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.1949\n",
      "250/250 [==============================] - 0s 771us/step - loss: 9.0828\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.6057\n",
      "250/250 [==============================] - 0s 799us/step - loss: 35.4140\n",
      "250/250 [==============================] - 0s 710us/step - loss: 1.5465\n",
      "250/250 [==============================] - 0s 758us/step - loss: 3.4940\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.3642\n",
      "250/250 [==============================] - 0s 775us/step - loss: 4.3129\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.2339\n",
      "250/250 [==============================] - 0s 807us/step - loss: 5.2458\n",
      "250/250 [==============================] - 0s 633us/step - loss: 0.4960\n",
      "250/250 [==============================] - 0s 787us/step - loss: 100.2216\n",
      "250/250 [==============================] - 0s 772us/step - loss: 1.1715\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 90.0110\n",
      "250/250 [==============================] - 0s 811us/step - loss: 1.2544\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.16991941630840302\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.7354\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.1113\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.2491\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0678\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.9585\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.1434\n",
      "250/250 [==============================] - 0s 747us/step - loss: 3.2734\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0898\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.7369\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.1124\n",
      "250/250 [==============================] - 0s 807us/step - loss: 3.9153\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.2115\n",
      "250/250 [==============================] - 0s 774us/step - loss: 2.5430\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1816\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.5837\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.1163\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.8807\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.1590\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.9674\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.1503\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 770us/step - loss: 0.1027\n",
      "250/250 [==============================] - 0s 656us/step - loss: 0.0801\n",
      "250/250 [==============================] - 0s 790us/step - loss: 0.1537\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0655\n",
      "250/250 [==============================] - 0s 855us/step - loss: 2.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 606us/step - loss: 0.1068\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0800\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0625\n",
      "250/250 [==============================] - 0s 797us/step - loss: 0.0854\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 813us/step - loss: 3.5236\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.3239\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.1433\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.0746\n",
      "250/250 [==============================] - 0s 796us/step - loss: 0.0988\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0628\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1258\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0638\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.1385\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0806\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.05 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 778us/step - loss: 0.4075\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0589\n",
      "250/250 [==============================] - 0s 771us/step - loss: 2.2218\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.1210\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.4697\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0683\n",
      "250/250 [==============================] - 0s 809us/step - loss: 0.2264\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0599\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0856\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 774us/step - loss: 2.4599\n",
      "250/250 [==============================] - 0s 699us/step - loss: 0.1818\n",
      "250/250 [==============================] - 0s 967us/step - loss: 1.1262\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0715\n",
      "250/250 [==============================] - 0s 726us/step - loss: 1.2921\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0563\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.1827\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0531\n",
      "250/250 [==============================] - 0s 908us/step - loss: 0.4060\n",
      "250/250 [==============================] - 0s 608us/step - loss: 0.0781\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 720us/step - loss: 0.0629\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0668\n",
      "250/250 [==============================] - 0s 576us/step - loss: 0.0592\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.6809\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0628\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0607\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0507\n",
      "250/250 [==============================] - 1s 706us/step - loss: 0.0541\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0528\n",
      "250/250 [==============================] - 0s 734us/step - loss: 3.3110\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.1703\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0722\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0559\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.0535\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0446\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.5963\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0803\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0844\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0712\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 724us/step - loss: 4.9256\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.1082\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.2397\n",
      "250/250 [==============================] - 0s 617us/step - loss: 0.0601\n",
      "250/250 [==============================] - 0s 755us/step - loss: 2.2892\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1503\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.5178\n",
      "250/250 [==============================] - 0s 647us/step - loss: 0.0553\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.1810\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0503\n",
      "250/250 [==============================] - 0s 731us/step - loss: 1.5718\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0998\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.3816\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0590\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.584 - 0s 726us/step - loss: 0.5244\n",
      "250/250 [==============================] - 0s 613us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.2112\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0576\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.5158\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0633\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 714us/step - loss: 1.6099\n",
      "250/250 [==============================] - 0s 611us/step - loss: 0.1184\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 743us/step - loss: 10.9890\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1134\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0542\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0551\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0470\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0904\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0539\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0572\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0491\n",
      "250/250 [==============================] - 0s 727us/step - loss: 0.0499\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0510\n",
      "250/250 [==============================] - 0s 731us/step - loss: 23.0779\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.5412\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0690\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0768\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.5664\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0793\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.2501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 751us/step - loss: 0.0573\n",
      "250/250 [==============================] - 0s 731us/step - loss: 5.7572\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1456\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.7828\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0543\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.5607\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0578\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.4540\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0617\n",
      "250/250 [==============================] - 0s 718us/step - loss: 1.3245\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0824\n",
      "250/250 [==============================] - 0s 759us/step - loss: 1.6599\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0583\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.3701\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0728\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.6624\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0676\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 982us/step - loss: 1.9718\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.1479\n",
      "250/250 [==============================] - 0s 741us/step - loss: 21.9259\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.8163\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0808\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0534\n",
      "250/250 [==============================] - 0s 891us/step - loss: 30.4962\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.0763\n",
      "250/250 [==============================] - 0s 881us/step - loss: 0.0490\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0590\n",
      "250/250 [==============================] - 0s 633us/step - loss: 0.0546\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0517\n",
      "250/250 [==============================] - 1s 923us/step - loss: 63.0603\n",
      "250/250 [==============================] - 0s 685us/step - loss: 0.7394\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0610\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0509\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0750\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0577\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 769us/step - loss: 3.8022\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.1360\n",
      "250/250 [==============================] - 0s 730us/step - loss: 4.4656\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.2377\n",
      "250/250 [==============================] - 0s 815us/step - loss: 3.0267\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0529\n",
      "250/250 [==============================] - 0s 718us/step - loss: 3.4949\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.3288\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.2324\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 741us/step - loss: 0.2878\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0583\n",
      "250/250 [==============================] - 0s 702us/step - loss: 1.0568\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0967\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.9024\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1182\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.1870\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0479\n",
      "250/250 [==============================] - 0s 741us/step - loss: 1.1877\n",
      "250/250 [==============================] - 0s 632us/step - loss: 0.0933\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.1084\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0530\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.1625\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0629\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0567\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 838us/step - loss: 0.1966\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0956\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0572\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0451\n",
      "250/250 [==============================] - 0s 713us/step - loss: 0.0509\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0549\n",
      "250/250 [==============================] - 0s 737us/step - loss: 0.0494\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0485\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0912\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0499\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 747us/step - loss: 9.4101\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1453\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 791us/step - loss: 1.3720\n",
      "250/250 [==============================] - 0s 705us/step - loss: 0.0633\n",
      "250/250 [==============================] - 0s 714us/step - loss: 2.6696\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0930\n",
      "250/250 [==============================] - 0s 872us/step - loss: 0.2556\n",
      "250/250 [==============================] - 0s 683us/step - loss: 0.0464\n",
      "250/250 [==============================] - 0s 709us/step - loss: 0.4373\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1039\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.6739\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0469\n",
      "250/250 [==============================] - 0s 705us/step - loss: 6.9922\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.1177\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.9176\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0588\n",
      "250/250 [==============================] - 0s 729us/step - loss: 3.7673\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0791\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.8794\n",
      "250/250 [==============================] - 0s 645us/step - loss: 0.0638\n",
      "250/250 [==============================] - 0s 719us/step - loss: 5.8685\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0689\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0560\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0481\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0608\n",
      "250/250 [==============================] - 0s 744us/step - loss: 0.0544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 717us/step - loss: 0.0533\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0495\n",
      "250/250 [==============================] - 0s 716us/step - loss: 0.1175\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0887\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0509\n",
      "250/250 [==============================] - 0s 580us/step - loss: 0.0432\n",
      "250/250 [==============================] - 0s 717us/step - loss: 0.0473\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0438\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 995us/step - loss: 0.0862\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0542\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.7916\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.1196\n",
      "250/250 [==============================] - 0s 911us/step - loss: 1.6098\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.1503\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 6 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 859us/step - loss: 0.2716\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 702us/step - loss: 1.9888\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0501\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.2274\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0532\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.3337\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0686\n",
      "250/250 [==============================] - 0s 709us/step - loss: 0.1554\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0456\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.6586\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0468\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.3456\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0520\n",
      "250/250 [==============================] - 0s 714us/step - loss: 7.0708\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0655\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.2131\n",
      "250/250 [==============================] - 0s 632us/step - loss: 0.0580\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.4196\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1300\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0549\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0503\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.2262\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0475\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0512\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0478\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0845\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0655\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0459\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0433\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0526\n",
      "250/250 [==============================] - 0s 621us/step - loss: 0.0534\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0483\n",
      "250/250 [==============================] - 0s 597us/step - loss: 0.0479\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0550\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.0840\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0565\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 713us/step - loss: 1.1461\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0513\n",
      "250/250 [==============================] - 0s 727us/step - loss: 0.3377\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0468\n",
      "250/250 [==============================] - 0s 743us/step - loss: 12.7497\n",
      "250/250 [==============================] - 0s 608us/step - loss: 0.0851\n",
      "250/250 [==============================] - 0s 719us/step - loss: 1.0581\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0831\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.2089\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 709us/step - loss: 0.1331\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.9903\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0677\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.1587\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.5499\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0778\n",
      "250/250 [==============================] - 0s 746us/step - loss: 2.8573\n",
      "250/250 [==============================] - 0s 664us/step - loss: 0.0600\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0502\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0466\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 781us/step - loss: 0.0470\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0488\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0745\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 725us/step - loss: 0.0472\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0435\n",
      "250/250 [==============================] - 0s 722us/step - loss: 1.2221\n",
      "250/250 [==============================] - 0s 664us/step - loss: 0.1454\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.0484\n",
      "250/250 [==============================] - 0s 652us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0513\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0446\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0493\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0424\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.1839\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0711\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 8 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.4741\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 710us/step - loss: 1.7243\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0638\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.1791\n",
      "250/250 [==============================] - 0s 589us/step - loss: 0.0434\n",
      "250/250 [==============================] - 1s 715us/step - loss: 1.2785\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0604\n",
      "250/250 [==============================] - 0s 698us/step - loss: 2.1428\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0711\n",
      "250/250 [==============================] - 0s 999us/step - loss: 1.6354\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.1276\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.1157\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0460\n",
      "250/250 [==============================] - 0s 717us/step - loss: 0.2902\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0484\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.4228\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0506\n",
      "250/250 [==============================] - 0s 777us/step - loss: 0.9211\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0488\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 705us/step - loss: 8.4820\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.1708\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0457\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0422\n",
      "250/250 [==============================] - 0s 699us/step - loss: 1.3341\n",
      "250/250 [==============================] - 0s 584us/step - loss: 0.0582\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0705\n",
      "250/250 [==============================] - 0s 603us/step - loss: 0.0467\n",
      "250/250 [==============================] - 0s 697us/step - loss: 0.0465\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0435\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.1064\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0612\n",
      "250/250 [==============================] - 0s 717us/step - loss: 0.0464\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0433\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0466\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0438\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0758\n",
      "250/250 [==============================] - 0s 592us/step - loss: 0.0788\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 9 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 7.1882\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0626\n",
      "250/250 [==============================] - 0s 705us/step - loss: 0.3614\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.5966\n",
      "250/250 [==============================] - 0s 581us/step - loss: 0.0516\n",
      "250/250 [==============================] - 0s 686us/step - loss: 1.5601\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0561\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.5535\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0559\n",
      "250/250 [==============================] - 0s 722us/step - loss: 1.0364\n",
      "250/250 [==============================] - 0s 592us/step - loss: 0.0622\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.5922\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0573\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.3853\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0506\n",
      "250/250 [==============================] - 0s 706us/step - loss: 2.3732\n",
      "250/250 [==============================] - 0s 583us/step - loss: 0.2090\n",
      "250/250 [==============================] - 0s 723us/step - loss: 1.0684\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0542\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0653\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0459\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.0422\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 723us/step - loss: 27.7803\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.9346\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0683\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0593\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0445\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0432\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0599\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0466\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 737us/step - loss: 0.0473\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0447\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.0433\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0418\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0719\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0507\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 10 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "第 3 个CS算法开始\n",
      "250/250 [==============================] - 0s 725us/step - loss: 2.0113\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.2505\n",
      "250/250 [==============================] - 0s 718us/step - loss: 14.4686\n",
      "250/250 [==============================] - 0s 641us/step - loss: 0.3554\n",
      "250/250 [==============================] - 0s 743us/step - loss: 13.8313\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.8838\n",
      "250/250 [==============================] - 0s 722us/step - loss: 44.1566\n",
      "250/250 [==============================] - 0s 650us/step - loss: 1.0422\n",
      "250/250 [==============================] - 0s 747us/step - loss: 4.6061\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.3172\n",
      "250/250 [==============================] - 0s 749us/step - loss: 11.9445\n",
      "250/250 [==============================] - 0s 644us/step - loss: 0.4661\n",
      "250/250 [==============================] - 1s 722us/step - loss: 4.4639\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.2122\n",
      "250/250 [==============================] - 0s 738us/step - loss: 5.7208\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.4568\n",
      "250/250 [==============================] - 0s 731us/step - loss: 12.4370\n",
      "250/250 [==============================] - 0s 671us/step - loss: 0.7791\n",
      "250/250 [==============================] - 0s 694us/step - loss: 9.9711\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.6585\n",
      "250/250 [==============================] - 0s 710us/step - loss: 1.5612\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.1952\n",
      "250/250 [==============================] - 0s 735us/step - loss: 18.0762\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.3156\n",
      "250/250 [==============================] - 0s 728us/step - loss: 734.6646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 590us/step - loss: 19.8469\n",
      "250/250 [==============================] - 0s 690us/step - loss: 49.2399\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.6986\n",
      "250/250 [==============================] - 0s 726us/step - loss: 4.7783\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.3388\n",
      "250/250 [==============================] - 0s 706us/step - loss: 7.2078\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.4406\n",
      "250/250 [==============================] - 0s 719us/step - loss: 37.4991\n",
      "250/250 [==============================] - 0s 589us/step - loss: 1.3379\n",
      "250/250 [==============================] - 0s 718us/step - loss: 11.2227\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.5537\n",
      "250/250 [==============================] - 0s 697us/step - loss: 12.0509\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.6148\n",
      "250/250 [==============================] - 0s 718us/step - loss: 4.5696\n",
      "250/250 [==============================] - 0s 600us/step - loss: 0.5110\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.19515787065029144\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.7378\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1067\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.2826\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0792\n",
      "250/250 [==============================] - 0s 710us/step - loss: 7.6441\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.3406\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.7204\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0872\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.2754\n",
      "250/250 [==============================] - 0s 657us/step - loss: 0.0770\n",
      "250/250 [==============================] - 0s 730us/step - loss: 1.4065\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0982\n",
      "250/250 [==============================] - 0s 735us/step - loss: 2.0515\n",
      "250/250 [==============================] - 0s 673us/step - loss: 0.1525\n",
      "250/250 [==============================] - 0s 706us/step - loss: 2.1258\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.2710\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.6211\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.1019\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.5660\n",
      "250/250 [==============================] - 0s 660us/step - loss: 0.1026\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0829\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0529\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0767\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0590\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.2892\n",
      "250/250 [==============================] - 0s 572us/step - loss: 0.0658\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0866\n",
      "250/250 [==============================] - 0s 607us/step - loss: 0.0526\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0688\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0966\n",
      "250/250 [==============================] - 0s 688us/step - loss: 0.0613\n",
      "250/250 [==============================] - 0s 685us/step - loss: 0.1264\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0648\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.2171\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0737\n",
      "250/250 [==============================] - 0s 730us/step - loss: 3.4554\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.2545\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.0877\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0531\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.05 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 741us/step - loss: 0.1568\n",
      "250/250 [==============================] - 0s 580us/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.1160\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0576\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1983\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0529\n",
      "250/250 [==============================] - 0s 718us/step - loss: 1.4149\n",
      "250/250 [==============================] - 0s 616us/step - loss: 0.0591\n",
      "250/250 [==============================] - 0s 735us/step - loss: 1.1936\n",
      "250/250 [==============================] - 0s 583us/step - loss: 0.0572\n",
      "250/250 [==============================] - 0s 725us/step - loss: 5.5828\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.1062\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.667 - 0s 710us/step - loss: 0.6189\n",
      "250/250 [==============================] - 0s 596us/step - loss: 0.0702\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.2510\n",
      "250/250 [==============================] - 0s 589us/step - loss: 0.0637\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.2079\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0562\n",
      "250/250 [==============================] - 1s 714us/step - loss: 0.1452\n",
      "250/250 [==============================] - 0s 597us/step - loss: 0.0499\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0535\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0483\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0575\n",
      "250/250 [==============================] - 0s 576us/step - loss: 0.0575\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0617\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0505\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.3703\n",
      "250/250 [==============================] - 0s 581us/step - loss: 0.0481\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0516\n",
      "250/250 [==============================] - 0s 629us/step - loss: 0.0459\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0626\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0478\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0612\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0495\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.0641\n",
      "250/250 [==============================] - 0s 573us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0566\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 729us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0494\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.05 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.1459\n",
      "250/250 [==============================] - 0s 644us/step - loss: 0.0505\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.0710\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 726us/step - loss: 0.5561\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0575\n",
      "250/250 [==============================] - 0s 751us/step - loss: 2.5532\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0608\n",
      "250/250 [==============================] - 0s 702us/step - loss: 3.8833\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0901\n",
      "250/250 [==============================] - 0s 694us/step - loss: 11.0047\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.1079\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0702\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0512\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.7192\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0630\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.5114\n",
      "250/250 [==============================] - 0s 607us/step - loss: 0.0618\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.1067\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0444\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0463\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0517\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0487\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0789\n",
      "250/250 [==============================] - 0s 567us/step - loss: 0.0436\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0553\n",
      "250/250 [==============================] - 0s 808us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 722us/step - loss: 2.0440\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0594\n",
      "250/250 [==============================] - 0s 726us/step - loss: 1.8945\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1180\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0556\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0518\n",
      "250/250 [==============================] - 0s 702us/step - loss: 11.9654\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.4376\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 3 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.8321\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0853\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.6459\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0643\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.4206\n",
      "250/250 [==============================] - 0s 569us/step - loss: 0.0540\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.2086\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0462\n",
      "250/250 [==============================] - 0s 730us/step - loss: 3.5839\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0653\n",
      "250/250 [==============================] - 0s 701us/step - loss: 0.2535\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0550\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.2220\n",
      "250/250 [==============================] - 0s 548us/step - loss: 0.0626\n",
      "250/250 [==============================] - 0s 714us/step - loss: 2.7833\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0988\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.4029\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0530\n",
      "250/250 [==============================] - 0s 737us/step - loss: 6.6535\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.1679\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 713us/step - loss: 0.0467\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.4519\n",
      "250/250 [==============================] - 0s 605us/step - loss: 0.0576\n",
      "250/250 [==============================] - 1s 718us/step - loss: 0.0495\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0433\n",
      "250/250 [==============================] - 0s 729us/step - loss: 0.0463\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.0465\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0431\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0588\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.0616\n",
      "250/250 [==============================] - 0s 645us/step - loss: 0.0456\n",
      "250/250 [==============================] - 0s 720us/step - loss: 0.0522\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0443 0s - loss: 0.052\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0468\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.1268\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0692\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 4 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.3139\n",
      "250/250 [==============================] - 0s 671us/step - loss: 0.0539\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.150 - 0s 741us/step - loss: 0.1395\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0491\n",
      "250/250 [==============================] - 0s 730us/step - loss: 2.4779\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0704\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.1375\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0416\n",
      "250/250 [==============================] - 0s 728us/step - loss: 38.1014\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.2519\n",
      "250/250 [==============================] - 0s 722us/step - loss: 1.1321\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0537\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0949\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 901us/step - loss: 3.2228\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.1153\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.4793\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0654\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.4422\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0734\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0962\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 763us/step - loss: 3.1433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 610us/step - loss: 0.2027\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0524\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0435\n",
      "250/250 [==============================] - 0s 697us/step - loss: 0.0456\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0414\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0546\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0437\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0416\n",
      "250/250 [==============================] - 1s 983us/step - loss: 2.3809\n",
      "250/250 [==============================] - 0s 746us/step - loss: 0.3480\n",
      "250/250 [==============================] - 0s 952us/step - loss: 0.0506\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0629\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0481\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 5 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1388\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.2505\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0674\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1664\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.1885\n",
      "250/250 [==============================] - 0s 639us/step - loss: 0.0431\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1571\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0420\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.7747\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0578\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0661\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0418\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.5146\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.1226\n",
      "250/250 [==============================] - 1s 991us/step - loss: 50.1423\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.2148\n",
      "250/250 [==============================] - 0s 763us/step - loss: 1.2526\n",
      "250/250 [==============================] - 0s 596us/step - loss: 0.0543\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.0451\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0474\n",
      "250/250 [==============================] - 0s 876us/step - loss: 10.1507\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.3942\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0543\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0450\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0418\n",
      "250/250 [==============================] - 0s 735us/step - loss: 12.6878\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.2536\n",
      "250/250 [==============================] - 1s 738us/step - loss: 0.0589\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 878us/step - loss: 0.0424\n",
      "250/250 [==============================] - 0s 596us/step - loss: 0.0406\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0910\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0557\n",
      "250/250 [==============================] - 0s 739us/step - loss: 1.2066\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0939\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0516\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0470\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 6 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4935\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0556\n",
      "250/250 [==============================] - 0s 702us/step - loss: 2.1573\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1033\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0659\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0425\n",
      "250/250 [==============================] - 0s 741us/step - loss: 2.3673\n",
      "250/250 [==============================] - 0s 677us/step - loss: 0.0462\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.2563\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0793\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.5923\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0452\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.1355\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.2707\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 739us/step - loss: 9.6488\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.1335\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.6960\n",
      "250/250 [==============================] - 0s 651us/step - loss: 0.0510\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 713us/step - loss: 0.0450\n",
      "250/250 [==============================] - 0s 639us/step - loss: 0.0447\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0747\n",
      "250/250 [==============================] - 0s 575us/step - loss: 0.0474\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0450\n",
      "250/250 [==============================] - 0s 639us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.2485\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8109\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0892\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0524\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0471\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0424\n",
      "250/250 [==============================] - 0s 762us/step - loss: 0.0412\n",
      "250/250 [==============================] - 0s 907us/step - loss: 0.0555\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0554\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.0782\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0517\n",
      "250/250 [==============================] - 0s 762us/step - loss: 0.0476\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0479\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 13.7811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 626us/step - loss: 0.0976\n",
      "250/250 [==============================] - 0s 725us/step - loss: 0.2054\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0460\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.6096\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 716us/step - loss: 0.1141\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.9443\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0659\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.1620\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0439\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0964\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0452\n",
      "250/250 [==============================] - 0s 761us/step - loss: 1.1310\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.1470\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.3065\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.9050\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0523\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 727us/step - loss: 0.0455\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0413\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0500\n",
      "250/250 [==============================] - 0s 652us/step - loss: 0.0450\n",
      "250/250 [==============================] - 0s 748us/step - loss: 0.0449\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0417\n",
      "250/250 [==============================] - 0s 736us/step - loss: 0.0456\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0415\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0652\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0479\n",
      "250/250 [==============================] - 0s 766us/step - loss: 0.0612\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0438\n",
      "250/250 [==============================] - 0s 766us/step - loss: 0.0424\n",
      "250/250 [==============================] - 0s 709us/step - loss: 0.0400\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.0501\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0483\n",
      "250/250 [==============================] - 1s 702us/step - loss: 0.0557\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 963us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0577\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 8 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 734us/step - loss: 0.8937\n",
      "250/250 [==============================] - 0s 680us/step - loss: 0.0554\n",
      "250/250 [==============================] - 0s 698us/step - loss: 2.5082\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0848\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.4762\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0451\n",
      "250/250 [==============================] - 0s 865us/step - loss: 0.0871\n",
      "250/250 [==============================] - 0s 592us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.3729\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0574\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.2062\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0534\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1116\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 847us/step - loss: 8.2526\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.1473\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.6065\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0540\n",
      "250/250 [==============================] - 0s 771us/step - loss: 1.2739\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0641\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0467\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0431\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0452\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0412\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0541\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0556\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0438\n",
      "250/250 [==============================] - 0s 768us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0472\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0481\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0419\n",
      "250/250 [==============================] - 0s 717us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0636\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0442\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 765us/step - loss: 0.4289\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.3141\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0484\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.2157\n",
      "250/250 [==============================] - 0s 657us/step - loss: 0.0451\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0636\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0434\n",
      "250/250 [==============================] - 0s 767us/step - loss: 3.4109\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0985\n",
      "250/250 [==============================] - 0s 728us/step - loss: 11.2665\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.2334\n",
      "250/250 [==============================] - 0s 793us/step - loss: 0.1859\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0435\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.2719\n",
      "250/250 [==============================] - 0s 609us/step - loss: 0.0493\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.2114\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0482\n",
      "250/250 [==============================] - 0s 741us/step - loss: 0.6899\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0755\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 738us/step - loss: 10.4603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 606us/step - loss: 0.2167\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0437\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0432\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0402\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1208\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0437\n",
      "250/250 [==============================] - 0s 717us/step - loss: 0.0496\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0510\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0574\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0473\n",
      "250/250 [==============================] - 0s 750us/step - loss: 0.0460\n",
      "250/250 [==============================] - 0s 616us/step - loss: 0.0413\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0466\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0536\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0469\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0411\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0499\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0491\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "第 4 个CS算法开始\n",
      "250/250 [==============================] - 0s 879us/step - loss: 41.7567\n",
      "250/250 [==============================] - 0s 626us/step - loss: 1.3661\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 41.8761\n",
      "250/250 [==============================] - 0s 554us/step - loss: 2.0411\n",
      "250/250 [==============================] - 0s 664us/step - loss: 1390.6278\n",
      "250/250 [==============================] - 0s 709us/step - loss: 82.4915\n",
      "250/250 [==============================] - 0s 658us/step - loss: 2.3844\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.1902\n",
      "250/250 [==============================] - 0s 662us/step - loss: 38.8658\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.6864\n",
      "250/250 [==============================] - 0s 874us/step - loss: 6.8651\n",
      "250/250 [==============================] - 0s 537us/step - loss: 0.3868\n",
      "250/250 [==============================] - 0s 767us/step - loss: 16.5383\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.9382\n",
      "250/250 [==============================] - 0s 694us/step - loss: 44.8398\n",
      "250/250 [==============================] - 0s 662us/step - loss: 1.4995\n",
      "250/250 [==============================] - 0s 739us/step - loss: 53.4626\n",
      "250/250 [==============================] - 0s 550us/step - loss: 1.7905\n",
      "250/250 [==============================] - 0s 740us/step - loss: 98.0811\n",
      "250/250 [==============================] - 0s 585us/step - loss: 1.3222\n",
      "250/250 [==============================] - 0s 626us/step - loss: 8.5375\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.5411\n",
      "250/250 [==============================] - 0s 815us/step - loss: 7.7023\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.4408\n",
      "250/250 [==============================] - 0s 667us/step - loss: 99.7235\n",
      "250/250 [==============================] - 0s 647us/step - loss: 2.4544\n",
      "250/250 [==============================] - 0s 622us/step - loss: 402.4407\n",
      "250/250 [==============================] - 0s 542us/step - loss: 4.9967\n",
      "250/250 [==============================] - 0s 698us/step - loss: 13.7060\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.5571\n",
      "250/250 [==============================] - 0s 843us/step - loss: 33.1132\n",
      "250/250 [==============================] - 0s 642us/step - loss: 1.4061\n",
      "250/250 [==============================] - 0s 671us/step - loss: 3.6657\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.2765\n",
      "250/250 [==============================] - 0s 638us/step - loss: 38.8431\n",
      "250/250 [==============================] - 0s 502us/step - loss: 1.5784\n",
      "250/250 [==============================] - 0s 693us/step - loss: 15.6677\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.6328\n",
      "250/250 [==============================] - 0s 666us/step - loss: 14.2566\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.2058\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.1901676207780838\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.5165\n",
      "250/250 [==============================] - 0s 508us/step - loss: 0.0795\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1787\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0557\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.7233\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.1081\n",
      "250/250 [==============================] - 0s 667us/step - loss: 0.9592\n",
      "250/250 [==============================] - 0s 561us/step - loss: 0.1086\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.6543\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.1169\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.4653\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0968\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.5632\n",
      "250/250 [==============================] - 0s 514us/step - loss: 0.1041\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.9582\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.2006\n",
      "250/250 [==============================] - 0s 708us/step - loss: 0.8949\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.1045\n",
      "250/250 [==============================] - 0s 690us/step - loss: 5.9799\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.3964\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0729\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0627\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0519\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0436\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0994\n",
      "250/250 [==============================] - 0s 526us/step - loss: 0.0649\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.5683\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0685\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.1003\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0634\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0811\n",
      "250/250 [==============================] - 0s 518us/step - loss: 0.0592\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0934\n",
      "250/250 [==============================] - 0s 514us/step - loss: 0.0685\n",
      "250/250 [==============================] - 0s 697us/step - loss: 0.1675\n",
      "250/250 [==============================] - 0s 535us/step - loss: 0.0981\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0800\n",
      "250/250 [==============================] - 0s 529us/step - loss: 0.0522\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.2690\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.1333\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 654us/step - loss: 10.6559\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.3289\n",
      "250/250 [==============================] - 0s 658us/step - loss: 1.1516\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 947us/step - loss: 0.2159\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0615\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.1539\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0640\n",
      "250/250 [==============================] - 1s 794us/step - loss: 0.3146\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0520\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.5721\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0657\n",
      "250/250 [==============================] - 0s 726us/step - loss: 1.8827\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.5351\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0944\n",
      "250/250 [==============================] - 0s 749us/step - loss: 0.5035\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0543\n",
      "250/250 [==============================] - 0s 915us/step - loss: 3.8220\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.2250\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 905us/step - loss: 0.0535\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0475\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0429\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0652\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0517\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0591\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0491\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0561\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 729us/step - loss: 0.0547\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0457\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0563\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0936\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0664\n",
      "250/250 [==============================] - 0s 940us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.1255\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0843\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0925\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0820\n",
      "250/250 [==============================] - 0s 699us/step - loss: 0.0416\n",
      "250/250 [==============================] - 0s 794us/step - loss: 0.4858\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0559\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.1814\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.2293\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0459\n",
      "250/250 [==============================] - 0s 746us/step - loss: 0.9998\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0630\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.2483\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0492\n",
      "250/250 [==============================] - 0s 735us/step - loss: 1.0029\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0772\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0977\n",
      "250/250 [==============================] - 0s 649us/step - loss: 0.0466\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.425 - 0s 718us/step - loss: 0.3922\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0892\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 723us/step - loss: 0.0507\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0442\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0590\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0429\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0528\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0543\n",
      "250/250 [==============================] - 0s 573us/step - loss: 0.0459\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0491\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0436\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0491\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0431\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.0479\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0565\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0690\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0488\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0518\n",
      "250/250 [==============================] - 0s 787us/step - loss: 9.2433\n",
      "250/250 [==============================] - 0s 592us/step - loss: 0.3006\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.7918\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0530\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.1706\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0414\n",
      "250/250 [==============================] - 0s 927us/step - loss: 1.4003\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0507\n",
      "250/250 [==============================] - 0s 981us/step - loss: 0.1852\n",
      "250/250 [==============================] - 0s 663us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 927us/step - loss: 1.3166\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0508\n",
      "250/250 [==============================] - 0s 768us/step - loss: 0.2158\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0438\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.319 - 0s 903us/step - loss: 1.2394\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0540\n",
      "250/250 [==============================] - 1s 747us/step - loss: 0.1197\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0598\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.9746\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0913\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.6999\n",
      "250/250 [==============================] - 0s 601us/step - loss: 0.1298\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 727us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0479\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0436\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 726us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0483\n",
      "250/250 [==============================] - 0s 746us/step - loss: 0.0512\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0486\n",
      "250/250 [==============================] - 0s 673us/step - loss: 0.0434\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0469\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0422\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 88.6537\n",
      "250/250 [==============================] - 0s 662us/step - loss: 2.0153\n",
      "250/250 [==============================] - 0s 771us/step - loss: 26.3434\n",
      "250/250 [==============================] - 0s 634us/step - loss: 1.2139\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0471\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.1129\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0760\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 4 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 722us/step - loss: 3.7340\n",
      "250/250 [==============================] - 0s 623us/step - loss: 0.1288\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0563\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0409\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.9023\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0763\n",
      "250/250 [==============================] - 0s 712us/step - loss: 0.3526\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0528\n",
      "250/250 [==============================] - 0s 751us/step - loss: 1.0100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0609\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.1056\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0437\n",
      "250/250 [==============================] - 0s 707us/step - loss: 1.2339\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.2844\n",
      "250/250 [==============================] - 0s 735us/step - loss: 21.3617\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.6249\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0672\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0416\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.8931\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0805\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 920us/step - loss: 0.0484\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0465\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0404\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0475\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0456\n",
      "250/250 [==============================] - 0s 739us/step - loss: 31.0362\n",
      "250/250 [==============================] - 0s 656us/step - loss: 0.8130\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0436\n",
      "250/250 [==============================] - 0s 730us/step - loss: 10.5264\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.1925\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.2215\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0988\n",
      "250/250 [==============================] - 0s 797us/step - loss: 156.7343\n",
      "250/250 [==============================] - 0s 598us/step - loss: 1.5758\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0464\n",
      "250/250 [==============================] - 0s 625us/step - loss: 0.0423\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0772\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0572\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 781us/step - loss: 5.3846\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0957\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0585\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0410\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.2823\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0499\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.6530\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.1453\n",
      "250/250 [==============================] - 0s 739us/step - loss: 4.0879\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0642\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.3678\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0683\n",
      "250/250 [==============================] - 0s 739us/step - loss: 2.8608\n",
      "250/250 [==============================] - 0s 609us/step - loss: 0.1320\n",
      "250/250 [==============================] - 0s 715us/step - loss: 0.9172\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1449\n",
      "250/250 [==============================] - 0s 801us/step - loss: 1.4003\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0457\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.3554\n",
      "250/250 [==============================] - 0s 745us/step - loss: 0.0726\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 931us/step - loss: 0.0501\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0418\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0422\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0404\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.8261\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0584\n",
      "250/250 [==============================] - 0s 786us/step - loss: 0.1363\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0776\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0475\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0457\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0651\n",
      "250/250 [==============================] - 0s 676us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 792us/step - loss: 0.0955\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0662\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.1270\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0725\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0463\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 743us/step - loss: 4.1684\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.1994\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 1.4285\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0588\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.8892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 622us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.7213\n",
      "250/250 [==============================] - 0s 667us/step - loss: 0.0574\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.6484\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0618\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.3905\n",
      "250/250 [==============================] - 0s 669us/step - loss: 0.0583\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.5728\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0922\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.3594\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0530\n",
      "250/250 [==============================] - 0s 735us/step - loss: 1.3257\n",
      "250/250 [==============================] - 0s 603us/step - loss: 0.0819\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.5553\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 723us/step - loss: 0.7739\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.1196\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.6508\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0578\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0399\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0423\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0577\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0481\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.3223\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0478\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0461\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0493\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.4735\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0655\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0827\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0667\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0454\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0442\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1658\n",
      "250/250 [==============================] - 0s 609us/step - loss: 0.0419\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.2621\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0460\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0968\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 967us/step - loss: 3.5910\n",
      "250/250 [==============================] - 0s 765us/step - loss: 0.1017\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.8421\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0556\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.9942\n",
      "250/250 [==============================] - 0s 607us/step - loss: 0.0551\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.5702\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0524\n",
      "250/250 [==============================] - 0s 837us/step - loss: 2.1351\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.1239\n",
      "250/250 [==============================] - 0s 735us/step - loss: 2.1499\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0735\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.1087\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0438\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0740\n",
      "250/250 [==============================] - 0s 661us/step - loss: 0.0409\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0481\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0416\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0405\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0410\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0499\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0537\n",
      "250/250 [==============================] - 1s 847us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 596us/step - loss: 0.0478\n",
      "250/250 [==============================] - 0s 732us/step - loss: 0.0467\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.0464\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0486\n",
      "250/250 [==============================] - 0s 991us/step - loss: 3.8998\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.2249\n",
      "250/250 [==============================] - 0s 750us/step - loss: 0.0789\n",
      "250/250 [==============================] - 0s 633us/step - loss: 0.0585\n",
      "250/250 [==============================] - 0s 895us/step - loss: 0.0472\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0433\n",
      "250/250 [==============================] - 0s 866us/step - loss: 0.0406\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0410\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.1317\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0428\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.1181\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0425\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.6186\n",
      "250/250 [==============================] - 0s 737us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 775us/step - loss: 2.8157\n",
      "250/250 [==============================] - 0s 641us/step - loss: 0.0978\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.7416\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0619\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.4430\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.3717\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0998\n",
      "250/250 [==============================] - 0s 996us/step - loss: 8.7263\n",
      "250/250 [==============================] - 0s 653us/step - loss: 0.1073\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0638\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0505\n",
      "250/250 [==============================] - 0s 728us/step - loss: 0.0506\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0411\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0429\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0414\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0540\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.0458\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0420\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0404\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0470\n",
      "250/250 [==============================] - 0s 740us/step - loss: 0.0460\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.2105\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0455\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0914\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0743\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0775\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0514\n",
      "250/250 [==============================] - 0s 758us/step - loss: 1.8726\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0454\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0406\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0412\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 729us/step - loss: 0.0998\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0489\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0571\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0404\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0987\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0485\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0443\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0402\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1681\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0462\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.6530\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0689\n",
      "250/250 [==============================] - 0s 757us/step - loss: 0.5318\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0691\n",
      "250/250 [==============================] - 0s 735us/step - loss: 2.1926\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.1841\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.7206\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0458\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0422\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 597us/step - loss: 0.0405\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0403\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0398\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.4267\n",
      "250/250 [==============================] - 0s 597us/step - loss: 0.0531\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0411\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0394\n",
      "250/250 [==============================] - 0s 704us/step - loss: 0.0494\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0408\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 629us/step - loss: 0.0415\n",
      "250/250 [==============================] - 1s 815us/step - loss: 0.0779\n",
      "250/250 [==============================] - 0s 704us/step - loss: 0.0529\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0659\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0533\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0490\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0434\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.0405\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0400\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 10 次迭代最优Loss是 0.04 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "第 1 个CS最优loss为:0.04155!\n",
      "第 2 个CS最优loss为:0.04178!\n",
      "第 3 个CS最优loss为:0.04001!\n",
      "第 4 个CS最优loss为:0.03936!\n"
     ]
    }
   ],
   "source": [
    "# if __name__=='__main__':\n",
    "low = -3*np.ones(numsum)\n",
    "upp = 3*np.ones(numsum)\n",
    "i = 0\n",
    "j = 0\n",
    "best_nest = [] #保存每次CS最佳loss对应的nest，用于赋值给不同神经网络构成多个弱分类器\n",
    "best_loss = []\n",
    "best_fitness = []\n",
    "alpha = [0.22,0.46,0.65,0.82]  #alpha不取0.5,0<alpha<1\n",
    "xn = [0.90,0.70,0.41,0.24]  #xn值不能和alpha值相同，否则将演化为周期系统，就不是混沌系统了\n",
    "for i in range(M):\n",
    "    print('第 %d 个CS算法开始'%(i+1))\n",
    "    nest,loss = cuckoo_search(10,numsum, low,upp, step_size = 0.4,alpha=alpha[i],xn=xn[i])\n",
    "    best_nest.append(nest)\n",
    "    best_loss.append(loss)\n",
    "\n",
    "for j in range(M):    \n",
    "    print('第 %d 个CS最优loss为:%.5f!'%(j+1,best_loss[j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.041550491005182266,\n",
       " 0.041778650134801865,\n",
       " 0.04000695049762726,\n",
       " 0.03936140611767769]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "x = list(range(0,len(LossArr[28:-3])))\n",
    "fig = sns.lineplot(x,LossArr[28:-3])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"fitness\")\n",
    "scatter_fig = fig.get_figure()\n",
    "scatter_fig.savefig('./SO2Loss', dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.1696\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.1504\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.1604\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.1492\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 0s 934us/step - loss: 0.1473\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 0s 948us/step - loss: 0.1277\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.1290\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.1157\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.1205\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.1121\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.1029\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.1065\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0982\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0942\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0920\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0929\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0878\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0852\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.0801\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0780\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0744\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0746\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0686\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0741\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0665\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0661\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0640\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0619\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 0s 770us/step - loss: 0.0613\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0659\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0568\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0580\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0570\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0598\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0647\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0549\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0520\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0523\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0525\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0554\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0538\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0522\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.0501\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0494\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0498\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0501\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0521\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 0s 825us/step - loss: 0.0495\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0503\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0480\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0510\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 0s 877us/step - loss: 0.0493\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0511\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0523\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 0s 794us/step - loss: 0.0483\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.0469\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0457\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0459\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0459\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0447\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0473\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0452\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0452\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0458\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 0s 848us/step - loss: 0.0451\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0447\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 0s 802us/step - loss: 0.0456\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0427\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0442\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0440\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 0s 979us/step - loss: 0.0448\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 0s 932us/step - loss: 0.0438\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 0s 892us/step - loss: 0.0442 0s - loss: 0.\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0449\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0440\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 0s 995us/step - loss: 0.0455\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0439\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0424\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0444\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 0s 987us/step - loss: 0.0429\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0437\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0423\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0432\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 0s 742us/step - loss: 0.0436\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0432\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0415\n",
      "Epoch 88/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0416\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0442\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0428\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0425\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0422\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0424\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0433\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 0s 830us/step - loss: 0.0426\n",
      "Epoch 96/200\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0435\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 751us/step - loss: 0.0414\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0410\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0420\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 0s 782us/step - loss: 0.0420\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0414\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.0428\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0407\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0424\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 0s 979us/step - loss: 0.0411\n",
      "Epoch 107/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0427\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0406\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0417\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0432\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0410\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 0s 786us/step - loss: 0.0421\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 0s 872us/step - loss: 0.0414\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 0s 801us/step - loss: 0.0432\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0405\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0408\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0399\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0416\n",
      "Epoch 121/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0419\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0418\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0416\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 0s 781us/step - loss: 0.0407\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 0s 864us/step - loss: 0.0431\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0413\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0425\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 0s 810us/step - loss: 0.0420\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0407\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0416\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0414\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0414\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0418\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0421\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0412\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0409\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0404\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0402\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0415\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0406\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0408\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0416\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0412\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0423\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0399\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 0s 874us/step - loss: 0.0403\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0417\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0408\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 0s 793us/step - loss: 0.0402\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 0s 856us/step - loss: 0.0406\n",
      "Epoch 151/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0407\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0419\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0401\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0402\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0403\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0405\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0405\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 0s 769us/step - loss: 0.0411\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0403\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0410\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0405\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 0s 946us/step - loss: 0.0416\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0427\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0403\n",
      "Epoch 166/200\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0409\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0412\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0411\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 0s 796us/step - loss: 0.0398\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0398\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0406\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0402\n",
      "Epoch 174/200\n",
      "250/250 [==============================] - 0s 921us/step - loss: 0.0406\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0397\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0424\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0412\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 0s 952us/step - loss: 0.0425\n",
      "Epoch 180/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0417\n",
      "Epoch 181/200\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.0411\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0397\n",
      "Epoch 183/200\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0401\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.0393\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 0s 821us/step - loss: 0.0411\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0411\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0405\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 0s 773us/step - loss: 0.0409\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0412\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0397\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 875us/step - loss: 0.0394\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.0397\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0415\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.0396\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0407\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0399\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0399\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0400\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0406\n",
      "Epoch 1/200\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0389\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0406\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0403\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0415\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0412\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 0s 782us/step - loss: 0.0409\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0406\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0397\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0409\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0402\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0404\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0401\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 0s 826us/step - loss: 0.0403\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0396\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0406\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0401\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0424\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0412\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0398\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0415\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0406\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 0s 987us/step - loss: 0.0411\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0421\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 0s 985us/step - loss: 0.0398\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0409\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0410\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 0s 998us/step - loss: 0.0411\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0393\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0407\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0414\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0395\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.0399\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0397\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0399\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0408\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 0s 857us/step - loss: 0.0402\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.0404\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.0399\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 0s 997us/step - loss: 0.0411\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0398\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0394\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0419\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 0s 896us/step - loss: 0.0403\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 0s 758us/step - loss: 0.0386\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0415\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0398\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 0s 929us/step - loss: 0.0410\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0391\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0404\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0390\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 0s 963us/step - loss: 0.0406\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 0s 979us/step - loss: 0.0396\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 0s 828us/step - loss: 0.0394\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0387\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.0397\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 0s 856us/step - loss: 0.0404\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0400\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.0390\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 0s 792us/step - loss: 0.0404\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0405\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0397\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 0s 848us/step - loss: 0.0400\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0398\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0413\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0399\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 0s 869us/step - loss: 0.0405\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0397\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0394\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0414\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0393\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0399\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0411\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0410\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0406\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0397\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0391\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 0s 895us/step - loss: 0.0403\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0402\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0401\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0400\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 815us/step - loss: 0.0391\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0400\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.0394\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0399\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 0s 832us/step - loss: 0.0400\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 0s 733us/step - loss: 0.0397\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.040 - 0s 923us/step - loss: 0.0400\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.0398\n",
      "Epoch 96/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 97/200\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.0404\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0390\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0398\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.0401\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0402\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0399\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 0s 858us/step - loss: 0.0393\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0409\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 0s 926us/step - loss: 0.0392\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0395\n",
      "Epoch 107/200\n",
      "250/250 [==============================] - 0s 784us/step - loss: 0.0408\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 0s 782us/step - loss: 0.0398\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0391\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0393\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0403\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0392\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0403\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0379\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0400\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0408\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 0s 897us/step - loss: 0.0381\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0389\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0405\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 121/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0399\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0401\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0404\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0406\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0408\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0402\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0410\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 0s 765us/step - loss: 0.0394\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0399\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0411\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 0s 932us/step - loss: 0.0394\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0410\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 0s 809us/step - loss: 0.0400\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.0395\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0404\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0405\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0400\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 0s 757us/step - loss: 0.0397\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0398\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 0s 907us/step - loss: 0.0387\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0395\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0404\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 0s 910us/step - loss: 0.0400\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0403\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0396\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0400\n",
      "Epoch 151/200\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0400\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0398\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0400\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0409\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0394\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0406\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0408\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 0s 782us/step - loss: 0.0397\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0391\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0401\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0404\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0399\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0380\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 0s 785us/step - loss: 0.0399\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0402\n",
      "Epoch 166/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0389\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0399\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0409\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 0s 797us/step - loss: 0.0389\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 0s 829us/step - loss: 0.0406\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 0s 830us/step - loss: 0.0401\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0394\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0388\n",
      "Epoch 174/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 0s 937us/step - loss: 0.0402\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0393\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0397\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0391\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.0393\n",
      "Epoch 180/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0408\n",
      "Epoch 181/200\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.0391\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0402\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 0s 781us/step - loss: 0.0390\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0392\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0397\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0408\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0401\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0383\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0390\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 0s 962us/step - loss: 0.0406\n",
      "Epoch 192/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0396\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0395\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0401\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0398\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.0394\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0416\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0390\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0393\n",
      "Epoch 1/200\n",
      "250/250 [==============================] - 0s 744us/step - loss: 0.0413\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0397\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 0s 942us/step - loss: 0.0400\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0402\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 0s 987us/step - loss: 0.0406\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0398\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0398\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0396\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0414\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0406\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0416\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0422\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 0s 789us/step - loss: 0.0399\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0399\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 0s 756us/step - loss: 0.0407\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 0s 995us/step - loss: 0.0409\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0407\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0413\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0422\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0397\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0404\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0392\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0404\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0398\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.0399\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0385\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.0411\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0404\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 0s 777us/step - loss: 0.0388\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.0405\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0396\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0390\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 0s 785us/step - loss: 0.0400\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0396\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 0s 966us/step - loss: 0.0403\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0399\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0392\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0393\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0385\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 0s 907us/step - loss: 0.0411\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 0s 777us/step - loss: 0.0393\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0382\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0397\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0393\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0391\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 0s 797us/step - loss: 0.0391\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0394\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0396\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0377\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0392\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 0s 906us/step - loss: 0.0406\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0397\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0403\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0400\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 0s 890us/step - loss: 0.0386\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0374\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 0s 757us/step - loss: 0.0397\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0383\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 0s 829us/step - loss: 0.0390\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0397\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0396\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0391\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 0s 920us/step - loss: 0.0381\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0396\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0406\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0406\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.0382\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0398\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0392\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0391\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0386\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0402\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0395\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 831us/step - loss: 0.0387\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0387\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0407\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0398\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0397\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0400\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 0s 782us/step - loss: 0.0394\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 0s 814us/step - loss: 0.0398\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0394\n",
      "Epoch 88/200\n",
      "250/250 [==============================] - 0s 757us/step - loss: 0.0379\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0393\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.0378\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0383\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0389\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 0s 869us/step - loss: 0.0395\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0400\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 96/200\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0393\n",
      "Epoch 97/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0390\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 0s 809us/step - loss: 0.0387\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.0384\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0401\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0397\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0391\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0391\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0386\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0403\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 0s 774us/step - loss: 0.0396\n",
      "Epoch 107/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0405\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.0408\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0397\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.0395\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 0s 963us/step - loss: 0.0397\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0393\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0389\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0385\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 0s 878us/step - loss: 0.0391\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 121/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0387 0s - loss: 0.03\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0406\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0411\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0395\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0391\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0393\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 0s 825us/step - loss: 0.0392\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 0s 948us/step - loss: 0.0398\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0393\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 0s 992us/step - loss: 0.0379\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 0s 892us/step - loss: 0.0384\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0403 0s - loss: 0.04\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 0s 889us/step - loss: 0.0387\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0384\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 0s 929us/step - loss: 0.0394\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 0s 868us/step - loss: 0.0398\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0398\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0390\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0384\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0387\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0395\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0408\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0389\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.0384\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0400\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0396\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0393\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.0387\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 151/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0386\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 0s 941us/step - loss: 0.0396\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 0s 918us/step - loss: 0.0397\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0395\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0387\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 0s 804us/step - loss: 0.0397\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0406\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0407\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0399\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0400\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 0s 895us/step - loss: 0.0393\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0387\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0397\n",
      "Epoch 166/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0402\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0401\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 0s 979us/step - loss: 0.0384\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0394\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0378\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0380\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0391\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.0389\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 873us/step - loss: 0.0387\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 0s 854us/step - loss: 0.0392\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0397\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0393\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0392\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0388\n",
      "Epoch 180/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0389\n",
      "Epoch 181/200\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0386\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0381\n",
      "Epoch 183/200\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.0390\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0387\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0394\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0393\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0375\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0391\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0394\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0392 0s - loss: 0.0\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0377\n",
      "Epoch 192/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0400\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0387\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 0s 963us/step - loss: 0.0397\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0398\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0384\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 0s 945us/step - loss: 0.0384\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 0s 793us/step - loss: 0.0376\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.0385\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.0380\n",
      "Epoch 1/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0381\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0392\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0391\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0396\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0385A: 0s - loss: 0.03\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0392\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0382\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0376\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0388\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0378\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0381\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0392\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0384\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0397\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 0s 912us/step - loss: 0.0371\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0386\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0390\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0375\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 0s 806us/step - loss: 0.0396\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0382\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0390\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0384\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.0376\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0386\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.0382\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0403\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 0s 907us/step - loss: 0.0380\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0383\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0391\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0380\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 0s 987us/step - loss: 0.0394\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0393\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.0391\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0377\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 0s 909us/step - loss: 0.0375\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 0s 883us/step - loss: 0.0387\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 0s 813us/step - loss: 0.0376\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0380\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 0s 918us/step - loss: 0.0386\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.0391\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0381\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0379\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0378\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0381\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 0s 793us/step - loss: 0.0373\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0375\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 0s 907us/step - loss: 0.0388\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0398\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0380\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 0s 797us/step - loss: 0.0380\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0379\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0391\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 0s 823us/step - loss: 0.0382\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 0s 962us/step - loss: 0.0385\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0382\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0371\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0381\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0378\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0376\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0390\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0379\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0379\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0374\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0389\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0392\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0374\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 0s 790us/step - loss: 0.0383\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0398\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0386\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0378\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0377\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0382\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0390\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0396\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 0s 963us/step - loss: 0.0369\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0382\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0386\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 88/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0376\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 0s 904us/step - loss: 0.0389\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 0s 949us/step - loss: 0.0374\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0373\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 0s 913us/step - loss: 0.0374\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 96/200\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.0379\n",
      "Epoch 97/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0379\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0368\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 0s 916us/step - loss: 0.0382\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.0371\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0365\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0375\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0389\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0382\n",
      "Epoch 107/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0374\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0379\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0385\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 0s 895us/step - loss: 0.0386\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0383\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0375\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 0s 894us/step - loss: 0.0380\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 0s 811us/step - loss: 0.0369\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0373\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0376\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0384\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.0370\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 0s 889us/step - loss: 0.0384\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 0s 891us/step - loss: 0.0371\n",
      "Epoch 121/200\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0374\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 0s 857us/step - loss: 0.0391\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 0s 880us/step - loss: 0.0366\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0366\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 0s 895us/step - loss: 0.0372\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0383\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0366\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0367\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 0s 809us/step - loss: 0.0381\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 0s 955us/step - loss: 0.0369\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 0s 943us/step - loss: 0.0371\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 0s 903us/step - loss: 0.0380\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0383\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 0s 794us/step - loss: 0.0376\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.0373\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0374\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.0388\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0382\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.0374\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 0s 961us/step - loss: 0.0368\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0378\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0388\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0368\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 0s 950us/step - loss: 0.0363\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0373\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0382\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0376\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.0386\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0376\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0369\n",
      "Epoch 151/200\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0384\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0369\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0373\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0377\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 0s 819us/step - loss: 0.0383\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.0378\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 0s 865us/step - loss: 0.0364\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0383\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0369\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0386\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0377\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 0s 760us/step - loss: 0.0368\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 778us/step - loss: 0.0381\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 0s 762us/step - loss: 0.0384\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0367\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0378\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0376\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0382\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0379\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0382\n",
      "Epoch 174/200\n",
      "250/250 [==============================] - 0s 713us/step - loss: 0.0374\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0369\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0379\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0372\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0380\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.0385\n",
      "Epoch 180/200\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0374\n",
      "Epoch 181/200\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0391\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0391\n",
      "Epoch 183/200\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0376\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0363\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 0s 954us/step - loss: 0.0379\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0378\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0371\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0374\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 0s 866us/step - loss: 0.0372\n",
      "Epoch 192/200\n",
      "250/250 [==============================] - 0s 721us/step - loss: 0.0381\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0372\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.0393\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0364\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0372\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0388\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 0s 996us/step - loss: 0.0375\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0384\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0374\n"
     ]
    }
   ],
   "source": [
    "Models = []\n",
    "for i in range(0,M):\n",
    "    chrom = best_nest[i]\n",
    "    w1 = chrom[:inputnum*hiddennum]\n",
    "    w1 = w1.reshape(inputnum,hiddennum)\n",
    "    b1 = chrom[inputnum*hiddennum:inputnum*hiddennum+hiddennum]\n",
    "    w2 = chrom[inputnum*hiddennum+hiddennum:inputnum*hiddennum+hiddennum+hiddennum*outputnum]\n",
    "    w2 = w2.reshape(hiddennum,outputnum)\n",
    "    b2 = chrom[inputnum*hiddennum+hiddennum+hiddennum*outputnum:]\n",
    "\n",
    "    WB_layer1 = (w1,b1)\n",
    "    WB_layer2 = (w2,b2)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(hiddnum1,name='layer1',activation='relu'),\n",
    "        #keras.layers.Dense(hiddnum2,activation='relu'),\n",
    "        #keras.layers.Dense(5,activation = 'relu'),\n",
    "        keras.layers.Dense(outputnum,name='layer2')\n",
    "        ])\n",
    "\n",
    "\n",
    "    model.build(input_shape=[None,inputnum])\n",
    "    #model.summary()\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.0005),\n",
    "                loss='mse',)\n",
    "\n",
    "    layer1 = model.get_layer('layer1')\n",
    "    layer2 = model.get_layer('layer2')\n",
    "    layer1.set_weights(WB_layer1)\n",
    "    layer2.set_weights(WB_layer2)\n",
    "\n",
    "\n",
    "    model.fit(SO2X,SO2Y,epochs=100)  #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    Models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 3ms/step - loss: 1.0993\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2869\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "第 1 个模型的预测结果为：\n",
      "\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "accuracy is : 0.06132\n",
      "\n",
      "第 2 个模型的预测结果为：\n",
      "\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0719\n",
      "accuracy is : 0.07193\n",
      "\n",
      "第 3 个模型的预测结果为：\n",
      "\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2869\n",
      "accuracy is : 0.28690\n",
      "\n",
      "第 4 个模型的预测结果为：\n",
      "\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0993\n",
      "accuracy is : 1.09928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#将弱分类器按准确率从小到大排序\n",
    "CSMODEL = []\n",
    "acc = []\n",
    "for i in range(0,M):\n",
    "    accuracy = Models[i].evaluate(SO2TestX,SO2TestY)\n",
    "    acc.append(accuracy)\n",
    "    \n",
    "accIdx = np.argsort(acc)  #模型准确度从大到小排序，保证强分类器至少不会弱于最好的弱分类器\n",
    "#accIdx = accIdx[::-1]  #取消注释从大到小排序\n",
    "\n",
    "for i in accIdx:\n",
    "    CSMODEL.append(Models[i])\n",
    "\n",
    "for i in range(0,M):\n",
    "    print('第 %d 个模型的预测结果为：\\r\\n'%(i+1))\n",
    "    accuracy = CSMODEL[i].evaluate(SO2TestX,SO2TestY)\n",
    " \n",
    "    print('accuracy is : %.5f\\r\\n'%(accuracy))\n",
    "Models = CSMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3618677   0.3144081   0.0790117   0.24844661  0.14350691  0.06947266\n",
      "  0.29586476 -0.01577614 -0.01994617  0.17344114 -0.01771707 -0.02653205\n",
      " -0.08246011 -0.0663026   0.00158554  0.05634566  0.1564345   0.12589267\n",
      "  0.22196946  0.22855166]\n"
     ]
    }
   ],
   "source": [
    "pre = Models[3].predict(SO2TestX)  #！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "pre = np.squeeze(pre)\n",
    "print(pre[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10944269 -0.04366533 -0.05904972 -0.06602409 -0.06522834 -0.08768451\n",
      " -0.23952923 -0.10755113 -0.24353942 -0.18938537 -0.11500999  0.0243351\n",
      " -0.11446136 -0.10544683 -0.10152236 -0.07846077 -0.06038195  0.55767579\n",
      "  0.10956857 -0.07761408]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(SO2TestY)[:20])#！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_train = []\n",
    "# 初始化数据权重\n",
    "w_data = np.ones(n_train) / n_train\n",
    "# 初始化模型权重\n",
    "w_model = np.zeros(M)\n",
    "miss = []\n",
    "#train_y = tf.argmax(Y_onehot,axis=1)  #train_y保存了原始训练集中标签经过onehot编码后的结果\n",
    "#k = 3 #分类数量\n",
    "for i in range(M):\n",
    "    \n",
    "    #求第i个弱分类器训练集结果\n",
    "    y_pred_train = Models[i].predict(SO2X)  #三元概率\n",
    "    #y_pred_train = tf.argmax(y_pred_train,axis=1)#去最大概率的位置为预测值\n",
    "    \n",
    "    #求第i个弱分类器误差\n",
    "    for k in range(0,len(y_pred_train)):\n",
    "        if abs(y_pred_train[k] - SO2Y) > 2:\n",
    "            miss.append(0)\n",
    "        else:\n",
    "            miss.append(1)\n",
    "    #miss = [int(x) for x in range(0,len(y_pred_train)) (y_pred_train != train_y)]  #不相等则保存1，相等则保存0\n",
    "    error =np.dot(w_data, miss) #累加识别错误的样本权重，得到分类器误差\n",
    "    print('第 %d 个模型的误差是：%.2f'%(i,error))\n",
    "    #求第i个弱分类器权值，保存到w_model中\n",
    "    #a = 1/2 * log(1-e/e) + log(k-1),当k = 2 时为二分类更新权值公式不用修改，否则为多分类，算法准确率大于1/k即可\n",
    "    #该函数若准确率大于0.5（1-error）则值为正，否则为负值，越大说明模型分类越好\n",
    "    w_model[i] = 0.5 * np.log((1-error)/error) + np.log(k - 1)  \n",
    "    \n",
    "    # 更新数据权重 \n",
    "    #分类结果和真实的结果一致，那么结果是−w_model[m]，是一个负值，\n",
    "    #那么exp(-w_model[m]*train_y[i]*y_pred_train[i]) 结果小于1。也就是说该数据集的样本权重降低。否则该数据样本的权重增高。\n",
    "    #通过这种计算就可以让那些容易分错的样本的权重升高，容易分对的样本权重降低。继续迭代就会导致对难分的样本能分对的模型的权重上涨。\n",
    "    #最终，达到一个强分类器的目的。\n",
    "    #注意，这里只适合二分类【1，-1】\n",
    "    #多分类公式修改 wt = wt-1 * exp(at * (y_true!=y_pred)) \n",
    "    miss1 = np.array(miss)\n",
    "    miss1 = w_model[i]*miss1\n",
    "    for j in range(n_train):\n",
    "        w_data[j] = w_data[j] * np.exp(miss1[j])  #*train_y[i]*y_pred_train[i] #二分类时用这个\n",
    "    \n",
    "    #正则化数据权值\n",
    "    Z = np.sum(w_data)\n",
    "    for j in range(n_train):\n",
    "        w_data[j] /= Z\n",
    "\n",
    "#结果这个模块以后将得到每个模型的权值，保存在w_model中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#弱分类器权值归一化\n",
    "w_model = np.array(w_model / np.sum(w_model))\n",
    "w_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四问开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！最后一个数字\n",
    "predata = pd.read_csv('preData.csv',sep=',',header=0,usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "predata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preX = predata.iloc[:,[0,1,3,4,5,6,8,10,12]] \n",
    "SO2 = np.array(predata['SO2'])\n",
    "preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,5,6,8,12]] \n",
    "# NO2 = np.array(predata['NO2'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,4,6,8,10,12]] \n",
    "# PM10 = np.array(predata['PM10'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,5,6,12]] \n",
    "# PM2_5 = np.array(predata['PM2.5'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,5,6,12]] \n",
    "# O3 = np.array(predata['O3'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,6,8]] \n",
    "# CO = np.array(predata['CO'])\n",
    "# preX.head()\n",
    "\n",
    "\n",
    "# SO2X = data.iloc[:7900,[6,7,9,10,11,12,14,16,18]] \n",
    "# SO2TestX = data.iloc[7900:,[6,7,9,10,11,12,14,16,18]]\n",
    "# NO2X = data.iloc[:7900,[6,7,9,10,11,12,14,18]]\n",
    "# NO2TestX =data.iloc[7900:,[6,7,9,10,11,12,14,18]]\n",
    "# PM10X =data.iloc[:7900,[6,7,10,12,14,16,18]]\n",
    "# PM10TestX =data.iloc[7900:,[6,7,10,12,14,16,18]]\n",
    "# PM2_5X =data.iloc[:7900,[6,7,9,10,11,12,18]]\n",
    "# PM2_5TestX =data.iloc[7900:,[6,7,9,10,11,12,18]]\n",
    "# O3X =data.iloc[:7900,[6,7,10,12,14,16,18]]\n",
    "# O3TestX =data.iloc[7900:,[6,7,10,12,14,16,18]]\n",
    "# COX =data.iloc[:7900,[6,7,9,10,12,14]]\n",
    "# COTestX =data.iloc[7900:,[6,7,9,10,12,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#零均值处理\n",
    "def data_progress(X):\n",
    "    for i in range(0,X.shape[1]):\n",
    "        X.iloc[:,i] -= np.mean(X,axis=0)[i]\n",
    "        X.iloc[:,i] /= np.max(np.abs(X),axis=0)[i]\n",
    "\n",
    "    return 0\n",
    "data_progress(preX)\n",
    "preX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测到数据后反归一化\n",
    "pre_two = model.predict(preX)\n",
    "pre_two = np.squeeze(pre_two)\n",
    "pre_two = np.array(pre_two)\n",
    "pre_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_two = pre_two*PM10max      #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "pre_two = pre_two+PM10mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加一次预测结果，得到最终结果\n",
    "result = pre_two + PM10  #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "result1 = result[0:24]\n",
    "result2 = result[24:48]\n",
    "result3 = result[48:72]\n",
    "result4 = result[72:96]\n",
    "result5 = result[96:120]\n",
    "result6 = result[120:]\n",
    "len(result1),len(result2),len(result3),len(result4),len(result5),len(result6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去负值，对最终预测数据进行处理\n",
    "index1 = []\n",
    "index2 = []\n",
    "index3 = []\n",
    "index4 = []\n",
    "index5 = []\n",
    "index6 = []\n",
    "for i in range(0,len(result1)):\n",
    "    if result1[i] <= 0 :\n",
    "        index1.append(i)\n",
    "for i in range(0,len(result2)):\n",
    "    if result2[i] <= 0 :\n",
    "        index2.append(i)\n",
    "for i in range(0,len(result3)):\n",
    "    if result3[i] <= 0 :\n",
    "        index3.append(i)\n",
    "for i in range(0,len(result4)):\n",
    "    if result4[i] <= 0 :\n",
    "        index4.append(i)\n",
    "for i in range(0,len(result5)):\n",
    "    if result5[i] <= 0 :\n",
    "        index5.append(i)\n",
    "for i in range(0,len(result6)):\n",
    "    if result6[i] <= 0 :\n",
    "        index6.append(i)\n",
    "\n",
    "result1 = np.delete(result1, index1)\n",
    "result2 = np.delete(result2, index2)\n",
    "result3 = np.delete(result3,index3)\n",
    "result4 = np.delete(result4,index4)\n",
    "result5 = np.delete(result5,index5)\n",
    "result6 = np.delete(result6,index6)\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #去除3个最大最小值\n",
    "\n",
    "result1 = np.sort(result1)[3:-3]\n",
    "result2 = np.sort(result2)[3:-3]\n",
    "result3 = np.sort(result3)[3:-3]\n",
    "result4 = np.sort(result4)[3:-3]\n",
    "result5 = np.sort(result5)[3:-3]\n",
    "result6 = np.sort(result6)[3:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result4),len(result5),len(result6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(result1)/len(result1),sum(result2)/len(result2),sum(result3)/len(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(result4)/len(result4),sum(result5)/len(result5),sum(result6)/len(result6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
