{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sc_special\n",
    "import tensorflow as tf \n",
    "from \ttensorflow import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras import optimizers,metrics,layers,Sequential\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time1</th>\n",
       "      <th>Time2</th>\n",
       "      <th>localation</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>w_speed</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 0:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.8890</td>\n",
       "      <td>304.016</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>66.7409</td>\n",
       "      <td>4.16382</td>\n",
       "      <td>162.577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94818</td>\n",
       "      <td>428.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.401510</td>\n",
       "      <td>20.9208</td>\n",
       "      <td>8.17336</td>\n",
       "      <td>5.27729</td>\n",
       "      <td>8.78723</td>\n",
       "      <td>0.124491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 1:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.8736</td>\n",
       "      <td>303.739</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>62.1551</td>\n",
       "      <td>4.65267</td>\n",
       "      <td>171.978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.14987</td>\n",
       "      <td>427.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.448340</td>\n",
       "      <td>14.8144</td>\n",
       "      <td>6.49054</td>\n",
       "      <td>4.33106</td>\n",
       "      <td>12.74530</td>\n",
       "      <td>0.109056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 2:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.6471</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>64.1760</td>\n",
       "      <td>4.10031</td>\n",
       "      <td>172.013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.01616</td>\n",
       "      <td>427.428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.271610</td>\n",
       "      <td>13.9154</td>\n",
       "      <td>6.86679</td>\n",
       "      <td>4.40045</td>\n",
       "      <td>12.22960</td>\n",
       "      <td>0.105957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 3:00</td>\n",
       "      <td>A</td>\n",
       "      <td>29.4555</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>68.7958</td>\n",
       "      <td>2.44317</td>\n",
       "      <td>168.135</td>\n",
       "      <td>0.047224</td>\n",
       "      <td>...</td>\n",
       "      <td>1.89003</td>\n",
       "      <td>442.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467429</td>\n",
       "      <td>11.1535</td>\n",
       "      <td>5.25900</td>\n",
       "      <td>3.35261</td>\n",
       "      <td>13.78000</td>\n",
       "      <td>0.101764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020/7/23</td>\n",
       "      <td>2020/7/23 4:00</td>\n",
       "      <td>A</td>\n",
       "      <td>28.5189</td>\n",
       "      <td>302.987</td>\n",
       "      <td>0.019881</td>\n",
       "      <td>76.5791</td>\n",
       "      <td>2.57759</td>\n",
       "      <td>207.884</td>\n",
       "      <td>8.260020</td>\n",
       "      <td>...</td>\n",
       "      <td>6.53753</td>\n",
       "      <td>458.394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574856</td>\n",
       "      <td>13.9989</td>\n",
       "      <td>6.05979</td>\n",
       "      <td>3.59303</td>\n",
       "      <td>9.96333</td>\n",
       "      <td>0.104536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time1           Time2 localation  temp_2m   temp_0        BS     humi  \\\n",
       "0  2020/7/23  2020/7/23 0:00          A  29.8890  304.016  0.018871  66.7409   \n",
       "1  2020/7/23  2020/7/23 1:00          A  29.8736  303.739  0.017556  62.1551   \n",
       "2  2020/7/23  2020/7/23 2:00          A  29.6471  303.419  0.017874  64.1760   \n",
       "3  2020/7/23  2020/7/23 3:00          A  29.4555  303.419  0.018935  68.7958   \n",
       "4  2020/7/23  2020/7/23 4:00          A  28.5189  302.987  0.019881  76.5791   \n",
       "\n",
       "   w_speed    w_dir      rain  ...     QRTL     CBFS  DBFS  sunFS       SO2  \\\n",
       "0  4.16382  162.577  0.000000  ...  0.94818  428.278   0.0    0.0  2.401510   \n",
       "1  4.65267  171.978  0.000000  ...  1.14987  427.531   0.0    0.0  1.448340   \n",
       "2  4.10031  172.013  0.000000  ...  1.01616  427.428   0.0    0.0  1.271610   \n",
       "3  2.44317  168.135  0.047224  ...  1.89003  442.472   0.0    0.0  0.467429   \n",
       "4  2.57759  207.884  8.260020  ...  6.53753  458.394   0.0    0.0  0.574856   \n",
       "\n",
       "       NO2     PM10    PM2.5        O3        CO  \n",
       "0  20.9208  8.17336  5.27729   8.78723  0.124491  \n",
       "1  14.8144  6.49054  4.33106  12.74530  0.109056  \n",
       "2  13.9154  6.86679  4.40045  12.22960  0.105957  \n",
       "3  11.1535  5.25900  3.35261  13.78000  0.101764  \n",
       "4  13.9989  6.05979  3.59303   9.96333  0.104536  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('A-A1-A2-A3.csv',sep=',',header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data)):\n",
    "    if data['Time2'][i].split('/')[2].split(' ')[0] != data['Time1'][i].split('/')[2]:\n",
    "        data.drop(index=[i],inplace = True)\n",
    "data.to_csv('NewThree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33888"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO21</th>\n",
       "      <th>NO21</th>\n",
       "      <th>PM101</th>\n",
       "      <th>PM2.51</th>\n",
       "      <th>O31</th>\n",
       "      <th>CO1</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>...</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.8890</td>\n",
       "      <td>304.016</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>66.7409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94818</td>\n",
       "      <td>428.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.401510</td>\n",
       "      <td>20.9208</td>\n",
       "      <td>8.17336</td>\n",
       "      <td>5.27729</td>\n",
       "      <td>8.78723</td>\n",
       "      <td>0.124491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.8736</td>\n",
       "      <td>303.739</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>62.1551</td>\n",
       "      <td>...</td>\n",
       "      <td>1.14987</td>\n",
       "      <td>427.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.448340</td>\n",
       "      <td>14.8144</td>\n",
       "      <td>6.49054</td>\n",
       "      <td>4.33106</td>\n",
       "      <td>12.74530</td>\n",
       "      <td>0.109056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.6471</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>64.1760</td>\n",
       "      <td>...</td>\n",
       "      <td>1.01616</td>\n",
       "      <td>427.428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.271610</td>\n",
       "      <td>13.9154</td>\n",
       "      <td>6.86679</td>\n",
       "      <td>4.40045</td>\n",
       "      <td>12.22960</td>\n",
       "      <td>0.105957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.4</td>\n",
       "      <td>29.4555</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>68.7958</td>\n",
       "      <td>...</td>\n",
       "      <td>1.89003</td>\n",
       "      <td>442.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467429</td>\n",
       "      <td>11.1535</td>\n",
       "      <td>5.25900</td>\n",
       "      <td>3.35261</td>\n",
       "      <td>13.78000</td>\n",
       "      <td>0.101764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.7843</td>\n",
       "      <td>302.383</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>72.2798</td>\n",
       "      <td>...</td>\n",
       "      <td>3.81680</td>\n",
       "      <td>429.953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.764980</td>\n",
       "      <td>21.7531</td>\n",
       "      <td>13.43040</td>\n",
       "      <td>7.64338</td>\n",
       "      <td>3.38709</td>\n",
       "      <td>0.144370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SO21  NO21  PM101  PM2.51  O31  CO1  temp_2m   temp_0        BS     humi  \\\n",
       "0     4    12      3       3   23  0.4  29.8890  304.016  0.018871  66.7409   \n",
       "1     3    11      3       1   24  0.4  29.8736  303.739  0.017556  62.1551   \n",
       "2     4    13     14       2   20  0.4  29.6471  303.419  0.017874  64.1760   \n",
       "3     4    12     12       1   20  0.4  29.4555  303.419  0.018935  68.7958   \n",
       "4     5    19      6       5   11  0.5  28.7843  302.383  0.019065  72.2798   \n",
       "\n",
       "   ...     QRTL     CBFS  DBFS  sunFS       SO2      NO2      PM10    PM2.5  \\\n",
       "0  ...  0.94818  428.278   0.0    0.0  2.401510  20.9208   8.17336  5.27729   \n",
       "1  ...  1.14987  427.531   0.0    0.0  1.448340  14.8144   6.49054  4.33106   \n",
       "2  ...  1.01616  427.428   0.0    0.0  1.271610  13.9154   6.86679  4.40045   \n",
       "3  ...  1.89003  442.472   0.0    0.0  0.467429  11.1535   5.25900  3.35261   \n",
       "4  ...  3.81680  429.953   0.0    0.0  1.764980  21.7531  13.43040  7.64338   \n",
       "\n",
       "         O3        CO  \n",
       "0   8.78723  0.124491  \n",
       "1  12.74530  0.109056  \n",
       "2  12.22960  0.105957  \n",
       "3  13.78000  0.101764  \n",
       "4   3.38709  0.144370  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('four.csv',sep=',',header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4. , 12. ,  3. ,  3. , 23. ,  0.4],\n",
       "       [ 3. , 11. ,  3. ,  1. , 24. ,  0.4],\n",
       "       [ 4. , 13. , 14. ,  2. , 20. ,  0.4],\n",
       "       ...,\n",
       "       [ 7. , 16. ,  9. ,  2. ,  9. ,  0.4],\n",
       "       [ 4. , 16. ,  7. ,  2. ,  9. ,  0.4],\n",
       "       [ 4. ,  7. , 14. ,  5. , 23. ,  0.4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollution1 = np.array(data.iloc[:,0:6])\n",
    "pollution2 = np.array(data.iloc[:,-6:])\n",
    "pollution1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存污染物相减后的数据，在将之前处理的预测环境数据手动添加进去\n",
    "pol = pd.DataFrame(pollution1-pollution2,columns = ['SO2','NO2','PM10','PM2.5','O3','CO'])\n",
    "pol.head()\n",
    "pol.to_csv('DataFour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入处理好的数据，若已处理好代码从这里开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>...</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloud</th>\n",
       "      <th>high</th>\n",
       "      <th>press</th>\n",
       "      <th>GRTL</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.598490</td>\n",
       "      <td>-8.9208</td>\n",
       "      <td>-5.17336</td>\n",
       "      <td>-2.27729</td>\n",
       "      <td>14.21277</td>\n",
       "      <td>0.275509</td>\n",
       "      <td>29.8890</td>\n",
       "      <td>304.016</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>66.7409</td>\n",
       "      <td>...</td>\n",
       "      <td>162.577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038201</td>\n",
       "      <td>769.903</td>\n",
       "      <td>100.665</td>\n",
       "      <td>20.13770</td>\n",
       "      <td>0.94818</td>\n",
       "      <td>428.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.551660</td>\n",
       "      <td>-3.8144</td>\n",
       "      <td>-3.49054</td>\n",
       "      <td>-3.33106</td>\n",
       "      <td>11.25470</td>\n",
       "      <td>0.290944</td>\n",
       "      <td>29.8736</td>\n",
       "      <td>303.739</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>62.1551</td>\n",
       "      <td>...</td>\n",
       "      <td>171.978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058892</td>\n",
       "      <td>682.085</td>\n",
       "      <td>100.671</td>\n",
       "      <td>15.83330</td>\n",
       "      <td>1.14987</td>\n",
       "      <td>427.531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.728390</td>\n",
       "      <td>-0.9154</td>\n",
       "      <td>7.13321</td>\n",
       "      <td>-2.40045</td>\n",
       "      <td>7.77040</td>\n",
       "      <td>0.294043</td>\n",
       "      <td>29.6471</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.017874</td>\n",
       "      <td>64.1760</td>\n",
       "      <td>...</td>\n",
       "      <td>172.013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065039</td>\n",
       "      <td>627.078</td>\n",
       "      <td>100.678</td>\n",
       "      <td>12.33630</td>\n",
       "      <td>1.01616</td>\n",
       "      <td>427.428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.532571</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>6.74100</td>\n",
       "      <td>-2.35261</td>\n",
       "      <td>6.22000</td>\n",
       "      <td>0.298236</td>\n",
       "      <td>29.4555</td>\n",
       "      <td>303.419</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>68.7958</td>\n",
       "      <td>...</td>\n",
       "      <td>168.135</td>\n",
       "      <td>0.047224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>636.497</td>\n",
       "      <td>100.675</td>\n",
       "      <td>11.12150</td>\n",
       "      <td>1.89003</td>\n",
       "      <td>442.472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.235020</td>\n",
       "      <td>-2.7531</td>\n",
       "      <td>-7.43040</td>\n",
       "      <td>-2.64338</td>\n",
       "      <td>7.61291</td>\n",
       "      <td>0.355630</td>\n",
       "      <td>28.7843</td>\n",
       "      <td>302.383</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>72.2798</td>\n",
       "      <td>...</td>\n",
       "      <td>196.949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378593</td>\n",
       "      <td>378.039</td>\n",
       "      <td>100.772</td>\n",
       "      <td>6.92296</td>\n",
       "      <td>3.81680</td>\n",
       "      <td>429.953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SO2     NO2     PM10    PM2.5        O3        CO  temp_2m   temp_0  \\\n",
       "0  1.598490 -8.9208 -5.17336 -2.27729  14.21277  0.275509  29.8890  304.016   \n",
       "1  1.551660 -3.8144 -3.49054 -3.33106  11.25470  0.290944  29.8736  303.739   \n",
       "2  2.728390 -0.9154  7.13321 -2.40045   7.77040  0.294043  29.6471  303.419   \n",
       "3  3.532571  0.8465  6.74100 -2.35261   6.22000  0.298236  29.4555  303.419   \n",
       "4  3.235020 -2.7531 -7.43040 -2.64338   7.61291  0.355630  28.7843  302.383   \n",
       "\n",
       "         BS     humi  ...    w_dir      rain     cloud     high    press  \\\n",
       "0  0.018871  66.7409  ...  162.577  0.000000  0.038201  769.903  100.665   \n",
       "1  0.017556  62.1551  ...  171.978  0.000000  0.058892  682.085  100.671   \n",
       "2  0.017874  64.1760  ...  172.013  0.000000  0.065039  627.078  100.678   \n",
       "3  0.018935  68.7958  ...  168.135  0.047224  1.000000  636.497  100.675   \n",
       "4  0.019065  72.2798  ...  196.949  0.000000  0.378593  378.039  100.772   \n",
       "\n",
       "       GRTL     QRTL     CBFS  DBFS  sunFS  \n",
       "0  20.13770  0.94818  428.278   0.0    0.0  \n",
       "1  15.83330  1.14987  427.531   0.0    0.0  \n",
       "2  12.33630  1.01616  427.428   0.0    0.0  \n",
       "3  11.12150  1.89003  442.472   0.0    0.0  \n",
       "4   6.92296  3.81680  429.953   0.0    0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据的各个Y是差值（实测值 - 一次预测值）\n",
    "data = pd.read_csv('DataFour.csv',sep=',',header=0)\n",
    "#data = data.sample(frac = 1) #打乱数据\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32498"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>...</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloud</th>\n",
       "      <th>high</th>\n",
       "      <th>press</th>\n",
       "      <th>GRTL</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "      <td>32498.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.094233</td>\n",
       "      <td>-15.939938</td>\n",
       "      <td>17.465385</td>\n",
       "      <td>-2.504328</td>\n",
       "      <td>17.786499</td>\n",
       "      <td>0.421860</td>\n",
       "      <td>24.916934</td>\n",
       "      <td>301.778926</td>\n",
       "      <td>0.013047</td>\n",
       "      <td>59.221886</td>\n",
       "      <td>...</td>\n",
       "      <td>124.234901</td>\n",
       "      <td>0.399914</td>\n",
       "      <td>0.465743</td>\n",
       "      <td>676.290666</td>\n",
       "      <td>101.253019</td>\n",
       "      <td>92.886432</td>\n",
       "      <td>16.606632</td>\n",
       "      <td>395.988812</td>\n",
       "      <td>208.787297</td>\n",
       "      <td>250.884365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>7.479913</td>\n",
       "      <td>29.151128</td>\n",
       "      <td>37.532859</td>\n",
       "      <td>30.707256</td>\n",
       "      <td>35.559385</td>\n",
       "      <td>0.262027</td>\n",
       "      <td>5.980101</td>\n",
       "      <td>8.561029</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>16.636423</td>\n",
       "      <td>...</td>\n",
       "      <td>82.821606</td>\n",
       "      <td>2.035541</td>\n",
       "      <td>0.328914</td>\n",
       "      <td>454.716321</td>\n",
       "      <td>0.646240</td>\n",
       "      <td>107.334489</td>\n",
       "      <td>19.380946</td>\n",
       "      <td>50.285896</td>\n",
       "      <td>273.216779</td>\n",
       "      <td>328.304546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-103.061000</td>\n",
       "      <td>-429.778000</td>\n",
       "      <td>-148.909000</td>\n",
       "      <td>-124.171000</td>\n",
       "      <td>-264.976000</td>\n",
       "      <td>-2.033030</td>\n",
       "      <td>2.584230</td>\n",
       "      <td>275.045000</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>9.069630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.575400</td>\n",
       "      <td>99.713600</td>\n",
       "      <td>-23.560700</td>\n",
       "      <td>-1.390840</td>\n",
       "      <td>229.260000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-3.115165</td>\n",
       "      <td>-30.841725</td>\n",
       "      <td>1.845700</td>\n",
       "      <td>-12.536525</td>\n",
       "      <td>-0.000526</td>\n",
       "      <td>0.283547</td>\n",
       "      <td>21.241975</td>\n",
       "      <td>296.019000</td>\n",
       "      <td>0.009172</td>\n",
       "      <td>47.172650</td>\n",
       "      <td>...</td>\n",
       "      <td>51.667050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152484</td>\n",
       "      <td>326.816250</td>\n",
       "      <td>100.698000</td>\n",
       "      <td>10.046575</td>\n",
       "      <td>1.089850</td>\n",
       "      <td>361.367000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.401255</td>\n",
       "      <td>-12.325500</td>\n",
       "      <td>15.626820</td>\n",
       "      <td>0.382690</td>\n",
       "      <td>13.973738</td>\n",
       "      <td>0.397692</td>\n",
       "      <td>25.933200</td>\n",
       "      <td>301.836500</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>59.219050</td>\n",
       "      <td>...</td>\n",
       "      <td>129.652000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454778</td>\n",
       "      <td>550.314000</td>\n",
       "      <td>101.252000</td>\n",
       "      <td>37.869700</td>\n",
       "      <td>6.393675</td>\n",
       "      <td>409.094000</td>\n",
       "      <td>24.071450</td>\n",
       "      <td>28.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>4.356280</td>\n",
       "      <td>1.052725</td>\n",
       "      <td>30.490700</td>\n",
       "      <td>8.474420</td>\n",
       "      <td>33.257918</td>\n",
       "      <td>0.547795</td>\n",
       "      <td>29.218950</td>\n",
       "      <td>307.314000</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>73.053650</td>\n",
       "      <td>...</td>\n",
       "      <td>180.807000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782031</td>\n",
       "      <td>973.546000</td>\n",
       "      <td>101.741000</td>\n",
       "      <td>166.994250</td>\n",
       "      <td>29.836850</td>\n",
       "      <td>436.387000</td>\n",
       "      <td>390.929500</td>\n",
       "      <td>469.751250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>58.261800</td>\n",
       "      <td>172.880700</td>\n",
       "      <td>947.092100</td>\n",
       "      <td>975.544690</td>\n",
       "      <td>269.230060</td>\n",
       "      <td>11.433869</td>\n",
       "      <td>37.532300</td>\n",
       "      <td>323.987000</td>\n",
       "      <td>0.023053</td>\n",
       "      <td>100.703000</td>\n",
       "      <td>...</td>\n",
       "      <td>359.997000</td>\n",
       "      <td>98.740700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2314.100000</td>\n",
       "      <td>102.997000</td>\n",
       "      <td>554.955000</td>\n",
       "      <td>77.052600</td>\n",
       "      <td>491.671000</td>\n",
       "      <td>893.002000</td>\n",
       "      <td>1073.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                SO2           NO2          PM10         PM2.5            O3  \\\n",
       "count  32498.000000  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean       0.094233    -15.939938     17.465385     -2.504328     17.786499   \n",
       "std        7.479913     29.151128     37.532859     30.707256     35.559385   \n",
       "min     -103.061000   -429.778000   -148.909000   -124.171000   -264.976000   \n",
       "25%       -3.115165    -30.841725      1.845700    -12.536525     -0.000526   \n",
       "50%        1.401255    -12.325500     15.626820      0.382690     13.973738   \n",
       "75%        4.356280      1.052725     30.490700      8.474420     33.257918   \n",
       "max       58.261800    172.880700    947.092100    975.544690    269.230060   \n",
       "\n",
       "                 CO       temp_2m        temp_0            BS          humi  \\\n",
       "count  32498.000000  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean       0.421860     24.916934    301.778926      0.013047     59.221886   \n",
       "std        0.262027      5.980101      8.561029      0.005260     16.636423   \n",
       "min       -2.033030      2.584230    275.045000      0.000647      9.069630   \n",
       "25%        0.283547     21.241975    296.019000      0.009172     47.172650   \n",
       "50%        0.397692     25.933200    301.836500      0.013393     59.219050   \n",
       "75%        0.547795     29.218950    307.314000      0.017922     73.053650   \n",
       "max       11.433869     37.532300    323.987000      0.023053    100.703000   \n",
       "\n",
       "       ...         w_dir          rain         cloud          high  \\\n",
       "count  ...  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean   ...    124.234901      0.399914      0.465743    676.290666   \n",
       "std    ...     82.821606      2.035541      0.328914    454.716321   \n",
       "min    ...      0.004486      0.000000      0.000000     19.575400   \n",
       "25%    ...     51.667050      0.000000      0.152484    326.816250   \n",
       "50%    ...    129.652000      0.000000      0.454778    550.314000   \n",
       "75%    ...    180.807000      0.000000      0.782031    973.546000   \n",
       "max    ...    359.997000     98.740700      1.000000   2314.100000   \n",
       "\n",
       "              press          GRTL          QRTL          CBFS          DBFS  \\\n",
       "count  32498.000000  32498.000000  32498.000000  32498.000000  32498.000000   \n",
       "mean     101.253019     92.886432     16.606632    395.988812    208.787297   \n",
       "std        0.646240    107.334489     19.380946     50.285896    273.216779   \n",
       "min       99.713600    -23.560700     -1.390840    229.260000      0.000000   \n",
       "25%      100.698000     10.046575      1.089850    361.367000      0.000000   \n",
       "50%      101.252000     37.869700      6.393675    409.094000     24.071450   \n",
       "75%      101.741000    166.994250     29.836850    436.387000    390.929500   \n",
       "max      102.997000    554.955000     77.052600    491.671000    893.002000   \n",
       "\n",
       "              sunFS  \n",
       "count  32498.000000  \n",
       "mean     250.884365  \n",
       "std      328.304546  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%       28.924900  \n",
       "75%      469.751250  \n",
       "max     1073.060000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32498"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(0,len(data)):\n",
    "    if  (data['SO2'][i] <= -15)|(data['SO2'][i]>=15):\n",
    "        index.append(i)\n",
    "    elif (data['NO2'][i] <= -25) | (data['NO2'][i] >=25):\n",
    "        index.append(i)\n",
    "    elif (data['PM10'][i] <= -25) | (data['PM10'][i] >= 25):\n",
    "        index.append(i)\n",
    "    elif (data['PM2.5'][i] <= -20) | (data['PM2.5'][i] >= 20):\n",
    "        index.append(i)\n",
    "    elif (data['O3'][i] <= -25) | (data['O3'][i] >=25):\n",
    "        index.append(i)\n",
    "    elif (data['CO'][i] >=3):\n",
    "        index.append(i)\n",
    "data.drop(index=index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24320"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8177, 21)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(index=index,inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常值删除结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4 #模型数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各污染物浓度分别建模，自变量X不变（所有环节情况）\n",
    "SO2X = data.iloc[:8000,[6,7,9,10,11,12,14,16,18]] \n",
    "SO2TestX = data.iloc[8000:,[6,7,9,10,11,12,14,16,18]]\n",
    "NO2X = data.iloc[:8000,[6,7,9,10,11,12,14,18]]\n",
    "NO2TestX =data.iloc[8000:,[6,7,9,10,11,12,14,18]]\n",
    "PM10X =data.iloc[:8000,[6,7,10,12,14,16,18]]\n",
    "PM10TestX =data.iloc[8000:,[6,7,10,12,14,16,18]]\n",
    "PM2_5X =data.iloc[:8000,[6,7,9,10,11,12,18]]\n",
    "PM2_5TestX =data.iloc[8000:,[6,7,9,10,11,12,18]]\n",
    "O3X =data.iloc[:8000,[6,7,10,12,14,16,18]]\n",
    "O3TestX =data.iloc[8000:,[6,7,10,12,14,16,18]]\n",
    "COX =data.iloc[:8000,[6,7,9,10,12,14]]\n",
    "COTestX =data.iloc[8000:,[6,7,9,10,12,14]]\n",
    "n_train = SO2X.shape[0]\n",
    "\n",
    "SO2Y = data.iloc[:8000,0]\n",
    "SO2TestY = data.iloc[8000:,0]\n",
    "NO2Y = data.iloc[:8000,1]\n",
    "NO2TestY = data.iloc[8000:,1]\n",
    "PM10Y = data.iloc[:8000,2]\n",
    "PM10TestY = data.iloc[8000:,2]\n",
    "PM2_5Y = data.iloc[:8000,3]\n",
    "PM2_5TestY = data.iloc[8000:,3]\n",
    "O3Y = data.iloc[:8000,4]\n",
    "O3TestY = data.iloc[8000:,4]\n",
    "COY = data.iloc[:8000,5]\n",
    "COTestY = data.iloc[8000:,5]\n",
    "\n",
    "inputnums = [SO2X.shape[1],NO2X.shape[1],PM10X.shape[1],PM2_5X.shape[1],O3X.shape[1],COX.shape[1]]  #!!!!!!!!!!!!!!!!!!输入层结构\n",
    "inputnum = inputnums[5]\n",
    "outputnum = 1\n",
    "hiddnum1 = 15\n",
    "hiddennum = hiddnum1 \n",
    "numsum = inputnum*hiddennum+hiddennum+hiddennum*outputnum+outputnum #6*10+10+10*3+3=103\n",
    "\n",
    "LossArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#零均值处理\n",
    "\n",
    "SO2max = np.max(np.abs(SO2Y),axis=0)\n",
    "SO2mean = np.mean(SO2Y,axis=0)\n",
    "NO2max = np.max(np.abs(NO2Y),axis=0)\n",
    "NO2mean = np.mean(NO2Y,axis=0)\n",
    "PM10max = np.max(np.abs(PM10Y),axis=0)\n",
    "PM10mean = np.mean(PM10Y,axis=0)\n",
    "PM2_5max = np.max(np.abs(PM2_5Y),axis=0)\n",
    "PM2_5mean = np.mean(PM2_5Y,axis=0)\n",
    "O3max = np.max(np.abs(O3Y),axis=0)\n",
    "O3mean = np.mean(O3Y,axis=0)\n",
    "COmax = np.max(np.abs(COY),axis=0)\n",
    "COmean = np.mean(COY,axis=0)\n",
    "def data_progress(X,TestX):\n",
    "    for i in range(0,X.shape[1]):\n",
    "        X.iloc[:,i] -= np.mean(X,axis=0)[i]\n",
    "        X.iloc[:,i] /= np.max(np.abs(X),axis=0)[i]\n",
    "        TestX.iloc[:,i] -= np.mean(TestX,axis=0)[i]\n",
    "        TestX.iloc[:,i] /= np.max(np.abs(TestX),axis=0)[i]\n",
    "        \n",
    "    return 0\n",
    "def data_progressY(Y,TestY):\n",
    "    Y -= np.mean(Y,axis=0)\n",
    "    Y /= np.max(np.abs(Y),axis=0)\n",
    "    TestY -= np.mean(TestY,axis=0)\n",
    "    TestY /= np.max(np.abs(TestY),axis=0)\n",
    "    return 1\n",
    "    \n",
    "data_progress(SO2X,SO2TestX)\n",
    "data_progressY(SO2Y,SO2TestY)\n",
    "data_progress(NO2X,NO2TestX)\n",
    "data_progressY(NO2Y,NO2TestY)\n",
    "data_progress(PM10X,PM10TestX)\n",
    "data_progressY(PM10Y,PM10TestY)\n",
    "data_progress(PM2_5X,PM2_5TestX)\n",
    "data_progressY(PM2_5Y,PM2_5TestY)\n",
    "data_progress(O3X,O3TestX)\n",
    "data_progressY(O3Y,O3TestY)\n",
    "data_progress(COX,COTestX)\n",
    "data_progressY(COY,COTestY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8605603169241648e-16"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.97744 2.9475783538887463 24.9993 -4.679705722500016 24.9965 11.227338726375043 19.9902 0.6359887460000033 24.99999156 5.7181293480728375\n"
     ]
    }
   ],
   "source": [
    "print(SO2max,SO2mean,NO2max,NO2mean,PM10max,PM10mean,PM2_5max,PM2_5mean,O3max,O3mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#n：布谷鸟数  m：布谷鸟的维度\n",
    "def cuckoo_search(n, m, lower_boundary, upper_boundary, iter_num = 10,pa = 0.25, beta = 1.5, step_size = 0.1,alpha=0.77,xn=0.33):\n",
    "\n",
    "    num=1\n",
    "    # get initial nests' locations \n",
    "    nests,best_nest,best_fitness,lossness = generate_nests(n, m,alpha,xn, lower_boundary, upper_boundary) #alpha,xn用于tent初始化阈值和初值设置\n",
    "    \n",
    "    # get the best nest and record it\n",
    "    LossArr.append(best_fitness) #全局变量，保存每代最佳适应度值\n",
    "    \n",
    "\n",
    "    print('\\r\\n BEST_LOSSNESS IS %.2f : \\r\\n',best_fitness)\n",
    "\n",
    "    for _ in range(iter_num):\n",
    "        \n",
    "        print('\\r\\n******************************************************第 %d 代开始迭代优化************************************************************\\r\\n'%num)\n",
    "        nests = update_nests(lower_boundary, upper_boundary, nests, best_nest, lossness, step_size,best_fitness)\n",
    "        nests = abandon_nests(nests, lower_boundary, upper_boundary, pa)\n",
    "        \n",
    "        print('\\r\\n*****************************************************第 %d 次迭代，计算适应度********************************************************\\r\\n'%num)\n",
    "        lossness,_ = calc_fitness( nests)\n",
    "        print('\\r\\n*********************************************************第 %d 次迭代结束************************************************************\\r\\n'%num)\n",
    "        \n",
    "        min_loss_index = np.argmin(lossness)\n",
    "        min_loss = lossness[min_loss_index]\n",
    "        min_nestloss = nests[min_loss_index]\n",
    "        LossArr.append(min_loss)\n",
    "            \n",
    "        if min_loss < best_fitness  : #and  min_loss_fit > best_two_fitness\n",
    "            best_nest = min_nestloss\n",
    "            best_fitness = min_loss\n",
    "            print('\\r\\n******')\n",
    "            print('\\r\\n 第 %d 次迭代最优Loss是 %.2f : \\r\\n'%(num,best_fitness))\n",
    "            print('\\r\\n******\\r\\n')\n",
    "        num+=1\n",
    "\n",
    "    return (best_nest, best_fitness)\n",
    "\n",
    "# #随机生成nest\n",
    "#     lower_boundary = np.array(lower_boundary)\n",
    "#     upper_boundary = np.array(upper_boundary)\n",
    "#     nests = np.empty((n, m))\n",
    "\n",
    "#     for each_nest in range(n):\n",
    "#         nests[each_nest] = lower_boundary + np.array([np.random.rand() for _ in range(m)]) * (upper_boundary - lower_boundary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nests(n, m,alpha,xn,upper_boundary , lower_boundary):\n",
    "\n",
    "#Tent混沌反向初始化\n",
    "\n",
    "# #混沌初始化\n",
    "    nests = np.empty((n, m))\n",
    "    sig_nest = np.empty(m)\n",
    "    alpha = alpha\n",
    "    xn = xn\n",
    "    for i in range(0,n):   #*2 值域为【-1,1】 *6 值域为[-3,3]\n",
    "        for j in range(0,m):\n",
    "            if 0<=xn<alpha:\n",
    "                xn = xn/alpha\n",
    "                sig_nest[j]=(xn-0.5)*6\n",
    "            elif alpha <= xn <= 1:\n",
    "                xn = (1-xn)/(1-alpha)\n",
    "                sig_nest[j] = (xn-0.5)*6\n",
    "            nests[i] = sig_nest\n",
    "            \n",
    "# #反向初始化            \n",
    "    renests = -1 * nests   #定义：Pi = ai + bi - pi  生成反向nests\n",
    "    \n",
    "# #拼接两个初始化nests\n",
    "    nests = np.vstack((nests,renests))  #拼接nests和renests 准备计算适应度选择最优的n个nest\n",
    "    \n",
    "#计算适应度\n",
    "    lossness,_ = calc_fitness( nests) \n",
    "    \n",
    "# #根据loss值排序\n",
    "    arrIndex = np.argsort(lossness)   #获得排序数组  从小到大\n",
    "    lossness = lossness[arrIndex]     #将lossness数组按照从小到大排序\n",
    "    nests = nests[arrIndex]    #将nests也按照相同序列进行排序，保证和lossness对齐\n",
    "    \n",
    "#删除多余的n组nest，这里从最底下开始一个个删，因为已经排好序了，所以删除的为效果最差的\n",
    "    for i in range(n):\n",
    "        nests = np.delete(nests,-1,0)\n",
    "        lossness = np.delete(lossness,-1,0)\n",
    "#现在的nests是按照loss排序的，第一个loss最小\n",
    "\n",
    "    return nests,nests[0],lossness[0],lossness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nests(lower_boundary, upper_boundary, nests, best_nest, lossness, step_coefficient,bestfitness):\n",
    "\n",
    "    lower_boundary = np.array(lower_boundary)\n",
    "    upper_boundary = np.array(upper_boundary)\n",
    "    n, m = nests.shape\n",
    "    # 莱维飞行生成步长：较长时间的短步长和较短时间的长步长\n",
    "    #生成的步长的shape也是n*m，保证所有元素得到更新\n",
    "    #这个步长通过step_size来控制大小\n",
    "    steps = levy_flight(n, m, 1.5)\n",
    "    new_nests = nests.copy()\n",
    "\n",
    "    for each_nest in range(n):\n",
    "        # coefficient 0.01 is to avoid levy flights becoming too aggresive\n",
    "        # and (nest[each_nest] - best_nest) 保留了最佳nest，但会导致局部最优\n",
    "        step_size = step_coefficient * steps[each_nest] # * (nests[each_nest] - best_nest)\n",
    "        step_direction = np.random.rand(m) #0-1均匀概率分布生成方向\n",
    "        #新的布谷鸟在原来的基础上加上步长和方向\n",
    "        new_nests[each_nest] += step_size * step_direction  \n",
    "        # apply boundary condtions\n",
    "        new_nests[each_nest][new_nests[each_nest] < lower_boundary] = lower_boundary[new_nests[each_nest] < lower_boundary]\n",
    "        new_nests[each_nest][new_nests[each_nest] > upper_boundary] = upper_boundary[new_nests[each_nest] > upper_boundary]\n",
    "\n",
    "    new_losses,new_nests = calc_fitness(new_nests)\n",
    "    #适应度更好的才更新过去\n",
    "    \n",
    "    nests[new_losses < lossness] = new_nests[new_losses < lossness] \n",
    "    return nests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abandon_nests(nests, lower_boundary, upper_boundary, pa):\n",
    "\n",
    "    lower_boundary = np.array(lower_boundary)\n",
    "    upper_boundary = np.array(upper_boundary)\n",
    "    n, m = nests.shape\n",
    "    for each_nest in range(n):  \n",
    "        #pa概率抛弃，抛弃后重新生成\n",
    "        if (np.random.rand() < pa):\n",
    "            #局部随机行走生成步长\n",
    "            #随机两个种群相差*一个0-1的随机数\n",
    "            step_size = np.random.rand() * (nests[np.random.randint(0, n)] - nests[np.random.randint(0, n)])\n",
    "            nests[each_nest] += step_size\n",
    "            # apply boundary condtions\n",
    "            nests[each_nest][nests[each_nest] < lower_boundary] = lower_boundary[nests[each_nest] < lower_boundary]\n",
    "            nests[each_nest][nests[each_nest] > upper_boundary] = upper_boundary[nests[each_nest] > upper_boundary]\n",
    "    \n",
    "    return nests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levy_flight(n, m, beta):\n",
    "\n",
    "    sigma_u = (sc_special.gamma(1+beta)*np.sin(np.pi*beta/2)/(sc_special.gamma((1+beta)/2)*beta*(2**((beta-1)/2))))**(1/beta)\n",
    "    sigma_v = 1\n",
    "\n",
    "    u =  np.random.normal(0, sigma_u, (n, m))\n",
    "    v = np.random.normal(0, sigma_v, (n, m))\n",
    "\n",
    "    steps = u/((np.abs(v))**(1/beta))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fitness(nests):\n",
    "    \n",
    "    n, m = nests.shape\n",
    "    lossness = np.empty(n)\n",
    "    new_nests = nests\n",
    "    for Sig_nest in range(n):\n",
    "        chrom = nests[Sig_nest]\n",
    "        #分解模型参数\n",
    "        w1 = chrom[:inputnum*hiddennum]\n",
    "        w1 = w1.reshape(inputnum,hiddennum)\n",
    "        b1 = chrom[inputnum*hiddennum:inputnum*hiddennum+hiddennum]\n",
    "        w2 = chrom[inputnum*hiddennum+hiddennum:inputnum*hiddennum+hiddennum+hiddennum*outputnum]\n",
    "        w2 = w2.reshape(hiddennum,outputnum)\n",
    "        b2 = chrom[inputnum*hiddennum+hiddennum+hiddennum*outputnum:]\n",
    "\n",
    "        WB_l1 = (w1,b1)\n",
    "        WB_l2 = (w2,b2)\n",
    "        #创建模型，并赋予参数\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(hiddennum,activation='relu',name='l1'),\n",
    "            keras.layers.Dense(outputnum,name='l2')\n",
    "        ])\n",
    "        model.build(input_shape=[None,inputnum])\n",
    "        \n",
    "        #model.summary()\n",
    "        model.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='mse',)\n",
    "        \n",
    "        layer1 = model.get_layer('l1')\n",
    "        layer2 = model.get_layer('l2')\n",
    "        layer1.set_weights(WB_l1)\n",
    "        layer2.set_weights(WB_l2)\n",
    "        \n",
    "        #训练模型\n",
    "        #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！下面两句输入数据集\n",
    "        model.fit(COX,COY,epochs=1)\n",
    "        loss = model.evaluate(COX,COY)  #这里是用验证集评估，其实应该用测试集吧 evaluate产生两个结果，前面是损失，后面是准确率\n",
    "        lossness[Sig_nest] = loss\n",
    "        \n",
    "        \n",
    "        (k1,y1) = layer1.get_weights()  #获取训练后的神经网络权值，并赋值给c\n",
    "        (k2,y2) = layer2.get_weights()\n",
    "        c=k1.reshape(1,-1).tolist()[0] + y1.reshape(1,-1).tolist()[0] + k2.reshape(1,-1).tolist()[0] + y2.reshape(1,-1).tolist()[0]\n",
    "        new_nests[Sig_nest] = c\n",
    "        \n",
    "        \n",
    "#         if(acc>bestfitness): #在计算适应度的过程当中，发现有更好的适应度就把参数保存\n",
    "#             #model.save_weights('my_model_fun.h5')\n",
    "#             bestfitness = acc\n",
    "    return lossness,new_nests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 个CS算法开始\n",
      "250/250 [==============================] - 0s 722us/step - loss: 36.4026\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.2572\n",
      "250/250 [==============================] - 0s 726us/step - loss: 1.4662\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.1169\n",
      "250/250 [==============================] - 1s 771us/step - loss: 16.0155\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.4114\n",
      "250/250 [==============================] - 0s 927us/step - loss: 17.4007\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.5888\n",
      "250/250 [==============================] - 0s 682us/step - loss: 7.3486\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.2969\n",
      "250/250 [==============================] - 0s 803us/step - loss: 85.9588\n",
      "250/250 [==============================] - 0s 767us/step - loss: 1.1280\n",
      "250/250 [==============================] - 0s 714us/step - loss: 50.5993\n",
      "250/250 [==============================] - 0s 650us/step - loss: 1.6307\n",
      "250/250 [==============================] - 0s 779us/step - loss: 79.5070\n",
      "250/250 [==============================] - 0s 638us/step - loss: 1.2122\n",
      "250/250 [==============================] - 0s 787us/step - loss: 10.7089\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.2333\n",
      "250/250 [==============================] - 0s 755us/step - loss: 8.1443\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.1823\n",
      "250/250 [==============================] - 0s 771us/step - loss: 5.6166\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1664\n",
      "250/250 [==============================] - 0s 759us/step - loss: 24.9386\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.8330\n",
      "250/250 [==============================] - 0s 714us/step - loss: 11.1230\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2235\n",
      "250/250 [==============================] - 0s 743us/step - loss: 36.9688\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.2585\n",
      "250/250 [==============================] - 0s 694us/step - loss: 15.9433\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.4896\n",
      "250/250 [==============================] - 0s 735us/step - loss: 99.7309\n",
      "250/250 [==============================] - 0s 602us/step - loss: 1.8786\n",
      "250/250 [==============================] - 0s 686us/step - loss: 80.3312\n",
      "250/250 [==============================] - 0s 562us/step - loss: 1.8557\n",
      "250/250 [==============================] - 0s 751us/step - loss: 7.9709\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1160\n",
      "250/250 [==============================] - 0s 755us/step - loss: 19.2573\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.4903\n",
      "250/250 [==============================] - 0s 738us/step - loss: 18.2360\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.2081\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.11601122468709946\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.4852\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0207\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1882\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0237\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.765 - 0s 743us/step - loss: 0.6622\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0170\n",
      "250/250 [==============================] - 0s 847us/step - loss: 1.2066\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0456\n",
      "250/250 [==============================] - 1s 739us/step - loss: 0.2519\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0386\n",
      "250/250 [==============================] - 0s 743us/step - loss: 3.3823\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.2466\n",
      "250/250 [==============================] - 0s 714us/step - loss: 2.3016\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1053\n",
      "250/250 [==============================] - 0s 907us/step - loss: 0.9335\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0180\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.2188\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0306\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.8220\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.1282\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0173\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0228\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0169\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0152\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0111\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0318\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0340\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0156\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.4330\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0576\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0822\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0381\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0144\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 763us/step - loss: 36.3791\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8313\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0963\n",
      "250/250 [==============================] - 0s 935us/step - loss: 0.0390\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.5553\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0198\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.6377\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0699\n",
      "250/250 [==============================] - 0s 690us/step - loss: 13.4527\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.1084\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0453\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.2036\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0119\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.7068\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0300\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.3032\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0220\n",
      "250/250 [==============================] - 0s 887us/step - loss: 8.5640\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0350\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.8088\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.1040\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.3345\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0352\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0099\n",
      "250/250 [==============================] - 0s 787us/step - loss: 110.2554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 658us/step - loss: 1.0843\n",
      "250/250 [==============================] - 0s 982us/step - loss: 0.0144\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 698us/step - loss: 4.9672\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.2777\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0252\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0131\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0248\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 735us/step - loss: 17.4425\n",
      "250/250 [==============================] - 0s 590us/step - loss: 1.0232\n",
      "250/250 [==============================] - 0s 698us/step - loss: 78.0974\n",
      "250/250 [==============================] - 0s 747us/step - loss: 2.4913\n",
      "250/250 [==============================] - 1s 739us/step - loss: 0.0384\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0260\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 799us/step - loss: 3.5908\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0511\n",
      "250/250 [==============================] - 0s 855us/step - loss: 10.3119\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.4406\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1451\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.7909\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0633\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.2267\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0211\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1812\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0154\n",
      "250/250 [==============================] - 0s 775us/step - loss: 1.0101\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0530\n",
      "250/250 [==============================] - 0s 714us/step - loss: 1.6721\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.1585\n",
      "250/250 [==============================] - 0s 694us/step - loss: 2.0113\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.2739\n",
      "250/250 [==============================] - 0s 895us/step - loss: 1.1718\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0794\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 911us/step - loss: 310.9019\n",
      "250/250 [==============================] - 0s 574us/step - loss: 2.3174\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.2928\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0929\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 17.1667\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.5272\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0507\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0236\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0093\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 690us/step - loss: 5.3451\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0621\n",
      "250/250 [==============================] - 1s 775us/step - loss: 0.0763\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0154\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.2428\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.1928\n",
      "250/250 [==============================] - 0s 767us/step - loss: 52.0284\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.8993\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 3 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 771us/step - loss: 3.6617\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.7280\n",
      "250/250 [==============================] - 0s 731us/step - loss: 31.5654\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.3163\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.7776\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.1404\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1533\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0227\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.1018\n",
      "250/250 [==============================] - 0s 534us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.5182\n",
      "250/250 [==============================] - 0s 681us/step - loss: 0.0152\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.7110\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0409\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.2299\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 722us/step - loss: 4.3114\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.1719\n",
      "250/250 [==============================] - 0s 931us/step - loss: 2.6502\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.1731\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.6238\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2205\n",
      "250/250 [==============================] - 0s 747us/step - loss: 24.7937\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.9267\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.1058\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0350\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0186\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 730us/step - loss: 3.6788\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1806\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0102\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0328\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0171\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0250\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1436\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0765\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.1683\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0994\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 78.1192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 735us/step - loss: 0.3883\n",
      "250/250 [==============================] - 0s 811us/step - loss: 1.9818\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.3306\n",
      "250/250 [==============================] - 0s 678us/step - loss: 19.5595\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0504\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.1295\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0144\n",
      "250/250 [==============================] - 0s 710us/step - loss: 6.4388\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.1749\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.4972\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0208\n",
      "250/250 [==============================] - 0s 803us/step - loss: 2.9194\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0659\n",
      "250/250 [==============================] - 0s 759us/step - loss: 1.6470\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0167\n",
      "250/250 [==============================] - 0s 670us/step - loss: 1.0012\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0454\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.9395\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0628\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2160\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.1091\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.3044\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.1299\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0413\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0182\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0131\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0186\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 815us/step - loss: 58.1148\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.5032\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0161\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0132\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0220\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0602\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0274\n",
      "250/250 [==============================] - 0s 706us/step - loss: 9.4757\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.3551\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 710us/step - loss: 3.1026\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.1236\n",
      "250/250 [==============================] - 0s 714us/step - loss: 1.6183\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.1358\n",
      "250/250 [==============================] - 0s 686us/step - loss: 4.4973\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0475\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.4954\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0315\n",
      "250/250 [==============================] - 0s 694us/step - loss: 8.0503\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0860\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.5997\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0840\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1973\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0145\n",
      "250/250 [==============================] - 0s 726us/step - loss: 3.3786\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0188\n",
      "250/250 [==============================] - 0s 682us/step - loss: 2.6556\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0318\n",
      "250/250 [==============================] - 0s 690us/step - loss: 3.6861\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.2521\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 799us/step - loss: 0.0989\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0633\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1186\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0600\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0268\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0173\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0106\n",
      "250/250 [==============================] - 1s 706us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0740\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0321\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 674us/step - loss: 1.6074\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0215\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0458\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0202\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2283\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0719\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.1252\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0779\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.2874\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0383\n",
      "250/250 [==============================] - 0s 658us/step - loss: 1.7327\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0197\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1040\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.1321\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0244\n",
      "250/250 [==============================] - 0s 670us/step - loss: 1.2736\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0799\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1831\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 698us/step - loss: 2.4243\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0234\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.4250\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0227\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.5209\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0380\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0860\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 674us/step - loss: 0.0498\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0317\n",
      "250/250 [==============================] - 1s 674us/step - loss: 0.0253\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0111\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0161\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0362\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0175\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0096\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0284\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0162\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0390\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0687\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0186\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 10.0491\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.2575\n",
      "250/250 [==============================] - 0s 686us/step - loss: 4.3958\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.1042\n",
      "250/250 [==============================] - 0s 658us/step - loss: 2.0284\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0150\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.1586\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 722us/step - loss: 1.0325\n",
      "250/250 [==============================] - 0s 568us/step - loss: 0.0227\n",
      "250/250 [==============================] - 0s 706us/step - loss: 9.5783\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.4663\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.3197\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 682us/step - loss: 7.1100\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0249\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.3860\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0189\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.5182\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0187\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0704\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0469\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0356\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0217\n",
      "250/250 [==============================] - 0s 690us/step - loss: 22.8480\n",
      "250/250 [==============================] - 0s 550us/step - loss: 1.1764\n",
      "250/250 [==============================] - 0s 674us/step - loss: 3.4034\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0332\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0254\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0277\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0329\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.9644\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0190\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 694us/step - loss: 2.8045\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0626\n",
      "250/250 [==============================] - 0s 686us/step - loss: 3.6149\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0540\n",
      "250/250 [==============================] - 0s 686us/step - loss: 3.2253\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.4148\n",
      "250/250 [==============================] - 0s 694us/step - loss: 1.2444\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0167\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.6710\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0570\n",
      "250/250 [==============================] - 0s 706us/step - loss: 24.3935\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1955\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.3886\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0188\n",
      "250/250 [==============================] - 0s 710us/step - loss: 3.4807\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.1524\n",
      "250/250 [==============================] - 0s 662us/step - loss: 2.8190\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0161\n",
      "250/250 [==============================] - 0s 726us/step - loss: 7.4227\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0854\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0551\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0422\n",
      "250/250 [==============================] - 0s 714us/step - loss: 318.7040\n",
      "250/250 [==============================] - 0s 546us/step - loss: 4.2066\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.2864\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0657\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0221\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0092\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0200\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0180\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0119\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0297\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0159\n",
      "250/250 [==============================] - 0s 662us/step - loss: 34.8588\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.3833\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0452\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0185\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 9 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 706us/step - loss: 71.1472\n",
      "250/250 [==============================] - 0s 594us/step - loss: 1.4944\n",
      "250/250 [==============================] - 0s 707us/step - loss: 5.2660\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.4027\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.9620\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0911\n",
      "250/250 [==============================] - 0s 662us/step - loss: 1.8042\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0122\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0451\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.6867\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0257\n",
      "250/250 [==============================] - 0s 666us/step - loss: 3.5356\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0367\n",
      "250/250 [==============================] - 0s 678us/step - loss: 1.6813\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0201\n",
      "250/250 [==============================] - 1s 658us/step - loss: 1.8338\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.1016\n",
      "250/250 [==============================] - 0s 763us/step - loss: 30.1997\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1092\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0600\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0334\n",
      "250/250 [==============================] - 0s 702us/step - loss: 2.2993\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0790\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0971\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0387\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0200\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 694us/step - loss: 1.8925\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0315\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0188\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0130\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0512\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0146\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0367\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0165\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "第 2 个CS算法开始\n",
      "250/250 [==============================] - 0s 686us/step - loss: 8.0176\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.2514\n",
      "250/250 [==============================] - 0s 686us/step - loss: 102.9123\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.6814\n",
      "250/250 [==============================] - 0s 658us/step - loss: 41.1181\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.9190\n",
      "250/250 [==============================] - 0s 670us/step - loss: 219.7428\n",
      "250/250 [==============================] - 0s 610us/step - loss: 4.0203\n",
      "250/250 [==============================] - 0s 690us/step - loss: 19.0967\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.3291\n",
      "250/250 [==============================] - 0s 702us/step - loss: 3.2976\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.1128\n",
      "250/250 [==============================] - 1s 694us/step - loss: 106.3720\n",
      "250/250 [==============================] - 0s 570us/step - loss: 2.0120\n",
      "250/250 [==============================] - 0s 670us/step - loss: 60.1694\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.5643\n",
      "250/250 [==============================] - 0s 662us/step - loss: 35.0103\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.5623\n",
      "250/250 [==============================] - 0s 674us/step - loss: 29.9343\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.6051\n",
      "250/250 [==============================] - 0s 670us/step - loss: 44.8751\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.4407\n",
      "250/250 [==============================] - 0s 686us/step - loss: 10.3918\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1044\n",
      "250/250 [==============================] - 0s 755us/step - loss: 15.9938\n",
      "250/250 [==============================] - 0s 594us/step - loss: 1.1156\n",
      "250/250 [==============================] - 0s 678us/step - loss: 2.6665\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0587\n",
      "250/250 [==============================] - 0s 702us/step - loss: 24.4816\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.4201\n",
      "250/250 [==============================] - 0s 678us/step - loss: 85.1756\n",
      "250/250 [==============================] - 0s 566us/step - loss: 1.4117\n",
      "250/250 [==============================] - 0s 694us/step - loss: 19.2083\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.7649\n",
      "250/250 [==============================] - 0s 678us/step - loss: 2.8797\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0690\n",
      "250/250 [==============================] - 0s 726us/step - loss: 10.9452\n",
      "250/250 [==============================] - 0s 561us/step - loss: 0.4170\n",
      "250/250 [==============================] - 0s 690us/step - loss: 2.1973\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.1722\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.05865827575325966\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.2418\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0244\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.5855\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0261\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.7055\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0253\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.2015\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0321\n",
      "250/250 [==============================] - 1s 714us/step - loss: 0.1864\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0290\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.8673\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0792\n",
      "250/250 [==============================] - 0s 694us/step - loss: 6.2611\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.4693\n",
      "250/250 [==============================] - 0s 694us/step - loss: 1.6514\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.1097\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.3620\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0454\n",
      "250/250 [==============================] - 0s 662us/step - loss: 1.6782\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0817\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.2311\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0163\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0219\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0210\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 542us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0321\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0150\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0249\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1174\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0187\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.1890\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0164\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0871\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0255\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0429\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0201\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0808\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0365\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.2438\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0179\n",
      "250/250 [==============================] - 0s 658us/step - loss: 6.1610\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.1661\n",
      "250/250 [==============================] - 0s 702us/step - loss: 5.7560\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0788\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1749\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0155\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.1238\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0159\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.4645\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0193\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0760\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 670us/step - loss: 1.3535\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0503\n",
      "250/250 [==============================] - 0s 686us/step - loss: 2.0205\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0198\n",
      "250/250 [==============================] - 0s 686us/step - loss: 2.6861\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0618\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0234\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 702us/step - loss: 1.2279\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0363\n",
      "250/250 [==============================] - 0s 682us/step - loss: 71.1393\n",
      "250/250 [==============================] - 0s 538us/step - loss: 1.0866\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0168\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0210\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0141\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0147\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0223\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0239\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0137\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0421\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0217\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1107\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0184\n",
      "250/250 [==============================] - 0s 763us/step - loss: 1.7306\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0317\n",
      "250/250 [==============================] - 0s 690us/step - loss: 6.1204\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.1805\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.2870\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.0152\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0415\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.3070\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0119\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1734\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0099\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.4305\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0159\n",
      "250/250 [==============================] - 0s 698us/step - loss: 56.2035\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0193\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1656\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0240\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 773us/step - loss: 0.0152\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0250\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 686us/step - loss: 95.9954\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.8351\n",
      "250/250 [==============================] - 0s 674us/step - loss: 2.4090\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1861\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0147\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0138\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0187\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0270\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0182\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 682us/step - loss: 0.5517\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0170\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 570us/step - loss: 0.0141\n",
      "250/250 [==============================] - 0s 690us/step - loss: 24.1488\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.5255\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1871\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0359\n",
      "250/250 [==============================] - 0s 650us/step - loss: 23.9322\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.3493\n",
      "250/250 [==============================] - 0s 658us/step - loss: 1.2871\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0292\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.8805\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.2444\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 686us/step - loss: 2.0434\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0529\n",
      "250/250 [==============================] - 0s 674us/step - loss: 1.7319\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0402\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 526us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.3514\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0791\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0293\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0099\n",
      "250/250 [==============================] - 0s 682us/step - loss: 28.9103\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.2423\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0137\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0121\n",
      "250/250 [==============================] - 1s 686us/step - loss: 0.0183\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0218\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0333\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0380\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 4 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.5562\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0678\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 682us/step - loss: 3.0472\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0490\n",
      "250/250 [==============================] - 0s 674us/step - loss: 27.6535\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.3140\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.7255\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0431\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.4518\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0298\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.3550\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.2095\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 702us/step - loss: 1.0547\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0209\n",
      "250/250 [==============================] - 0s 690us/step - loss: 6.6893\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.1320\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0153\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0156\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0138\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0601\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0286\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0164\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0304\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0169\n",
      "250/250 [==============================] - 1s 706us/step - loss: 163.2249\n",
      "250/250 [==============================] - 0s 546us/step - loss: 2.8019\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 694us/step - loss: 4.1115\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0692\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0237\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0203\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1587\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.4858\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0181\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.8399\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0273\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0595\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1111\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0111\n",
      "250/250 [==============================] - 0s 702us/step - loss: 5.5186\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0368\n",
      "250/250 [==============================] - 0s 674us/step - loss: 2.7513\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.3464\n",
      "250/250 [==============================] - 0s 670us/step - loss: 65.3758\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.4447\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.8826\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0512\n",
      "250/250 [==============================] - 0s 690us/step - loss: 2.9602\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0276\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0154\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 670us/step - loss: 10.2893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 558us/step - loss: 0.1793\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0684\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0150\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 2.3750\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0634\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0169\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 686us/step - loss: 4.8089\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.4202\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0131\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0495\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0293\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0179\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0142\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.2675\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.4374\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0707\n",
      "250/250 [==============================] - 0s 674us/step - loss: 4.7185\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.4185\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1735\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.1225\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0168\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.1345\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0122\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.8807\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.2239\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.2843\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0188\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.4434\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0287\n",
      "250/250 [==============================] - 0s 686us/step - loss: 1.1856\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0235\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0205\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0320\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0172\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0149\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 694us/step - loss: 3.8846\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.5623\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0137\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.1857\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0885\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0320\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0235\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0155\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 7 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0472\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0102\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.8169\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0214\n",
      "250/250 [==============================] - 0s 658us/step - loss: 2.9514\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0392\n",
      "250/250 [==============================] - 0s 698us/step - loss: 4.7951\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0570\n",
      "250/250 [==============================] - 0s 690us/step - loss: 13.3420\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.6151\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.8460\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 883us/step - loss: 2.4218\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.1265\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.2921\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0125\n",
      "250/250 [==============================] - 0s 775us/step - loss: 3.8310\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0138\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.5376\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0207\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0147\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0094\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.2915\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0134\n",
      "250/250 [==============================] - 1s 702us/step - loss: 0.0286\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0141\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.3054\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0821\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0125\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0825\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0637\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0165\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0260\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0240\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0114\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 8 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 546us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.2383\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 710us/step - loss: 4.7626\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.1414\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0520\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 710us/step - loss: 1.0351\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0444\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.2163\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.9647\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0494\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.1592\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.5725\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0140\n",
      "250/250 [==============================] - 0s 650us/step - loss: 1.9032\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0278\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 694us/step - loss: 19.6663\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.1247\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0280\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0150\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0437\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0292\n",
      "250/250 [==============================] - 0s 654us/step - loss: 246.0526\n",
      "250/250 [==============================] - 0s 542us/step - loss: 3.1057\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0542\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0320\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0137\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0221\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0260\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0128\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.7055\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0652\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1237\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 706us/step - loss: 2.9624\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0548\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.1002\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0122\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.2382\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0287\n",
      "250/250 [==============================] - 0s 694us/step - loss: 2.9759\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.3554\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.9473\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0810\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.8267\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0263\n",
      "250/250 [==============================] - 1s 678us/step - loss: 0.7378\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.3963\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0232\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0485\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0158\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0150\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0290\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0142\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0316\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0195\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.2173\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0783\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0348\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0246\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0175\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0214\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0114\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "第 3 个CS算法开始\n",
      "250/250 [==============================] - 0s 839us/step - loss: 15.2717\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.2790\n",
      "250/250 [==============================] - 0s 714us/step - loss: 22.9355\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.1171\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.9958\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0678\n",
      "250/250 [==============================] - 0s 694us/step - loss: 1.0845\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0420\n",
      "250/250 [==============================] - 0s 690us/step - loss: 15.4528\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.3080\n",
      "250/250 [==============================] - 0s 678us/step - loss: 73.6776\n",
      "250/250 [==============================] - 0s 606us/step - loss: 1.6110\n",
      "250/250 [==============================] - 0s 694us/step - loss: 5.8000\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1316\n",
      "250/250 [==============================] - 0s 658us/step - loss: 13.8669\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.3998\n",
      "250/250 [==============================] - 0s 694us/step - loss: 2.4721\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.1131\n",
      "250/250 [==============================] - 0s 690us/step - loss: 15.7198\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0683\n",
      "250/250 [==============================] - 0s 674us/step - loss: 1.8385\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.1446\n",
      "250/250 [==============================] - 0s 682us/step - loss: 13.6185\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.2059\n",
      "250/250 [==============================] - 0s 694us/step - loss: 30.2790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 562us/step - loss: 0.2406\n",
      "250/250 [==============================] - 0s 694us/step - loss: 38.2997\n",
      "250/250 [==============================] - 0s 574us/step - loss: 1.1897\n",
      "250/250 [==============================] - 0s 678us/step - loss: 144.6886\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.3055\n",
      "250/250 [==============================] - 0s 682us/step - loss: 46.3590\n",
      "250/250 [==============================] - 0s 538us/step - loss: 1.9624\n",
      "250/250 [==============================] - 0s 662us/step - loss: 45.7934\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.1149\n",
      "250/250 [==============================] - 0s 690us/step - loss: 145.4694\n",
      "250/250 [==============================] - 0s 562us/step - loss: 1.1470\n",
      "250/250 [==============================] - 0s 682us/step - loss: 14.5478\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.5111\n",
      "250/250 [==============================] - 0s 686us/step - loss: 803.0474\n",
      "250/250 [==============================] - 0s 546us/step - loss: 3.4351\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.04195006564259529\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.3065\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0351\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.3153\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0320\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.2143\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0261\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.2355\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.0346\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 196.5195\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4872\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.2016\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0243\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.4477\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0451\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.1533\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0295\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.5004\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0554\n",
      "250/250 [==============================] - 0s 678us/step - loss: 2.3259\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0725\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0288\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0159\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0290\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0161\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0198\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0292\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0516\n",
      "250/250 [==============================] - 0s 565us/step - loss: 0.0147\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0218\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0146\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0337\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0187\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0251\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0148\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0430\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0137\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0554\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0244\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0331\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.8943\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1188\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0127\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0309\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0113\n",
      "250/250 [==============================] - 1s 690us/step - loss: 0.4226\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0219\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1647\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0111\n",
      "250/250 [==============================] - 0s 686us/step - loss: 7.1092\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0406\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.7086\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0326\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.5289\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0295\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.1075\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0250\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0127\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0158\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0356\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0111\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0169\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0122\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0156\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0152\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0152\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0175\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0122\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0262\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0175\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0643\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.7550\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 678us/step - loss: 0.0271\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0096\n",
      "250/250 [==============================] - 0s 674us/step - loss: 1.6096\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0193\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0272\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0146\n",
      "250/250 [==============================] - 0s 662us/step - loss: 3.8105\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0336\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.3482\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0401\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2538\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0137\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.6700\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0167\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 759us/step - loss: 85.1186\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.4911\n",
      "250/250 [==============================] - 0s 666us/step - loss: 6.7142\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1778\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0130\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 642us/step - loss: 5.9452\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0928\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0129\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0121\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0150\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0200\n",
      "250/250 [==============================] - 0s 568us/step - loss: 0.0129\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 650us/step - loss: 0.4349\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0125\n",
      "250/250 [==============================] - 0s 634us/step - loss: 1.7385\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1197\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.1237\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0210\n",
      "250/250 [==============================] - 0s 666us/step - loss: 1.6286\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0364\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.7992\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0348\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0894\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 662us/step - loss: 6.8083\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0657\n",
      "250/250 [==============================] - 0s 675us/step - loss: 0.1196\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0285\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.6181\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0436\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0802\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0199\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.6670\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0138\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 694us/step - loss: 20.9691\n",
      "250/250 [==============================] - 0s 553us/step - loss: 0.6557\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0119\n",
      "250/250 [==============================] - 0s 526us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.4728\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0168\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0235\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0185\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 4 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.5124\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0436\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1210\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0170\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.3075\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0156\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.1724\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 698us/step - loss: 7.3886\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.1605\n",
      "250/250 [==============================] - 0s 690us/step - loss: 1.1980\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.7597\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0170\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.1384\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0173\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.1396\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.1368\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0160\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0195\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 706us/step - loss: 0.0680\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.1067\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0233\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0099\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0092\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.8037\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0289\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0156\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0179\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0105\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 5 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.3429\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.4220\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 739us/step - loss: 1.1666\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0154\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.1291\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.9801\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0409\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.1620\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.2250\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0440\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1568\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0111\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.9031\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0149\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 23.9929\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.5403\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0207\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 686us/step - loss: 5.5535\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0448\n",
      "250/250 [==============================] - 0s 666us/step - loss: 10.2004\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.3205\n",
      "250/250 [==============================] - 1s 674us/step - loss: 6.3716\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.2311\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0176\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0103\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 694us/step - loss: 3.5910\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0679\n",
      "250/250 [==============================] - 0s 682us/step - loss: 7.7734\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.1228\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1844\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0540\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.5990\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0190\n",
      "250/250 [==============================] - 0s 718us/step - loss: 22.6556\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.1728\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.5929\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0392\n",
      "250/250 [==============================] - 0s 686us/step - loss: 2.2976\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0205\n",
      "250/250 [==============================] - 0s 690us/step - loss: 33.4198\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.1202\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0629\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0099\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0429\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0160\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0166\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0102\n",
      "250/250 [==============================] - 0s 678us/step - loss: 43.1366\n",
      "250/250 [==============================] - 0s 530us/step - loss: 1.0738\n",
      "250/250 [==============================] - 1s 674us/step - loss: 0.0426\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0145\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.5447\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0550\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0348\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0216\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0092\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0161\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0145\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0113\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0714\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0163\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.2853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 558us/step - loss: 0.0158\n",
      "250/250 [==============================] - 0s 694us/step - loss: 1.5531\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 670us/step - loss: 3.4370\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.2854\n",
      "250/250 [==============================] - 0s 678us/step - loss: 3.3613\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.1084\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.8874\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0489\n",
      "250/250 [==============================] - 0s 670us/step - loss: 1.6966\n",
      "250/250 [==============================] - 0s 522us/step - loss: 0.0688\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1388\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0219\n",
      "250/250 [==============================] - 0s 646us/step - loss: 1.6544\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.1734\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0126\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0158\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0148\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0107\n",
      "250/250 [==============================] - 1s 682us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0091\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2030\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0623\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0315\n",
      "250/250 [==============================] - 0s 526us/step - loss: 0.0314\n",
      "250/250 [==============================] - 0s 686us/step - loss: 16.3890\n",
      "250/250 [==============================] - 0s 558us/step - loss: 1.0275\n",
      "250/250 [==============================] - 0s 674us/step - loss: 1.4767\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0228\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0093\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0146\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0124\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0166\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 8 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.1952\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.3610\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.2559\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.7351\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0480\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.1148\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 694us/step - loss: 2.1921\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.1440\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1183\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.2593\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0175\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.1219\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0178\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.2072\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0098\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0132\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0163\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.0093\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0640\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0211\n",
      "250/250 [==============================] - 0s 690us/step - loss: 14.3160\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.2931\n",
      "250/250 [==============================] - 0s 738us/step - loss: 0.1254\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0537\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0102\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0121\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0154\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0098\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0822\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0129\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.4508\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0270\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 706us/step - loss: 3.8070\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0269\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.7385\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0477\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.5188\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0340\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.5719\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2009\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0145\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.7250\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0192\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3335\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0155\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0115\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 582us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 705us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0135\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0347\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0264\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0422\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0127\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0368\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0205\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0130\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0119\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0093\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "第 4 个CS算法开始\n",
      "250/250 [==============================] - 0s 670us/step - loss: 37.0402\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.1520\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.3881\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 698us/step - loss: 2.6883\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.1819\n",
      "250/250 [==============================] - 0s 662us/step - loss: 11.1890\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.5131\n",
      "250/250 [==============================] - 0s 787us/step - loss: 1.8472\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0602\n",
      "250/250 [==============================] - 0s 807us/step - loss: 2.9154\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0803\n",
      "250/250 [==============================] - 0s 682us/step - loss: 8.2901\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1433\n",
      "250/250 [==============================] - 0s 694us/step - loss: 27.3402\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.5967\n",
      "250/250 [==============================] - 1s 694us/step - loss: 34.0420\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.3190\n",
      "250/250 [==============================] - 0s 747us/step - loss: 7.8027\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.5266\n",
      "250/250 [==============================] - 0s 730us/step - loss: 11.4432\n",
      "250/250 [==============================] - 0s 526us/step - loss: 0.1522\n",
      "250/250 [==============================] - 0s 722us/step - loss: 55.1724\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.3783\n",
      "250/250 [==============================] - 0s 670us/step - loss: 1771.6532\n",
      "250/250 [==============================] - 0s 594us/step - loss: 5.6979\n",
      "250/250 [==============================] - 0s 690us/step - loss: 60.1143\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.3005\n",
      "250/250 [==============================] - 0s 682us/step - loss: 8.1068\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1386\n",
      "250/250 [==============================] - 0s 674us/step - loss: 6.6923\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1593\n",
      "250/250 [==============================] - 0s 682us/step - loss: 6.0007\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.1380\n",
      "250/250 [==============================] - 0s 662us/step - loss: 49.9772\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.7223\n",
      "250/250 [==============================] - 0s 682us/step - loss: 133.8486\n",
      "250/250 [==============================] - 0s 526us/step - loss: 0.7355\n",
      "250/250 [==============================] - 0s 686us/step - loss: 51.2098\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.5897\n",
      "\n",
      " BEST_LOSSNESS IS %.2f : \n",
      " 0.017709022387862206\n",
      "\n",
      "******************************************************第 1 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.1715\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.6340\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0244\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.3349\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0293\n",
      "250/250 [==============================] - 0s 678us/step - loss: 43.4279\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.5565\n",
      "250/250 [==============================] - 0s 730us/step - loss: 28.5456\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.2935\n",
      "250/250 [==============================] - 0s 662us/step - loss: 1.3467\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0731\n",
      "250/250 [==============================] - 1s 694us/step - loss: 0.6157\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0358\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.1886\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0464\n",
      "250/250 [==============================] - 0s 682us/step - loss: 7.6450\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0730\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.4888\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0113\n",
      "\n",
      "*****************************************************第 1 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0191\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0187\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0197\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0127\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0596\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0238\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0767\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0179\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0655\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0218\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0319\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0364\n",
      "250/250 [==============================] - 0s 686us/step - loss: 1.5156\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.1750\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0804\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0157\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0101\n",
      "\n",
      "*********************************************************第 1 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 1 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 2 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.3379\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0336\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.1084\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0976\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 666us/step - loss: 4.1496\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.1793\n",
      "250/250 [==============================] - 1s 682us/step - loss: 0.3190\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0133\n",
      "250/250 [==============================] - 0s 650us/step - loss: 5.1605\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.2494\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.2003\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0171\n",
      "250/250 [==============================] - 0s 714us/step - loss: 1.7859\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1322\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0383\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0167\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0110\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0103\n",
      "\n",
      "*****************************************************第 2 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0243\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0174\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0152\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0272\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0141\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0201\n",
      "250/250 [==============================] - 0s 534us/step - loss: 0.0192\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1133\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0449\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.6603\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1834\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0100\n",
      "\n",
      "*********************************************************第 2 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 2 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "\n",
      "******************************************************第 3 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0994\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0212\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0304\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.1584\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0220\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.3515\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0631\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0396\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.5509\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0362\n",
      "250/250 [==============================] - 0s 690us/step - loss: 294.3516\n",
      "250/250 [==============================] - 0s 538us/step - loss: 0.7912\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.2114\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0307\n",
      "250/250 [==============================] - 0s 686us/step - loss: 1.8752\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.1743\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1659\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0100\n",
      "\n",
      "*****************************************************第 3 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0094\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 879us/step - loss: 4.3645\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.2642\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0109\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0176\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 706us/step - loss: 2.6245\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.2245\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0232\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0132\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0172\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0301\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0153\n",
      "250/250 [==============================] - 0s 658us/step - loss: 26.2602\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.8280\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0099\n",
      "\n",
      "*********************************************************第 3 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 4 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0121\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0094\n",
      "250/250 [==============================] - 0s 693us/step - loss: 0.4773\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0559\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1438\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1465\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0158\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1337\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 658us/step - loss: 1.1530\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0605\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.3986\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.0177\n",
      "250/250 [==============================] - 0s 682us/step - loss: 66.0590\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.8367\n",
      "250/250 [==============================] - 0s 658us/step - loss: 1.0575\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.2695\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0106\n",
      "\n",
      "*****************************************************第 4 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0096\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0520\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0213\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 650us/step - loss: 0.0140\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0410\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0193\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0183\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0141\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0167\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.2352\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.1180\n",
      "250/250 [==============================] - 0s 670us/step - loss: 110.6766\n",
      "250/250 [==============================] - 0s 558us/step - loss: 4.2174\n",
      "\n",
      "*********************************************************第 4 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 5 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 650us/step - loss: 0.0754\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0523\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0796\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.1479\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0231\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.2190\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0126\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.3747\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0252\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.1354\n",
      "250/250 [==============================] - 0s 534us/step - loss: 0.0161\n",
      "250/250 [==============================] - 0s 670us/step - loss: 1.9360\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0388\n",
      "250/250 [==============================] - 0s 690us/step - loss: 57.5102\n",
      "250/250 [==============================] - 0s 558us/step - loss: 1.5105\n",
      "250/250 [==============================] - 0s 666us/step - loss: 19.7176\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.9404\n",
      "\n",
      "*****************************************************第 5 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.8387\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0982\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0158\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0113\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0141\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0178\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0192\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0149\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0154\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0152\n",
      "250/250 [==============================] - 1s 714us/step - loss: 0.1509\n",
      "250/250 [==============================] - 0s 530us/step - loss: 0.0949\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.5654\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.1609\n",
      "\n",
      "*********************************************************第 5 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 6 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.2460\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0368\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0459\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0122\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.4158\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 706us/step - loss: 2.7271\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0543\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0387\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.1080\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.4036\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0149\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.1798\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 714us/step - loss: 1.6535\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0695\n",
      "250/250 [==============================] - 0s 739us/step - loss: 3.3543\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.1853\n",
      "\n",
      "*****************************************************第 6 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0345\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0143\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0157\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0129\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0102\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0114\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 682us/step - loss: 23.0924\n",
      "250/250 [==============================] - 0s 566us/step - loss: 2.1456\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0203\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0114\n",
      "250/250 [==============================] - 1s 698us/step - loss: 0.0201\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0131\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0123\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0785\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0497\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.1697\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0685\n",
      "\n",
      "*********************************************************第 6 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 7 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.7896\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0432\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0336\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0162\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 554us/step - loss: 0.0104\n",
      "250/250 [==============================] - 0s 694us/step - loss: 1.5738\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0294\n",
      "250/250 [==============================] - 0s 710us/step - loss: 4.3767\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.8556\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.3575\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0167\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.2105\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0155\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.1472\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0146\n",
      "250/250 [==============================] - 0s 735us/step - loss: 9.5680\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0921\n",
      "250/250 [==============================] - 0s 666us/step - loss: 2.1734\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0858\n",
      "\n",
      "*****************************************************第 7 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0131\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0103\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.6895\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.3271\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0156\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0164\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0151\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0132\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0099\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0602\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0341\n",
      "250/250 [==============================] - 0s 690us/step - loss: 33.6420\n",
      "250/250 [==============================] - 0s 546us/step - loss: 1.3559\n",
      "\n",
      "*********************************************************第 7 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 8 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0285\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2760\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0257\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0223\n",
      "250/250 [==============================] - 0s 590us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0207\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 2.1288\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.2746\n",
      "250/250 [==============================] - 0s 686us/step - loss: 3.9811\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0331\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.5120\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0144\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.5091\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0139\n",
      "250/250 [==============================] - 0s 682us/step - loss: 1.4645\n",
      "250/250 [==============================] - 0s 602us/step - loss: 0.0673\n",
      "250/250 [==============================] - 0s 658us/step - loss: 2.3132\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.2504\n",
      "\n",
      "*****************************************************第 8 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0107\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0127\n",
      "250/250 [==============================] - 0s 598us/step - loss: 0.0105\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0098\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.2058\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.1211\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0176\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0183\n",
      "250/250 [==============================] - 0s 682us/step - loss: 97.5396\n",
      "250/250 [==============================] - 0s 594us/step - loss: 1.1832\n",
      "250/250 [==============================] - 0s 751us/step - loss: 1.2036\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0576\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0441\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0241\n",
      "250/250 [==============================] - 0s 695us/step - loss: 0.1907\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.1205\n",
      "\n",
      "*********************************************************第 8 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 9 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0193\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0101\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0344\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0118\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.1719\n",
      "250/250 [==============================] - 0s 594us/step - loss: 0.0211\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.1099\n",
      "250/250 [==============================] - 0s 586us/step - loss: 0.0127\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.6163\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0742\n",
      "250/250 [==============================] - 0s 702us/step - loss: 3.2745\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0285\n",
      "250/250 [==============================] - 0s 666us/step - loss: 391.2248\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.7845\n",
      "250/250 [==============================] - 0s 678us/step - loss: 13.6058\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.2715\n",
      "250/250 [==============================] - 0s 690us/step - loss: 9.3032\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0854\n",
      "250/250 [==============================] - 0s 706us/step - loss: 1.8733\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0520\n",
      "\n",
      "*****************************************************第 9 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0102\n",
      "250/250 [==============================] - 0s 582us/step - loss: 0.0099\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0121\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0099\n",
      "250/250 [==============================] - 1s 698us/step - loss: 1.8036\n",
      "250/250 [==============================] - 0s 554us/step - loss: 0.0840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 682us/step - loss: 0.0106\n",
      "250/250 [==============================] - 0s 542us/step - loss: 0.0097\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0701\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0389\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0136\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.0149\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.4131\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0912\n",
      "250/250 [==============================] - 0s 759us/step - loss: 29.8973\n",
      "250/250 [==============================] - 0s 546us/step - loss: 1.3700\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0499\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0204\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0842\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0338\n",
      "\n",
      "*********************************************************第 9 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******************************************************第 10 代开始迭代优化************************************************************\n",
      "\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0117\n",
      "250/250 [==============================] - 0s 578us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.1230\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0112\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.1028\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0165\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.2887\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0144\n",
      "250/250 [==============================] - 0s 646us/step - loss: 2.1934\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.2136\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.6769\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.0181\n",
      "250/250 [==============================] - 0s 735us/step - loss: 25.3885\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0237\n",
      "250/250 [==============================] - 0s 646us/step - loss: 2.6559\n",
      "250/250 [==============================] - 0s 570us/step - loss: 0.4283\n",
      "250/250 [==============================] - 0s 682us/step - loss: 34.9650\n",
      "250/250 [==============================] - 0s 562us/step - loss: 1.3815\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.3907\n",
      "250/250 [==============================] - 0s 558us/step - loss: 0.0272\n",
      "\n",
      "*****************************************************第 10 次迭代，计算适应度********************************************************\n",
      "\n",
      "250/250 [==============================] - 1s 690us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0128\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0116\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0323\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0108\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0095\n",
      "250/250 [==============================] - 0s 674us/step - loss: 2.8948\n",
      "250/250 [==============================] - 0s 566us/step - loss: 0.1554\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0200\n",
      "250/250 [==============================] - 0s 550us/step - loss: 0.0120\n",
      "250/250 [==============================] - 0s 662us/step - loss: 2.9263\n",
      "250/250 [==============================] - 0s 574us/step - loss: 0.0486\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.3878\n",
      "250/250 [==============================] - 0s 546us/step - loss: 0.1500\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0361\n",
      "250/250 [==============================] - 0s 534us/step - loss: 0.0245\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0490\n",
      "250/250 [==============================] - 0s 562us/step - loss: 0.0273\n",
      "\n",
      "*********************************************************第 10 次迭代结束************************************************************\n",
      "\n",
      "\n",
      "******\n",
      "\n",
      " 第 10 次迭代最优Loss是 0.01 : \n",
      "\n",
      "\n",
      "******\n",
      "\n",
      "第 1 个CS最优loss为:0.00915!\n",
      "第 2 个CS最优loss为:0.00938!\n",
      "第 3 个CS最优loss为:0.00912!\n",
      "第 4 个CS最优loss为:0.00949!\n"
     ]
    }
   ],
   "source": [
    "# if __name__=='__main__':\n",
    "low = -5*np.ones(numsum)\n",
    "upp = 5*np.ones(numsum)\n",
    "i = 0\n",
    "j = 0\n",
    "best_nest = [] #保存每次CS最佳loss对应的nest，用于赋值给不同神经网络构成多个弱分类器\n",
    "best_loss = []\n",
    "best_fitness = []\n",
    "alpha = [0.22,0.46,0.65,0.82]  #alpha不取0.5,0<alpha<1\n",
    "xn = [0.90,0.70,0.41,0.24]  #xn值不能和alpha值相同，否则将演化为周期系统，就不是混沌系统了\n",
    "for i in range(M):\n",
    "    print('第 %d 个CS算法开始'%(i+1))\n",
    "    nest,loss = cuckoo_search(10,numsum, low,upp, step_size = 0.4,alpha=alpha[i],xn=xn[i])\n",
    "    best_nest.append(nest)\n",
    "    best_loss.append(loss)\n",
    "\n",
    "for j in range(M):    \n",
    "    print('第 %d 个CS最优loss为:%.5f!'%(j+1,best_loss[j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009153966791927814,\n",
       " 0.009379342198371887,\n",
       " 0.009124599397182465,\n",
       " 0.009492403827607632]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "x = list(range(0,len(LossArr)))\n",
    "fig = sns.lineplot(x,LossArr)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"fitness\")\n",
    "scatter_fig = fig.get_figure()\n",
    "scatter_fig.savefig('./COLoss', dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 731us/step - loss: 0.0090\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0089\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0092\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0089\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0091\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0086\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 725us/step - loss: 0.0091\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0088\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0090\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 827us/step - loss: 0.0092\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0087\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0090\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0088\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0085\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0092\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0092\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0090\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0095\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0086\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0084\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 863us/step - loss: 0.0086\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0087\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.0095\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0089\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 803us/step - loss: 0.0091\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0092\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0092\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0087\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0089\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0088\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0086\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0087\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0092\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0087\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0086\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0090\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0093\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0087\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0089\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0086\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 807us/step - loss: 0.0091\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0087\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0085\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0089\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0085\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0087 0s - loss: 0.008\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0085\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0094\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0094\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0089\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0089\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 815us/step - loss: 0.0088\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0092\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0086\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.0088\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0086\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0087\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0092\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0088\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0093\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0090\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0086\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.009 - 0s 642us/step - loss: 0.0092\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0085\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0089\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0086\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0088\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0085\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0092\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0085\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0089\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0087\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0090\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0085\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0090\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0088\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0088\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0092\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0086\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0087\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0087\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0087\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0095\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0088\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0089\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 791us/step - loss: 0.0089\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.0090\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0087\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0085\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0087\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0088\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0089\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0092\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 795us/step - loss: 0.0089\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0085\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0086\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 702us/step - loss: 0.0093\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0089\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0090\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0089\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0150\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0148\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0144\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0130\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0130\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0119\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0126\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0116\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0105\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0112\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0110\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0106\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0097\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0102\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0104\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0099\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0096\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0098\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0100\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0091\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0098\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0096\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0097\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0089\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0096\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0093\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0093\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0089\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0088\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0093\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0093\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0095\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0088\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0091\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0096\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0096\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0094\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0091\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0086\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0091\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0092\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0091\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0087\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 755us/step - loss: 0.0092\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0092\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0092\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0096\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0097\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 706us/step - loss: 0.0091\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0091\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0093\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0092\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0093\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0092\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0091\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0090\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0089\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0091\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0093\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0089\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0091\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 747us/step - loss: 0.0091\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0091\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0086\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0093\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0093\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0093\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0092\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0089\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0089\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0088\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0092\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0091\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0091\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0095\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 685us/step - loss: 0.0089\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0091\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0093\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0092\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0091\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0088\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0089\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0084\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0095\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 787us/step - loss: 0.0088\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0086\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0087\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0092\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0087\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0088\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0094\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0091\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 650us/step - loss: 0.0089\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0089\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0092\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0088\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0087\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0088\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0090\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0090\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0094\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 695us/step - loss: 0.0088\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0091\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0090\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0091\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0088\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0090\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0086\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0090\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0087\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.0089\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 775us/step - loss: 0.0087\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0088\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0092\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0088\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0087\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0092\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0090\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0088\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0092\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0091\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0090\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0095\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0089\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 618us/step - loss: 0.0088\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0092\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0089 0s - loss: 0.0\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0091\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0085\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0091\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0094\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0092\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0091\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0086\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0089\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0090\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0091\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0087\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0091\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0090\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.0093\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 875us/step - loss: 0.0089\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0089\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0090\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0092\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0095\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0090\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0093\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0088\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0088\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0089\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0089\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0088\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0093\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0086\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0086\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0087\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0088\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0093\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 739us/step - loss: 0.0089\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0088\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 779us/step - loss: 0.0089\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0086\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0092\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0089\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0091\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 771us/step - loss: 0.0089\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0089\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0094\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0090\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0087\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0095\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0089\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0085\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0084\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0089\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0093\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0090\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0088\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0087\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0092\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0090\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0090\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0086\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0090\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0090\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0085\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0095\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 690us/step - loss: 0.0089\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0091\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0087\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0085\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0087\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0088\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0089\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0085\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0086\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0092\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 759us/step - loss: 0.0088\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 690us/step - loss: 0.0088\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0093\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0085\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0092\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0086\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0084\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0090\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0089\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 638us/step - loss: 0.0087\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 610us/step - loss: 0.0090\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 678us/step - loss: 0.0088\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 642us/step - loss: 0.0089\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0088\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.008 - 0s 666us/step - loss: 0.0088\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0088\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 666us/step - loss: 0.0089\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0087\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0085\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0084\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0085\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0085\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0086\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0085\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0083\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0092\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0092\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 710us/step - loss: 0.0087\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0086\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0085\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0092\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0085\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 650us/step - loss: 0.0090\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0091\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0086\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0088\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 630us/step - loss: 0.0087\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 634us/step - loss: 0.0088\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0086\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0087\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 626us/step - loss: 0.0090\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0091\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0088\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0083\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 662us/step - loss: 0.0084\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0086\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.0092\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0087\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0085A: 0s - loss:\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0083\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0083\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 879us/step - loss: 0.0088\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0089\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0086\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0089\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0088\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 751us/step - loss: 0.0086\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.0087\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0087\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0087\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 654us/step - loss: 0.0089\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 743us/step - loss: 0.0085\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0085\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 702us/step - loss: 0.0086\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0087\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 763us/step - loss: 0.0088\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.008 - 0s 698us/step - loss: 0.0086\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0090\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.008 - 0s 747us/step - loss: 0.0084\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 670us/step - loss: 0.0091\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 722us/step - loss: 0.0086\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 718us/step - loss: 0.0086\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 726us/step - loss: 0.0084\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 674us/step - loss: 0.0086\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 664us/step - loss: 0.0092\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 698us/step - loss: 0.0088\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 735us/step - loss: 0.0088\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0087\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0084\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 714us/step - loss: 0.0089\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.0087\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 767us/step - loss: 0.0090\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0084\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0089\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0086\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0083\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0086\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 783us/step - loss: 0.0085\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 871us/step - loss: 0.0086\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 694us/step - loss: 0.0085\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 658us/step - loss: 0.0088\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 622us/step - loss: 0.0087\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 686us/step - loss: 0.0085\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 646us/step - loss: 0.0089\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 614us/step - loss: 0.0087\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 682us/step - loss: 0.0092\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 606us/step - loss: 0.0089\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 730us/step - loss: 0.0091\n"
     ]
    }
   ],
   "source": [
    "Models = []\n",
    "for i in range(0,M):\n",
    "    chrom = best_nest[i]\n",
    "    w1 = chrom[:inputnum*hiddennum]\n",
    "    w1 = w1.reshape(inputnum,hiddennum)\n",
    "    b1 = chrom[inputnum*hiddennum:inputnum*hiddennum+hiddennum]\n",
    "    w2 = chrom[inputnum*hiddennum+hiddennum:inputnum*hiddennum+hiddennum+hiddennum*outputnum]\n",
    "    w2 = w2.reshape(hiddennum,outputnum)\n",
    "    b2 = chrom[inputnum*hiddennum+hiddennum+hiddennum*outputnum:]\n",
    "\n",
    "    WB_layer1 = (w1,b1)\n",
    "    WB_layer2 = (w2,b2)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(hiddnum1,name='layer1',activation='relu'),\n",
    "        #keras.layers.Dense(hiddnum2,activation='relu'),\n",
    "        #keras.layers.Dense(5,activation = 'relu'),\n",
    "        keras.layers.Dense(outputnum,name='layer2')\n",
    "        ])\n",
    "\n",
    "\n",
    "    model.build(input_shape=[None,inputnum])\n",
    "    #model.summary()\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.0005),\n",
    "                loss='mse',)\n",
    "\n",
    "    layer1 = model.get_layer('layer1')\n",
    "    layer2 = model.get_layer('layer2')\n",
    "    layer1.set_weights(WB_layer1)\n",
    "    layer2.set_weights(WB_layer2)\n",
    "\n",
    "\n",
    "    model.fit(COX,COY,epochs=100)  #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    Models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1000us/step - loss: 0.0959\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0992\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "6/6 [==============================] - 0s 999us/step - loss: 0.2958\n",
      "第 1 个模型的预测结果为：\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0087\n",
      "accuracy is : 0.00875\n",
      "\n",
      "第 2 个模型的预测结果为：\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "accuracy is : 0.00892\n",
      "\n",
      "第 3 个模型的预测结果为：\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "accuracy is : 0.00886\n",
      "\n",
      "第 4 个模型的预测结果为：\n",
      "\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "accuracy is : 0.00894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#将弱分类器按准确率从小到大排序\n",
    "CSMODEL = []\n",
    "acc = []\n",
    "for i in range(0,M):\n",
    "    accuracy = Models[i].evaluate(COTestX,COTestY)   #！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    acc.append(accuracy)\n",
    "    \n",
    "accIdx = np.argsort(acc)  #模型准确度从大到小排序，保证强分类器至少不会弱于最好的弱分类器\n",
    "#accIdx = accIdx[::-1]  #取消注释从大到小排序\n",
    "\n",
    "for i in accIdx:\n",
    "    CSMODEL.append(Models[i])\n",
    "\n",
    "for i in range(0,M):\n",
    "    print('第 %d 个模型的预测结果为：\\r\\n'%(i+1))\n",
    "    accuracy = CSMODEL[i].evaluate(COX,COY)   #！！！！！！！！！！！！！！！！！！！\n",
    " \n",
    "    print('accuracy is : %.5f\\r\\n'%(accuracy))\n",
    "Models = CSMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14770854 -0.11253625 -0.07659578 -0.09307647 -0.06314832 -0.03963506\n",
      " -0.02396938 -0.01655844 -0.03115997  0.00650108  0.03808141  0.08820231\n",
      "  0.07846224  0.11161655 -0.03661889 -0.05700341 -0.08629113 -0.07197994\n",
      " -0.10409594 -0.11768895]\n"
     ]
    }
   ],
   "source": [
    "pre = Models[0].predict(COTestX)  #！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "pre = np.squeeze(pre)\n",
    "print(pre[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01124212 -0.00570384 -0.00378263  0.00077003 -0.01351649 -0.00246888\n",
      " -0.01038435 -0.01038435 -0.02167106 -0.01218495 -0.00568697 -0.01891851\n",
      " -0.01989955 -0.01244664 -0.0159243  -0.00798237 -0.00888219 -0.02085203\n",
      " -0.02457434 -0.01075861]\n"
     ]
    }
   ],
   "source": [
    "pre = Models[0].predict(COX)  #！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "pre = np.squeeze(pre)\n",
    "print(pre[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08605729  0.08712309  0.06725578  0.05526296  0.03688053 -0.02330603\n",
      " -0.03151681  0.47532717  0.0380705   0.17029157  0.25136985  0.21336327\n",
      " -0.33866849 -0.40889218 -0.03081318  0.03048574  0.05642706  0.09975228\n",
      "  0.1016821   0.05746699]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(COTestY)[:20])#！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四问开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>BS</th>\n",
       "      <th>humi</th>\n",
       "      <th>w_speed</th>\n",
       "      <th>w_dir</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloud</th>\n",
       "      <th>high</th>\n",
       "      <th>press</th>\n",
       "      <th>GRTL</th>\n",
       "      <th>QRTL</th>\n",
       "      <th>CBFS</th>\n",
       "      <th>DBFS</th>\n",
       "      <th>sunFS</th>\n",
       "      <th>CO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>29.2888</td>\n",
       "      <td>303.284</td>\n",
       "      <td>0.018867</td>\n",
       "      <td>69.2701</td>\n",
       "      <td>3.56274</td>\n",
       "      <td>185.930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015980</td>\n",
       "      <td>668.990</td>\n",
       "      <td>100.693</td>\n",
       "      <td>15.33200</td>\n",
       "      <td>0.975764</td>\n",
       "      <td>425.464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29.0344</td>\n",
       "      <td>302.894</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>70.2078</td>\n",
       "      <td>3.72649</td>\n",
       "      <td>187.076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>583.845</td>\n",
       "      <td>100.676</td>\n",
       "      <td>13.18090</td>\n",
       "      <td>0.965678</td>\n",
       "      <td>426.919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28.7594</td>\n",
       "      <td>302.460</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>72.4598</td>\n",
       "      <td>3.44374</td>\n",
       "      <td>181.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022799</td>\n",
       "      <td>519.720</td>\n",
       "      <td>100.671</td>\n",
       "      <td>9.45429</td>\n",
       "      <td>0.761164</td>\n",
       "      <td>421.494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28.5811</td>\n",
       "      <td>302.177</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>74.5583</td>\n",
       "      <td>3.29525</td>\n",
       "      <td>183.259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026166</td>\n",
       "      <td>467.657</td>\n",
       "      <td>100.660</td>\n",
       "      <td>7.29238</td>\n",
       "      <td>0.649817</td>\n",
       "      <td>422.513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28.3925</td>\n",
       "      <td>301.869</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>76.5425</td>\n",
       "      <td>3.32493</td>\n",
       "      <td>184.781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031260</td>\n",
       "      <td>384.344</td>\n",
       "      <td>100.672</td>\n",
       "      <td>5.30148</td>\n",
       "      <td>0.537449</td>\n",
       "      <td>420.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temp_2m   temp_0        BS     humi  w_speed    w_dir  rain     cloud  \\\n",
       "0  29.2888  303.284  0.018867  69.2701  3.56274  185.930   0.0  0.015980   \n",
       "1  29.0344  302.894  0.018827  70.2078  3.72649  187.076   0.0  0.014324   \n",
       "2  28.7594  302.460  0.019103  72.4598  3.44374  181.234   0.0  0.022799   \n",
       "3  28.5811  302.177  0.019443  74.5583  3.29525  183.259   0.0  0.026166   \n",
       "4  28.3925  301.869  0.019725  76.5425  3.32493  184.781   0.0  0.031260   \n",
       "\n",
       "      high    press      GRTL      QRTL     CBFS  DBFS  sunFS        CO  \n",
       "0  668.990  100.693  15.33200  0.975764  425.464   0.0    0.0  0.311734  \n",
       "1  583.845  100.676  13.18090  0.965678  426.919   0.0    0.0  0.293062  \n",
       "2  519.720  100.671   9.45429  0.761164  421.494   0.0    0.0  0.292159  \n",
       "3  467.657  100.660   7.29238  0.649817  422.513   0.0    0.0  0.301029  \n",
       "4  384.344  100.672   5.30148  0.537449  420.900   0.0    0.0  0.275404  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！最后一个数字\n",
    "predata = pd.read_csv('preData.csv',sep=',',header=0,usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,20])\n",
    "predata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>humi</th>\n",
       "      <th>w_speed</th>\n",
       "      <th>rain</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>29.2888</td>\n",
       "      <td>303.284</td>\n",
       "      <td>69.2701</td>\n",
       "      <td>3.56274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>668.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29.0344</td>\n",
       "      <td>302.894</td>\n",
       "      <td>70.2078</td>\n",
       "      <td>3.72649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>583.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28.7594</td>\n",
       "      <td>302.460</td>\n",
       "      <td>72.4598</td>\n",
       "      <td>3.44374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>519.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28.5811</td>\n",
       "      <td>302.177</td>\n",
       "      <td>74.5583</td>\n",
       "      <td>3.29525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>467.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28.3925</td>\n",
       "      <td>301.869</td>\n",
       "      <td>76.5425</td>\n",
       "      <td>3.32493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>384.344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temp_2m   temp_0     humi  w_speed  rain     high\n",
       "0  29.2888  303.284  69.2701  3.56274   0.0  668.990\n",
       "1  29.0344  302.894  70.2078  3.72649   0.0  583.845\n",
       "2  28.7594  302.460  72.4598  3.44374   0.0  519.720\n",
       "3  28.5811  302.177  74.5583  3.29525   0.0  467.657\n",
       "4  28.3925  301.869  76.5425  3.32493   0.0  384.344"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preX = predata.iloc[:,[0,1,3,4,5,6,8,10,12]]    #！！！！！！！！！！！！！！！！！！！！！！！\n",
    "# SO2 = np.array(predata['SO2'])        \n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,5,6,8,12]] \n",
    "# NO2 = np.array(predata['NO2'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,4,6,8,10,12]] \n",
    "# PM10 = np.array(predata['PM10'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,5,6,12]] \n",
    "# PM2_5 = np.array(predata['PM2.5'])\n",
    "# preX.head()\n",
    "\n",
    "# preX = predata.iloc[:,[0,1,3,4,5,6,12]] \n",
    "# O3 = np.array(predata['O3'])\n",
    "# preX.head()\n",
    "\n",
    "preX = predata.iloc[:,[0,1,3,4,6,8]] \n",
    "CO = np.array(predata['CO'])\n",
    "preX.head()\n",
    "\n",
    "\n",
    "# SO2X = data.iloc[:7900,[6,7,9,10,11,12,14,16,18]] \n",
    "# SO2TestX = data.iloc[7900:,[6,7,9,10,11,12,14,16,18]]\n",
    "# NO2X = data.iloc[:7900,[6,7,9,10,11,12,14,18]]\n",
    "# NO2TestX =data.iloc[7900:,[6,7,9,10,11,12,14,18]]\n",
    "# PM10X =data.iloc[:7900,[6,7,10,12,14,16,18]]\n",
    "# PM10TestX =data.iloc[7900:,[6,7,10,12,14,16,18]]\n",
    "# PM2_5X =data.iloc[:7900,[6,7,9,10,11,12,18]]\n",
    "# PM2_5TestX =data.iloc[7900:,[6,7,9,10,11,12,18]]\n",
    "# O3X =data.iloc[:7900,[6,7,10,12,14,16,18]]\n",
    "# O3TestX =data.iloc[7900:,[6,7,10,12,14,16,18]]\n",
    "# COX =data.iloc[:7900,[6,7,9,10,12,14]]\n",
    "# COTestX =data.iloc[7900:,[6,7,9,10,12,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_2m</th>\n",
       "      <th>temp_0</th>\n",
       "      <th>humi</th>\n",
       "      <th>w_speed</th>\n",
       "      <th>rain</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.408770</td>\n",
       "      <td>-0.520980</td>\n",
       "      <td>0.671648</td>\n",
       "      <td>-0.003093</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>-0.140247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.458358</td>\n",
       "      <td>-0.551552</td>\n",
       "      <td>0.708481</td>\n",
       "      <td>0.047986</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>-0.202279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.511962</td>\n",
       "      <td>-0.585573</td>\n",
       "      <td>0.796940</td>\n",
       "      <td>-0.040212</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>-0.248998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.546717</td>\n",
       "      <td>-0.607757</td>\n",
       "      <td>0.879370</td>\n",
       "      <td>-0.086530</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>-0.286928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.583480</td>\n",
       "      <td>-0.631900</td>\n",
       "      <td>0.957310</td>\n",
       "      <td>-0.077272</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>-0.347626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.624168</td>\n",
       "      <td>0.608291</td>\n",
       "      <td>-0.245344</td>\n",
       "      <td>0.152207</td>\n",
       "      <td>0.097526</td>\n",
       "      <td>0.409437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.561910</td>\n",
       "      <td>0.341533</td>\n",
       "      <td>-0.173143</td>\n",
       "      <td>0.222574</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.382466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.433241</td>\n",
       "      <td>0.165236</td>\n",
       "      <td>-0.116281</td>\n",
       "      <td>-0.027463</td>\n",
       "      <td>0.378570</td>\n",
       "      <td>0.350439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.337533</td>\n",
       "      <td>-0.091330</td>\n",
       "      <td>-0.152639</td>\n",
       "      <td>0.564684</td>\n",
       "      <td>0.347059</td>\n",
       "      <td>0.247604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.123585</td>\n",
       "      <td>-0.234782</td>\n",
       "      <td>-0.067518</td>\n",
       "      <td>0.543036</td>\n",
       "      <td>-0.018699</td>\n",
       "      <td>0.009858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      temp_2m    temp_0      humi   w_speed      rain      high\n",
       "0   -0.408770 -0.520980  0.671648 -0.003093 -0.018699 -0.140247\n",
       "1   -0.458358 -0.551552  0.708481  0.047986 -0.018699 -0.202279\n",
       "2   -0.511962 -0.585573  0.796940 -0.040212 -0.018699 -0.248998\n",
       "3   -0.546717 -0.607757  0.879370 -0.086530 -0.018699 -0.286928\n",
       "4   -0.583480 -0.631900  0.957310 -0.077272 -0.018699 -0.347626\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "283  0.624168  0.608291 -0.245344  0.152207  0.097526  0.409437\n",
       "284  0.561910  0.341533 -0.173143  0.222574  0.019859  0.382466\n",
       "285  0.433241  0.165236 -0.116281 -0.027463  0.378570  0.350439\n",
       "286  0.337533 -0.091330 -0.152639  0.564684  0.347059  0.247604\n",
       "287  0.123585 -0.234782 -0.067518  0.543036 -0.018699  0.009858\n",
       "\n",
       "[288 rows x 6 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#零均值处理\n",
    "def data_progress(X):\n",
    "    for i in range(0,X.shape[1]):\n",
    "        X.iloc[:,i] -= np.mean(X,axis=0)[i]\n",
    "        X.iloc[:,i] /= np.max(np.abs(X),axis=0)[i]\n",
    "\n",
    "    return 0\n",
    "data_progress(preX)\n",
    "preX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_train = Models[0].predict(SO2X)       #！！！！！！！！！！！！！！！！！！\n",
    "# len(y_pred_train)\n",
    "# temp = np.array(abs(np.squeeze(y_pred_train) - SO2Y)) #！！！！！！！！！！！！！！！！！\n",
    "# len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 个模型的误差是：0.24\n",
      "第 1 个模型的误差是：0.34\n",
      "第 2 个模型的误差是：0.40\n",
      "第 3 个模型的误差是：0.45\n"
     ]
    }
   ],
   "source": [
    "y_pre_train = []\n",
    "# 初始化数据权重\n",
    "w_data = np.ones(n_train) / n_train\n",
    "# 初始化模型权重\n",
    "w_model = np.zeros(M)\n",
    "#train_y = tf.argmax(Y_onehot,axis=1)  #train_y保存了原始训练集中标签经过onehot编码后的结果\n",
    "#k = 3 #分类数量\n",
    "for i in range(M):\n",
    "    miss = []\n",
    "    #求第i个弱分类器训练集结果\n",
    "    y_pred_train = Models[i].predict(COX)  #三元概率   ！！！！！！！！！！！！！！！！！！！！！\n",
    "    y_pred_train = np.squeeze(y_pred_train)\n",
    "    temp = np.array(abs(y_pred_train - COY))  #！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    #y_pred_train = tf.argmax(y_pred_train,axis=1)#去最大概率的位置为预测值\n",
    "\n",
    "    #求第i个弱分类器误差\n",
    "    for k in range(0,len(y_pred_train)):\n",
    "        if temp[k] > 0.1:   #！！！！！！！！！！！！！！！！！！！！！！！！！！ 10倍\n",
    "            miss.append(1)\n",
    "        else:\n",
    "            miss.append(0)\n",
    "    #miss = [int(x) for x in range(0,len(y_pred_train)) (y_pred_train != train_y)]  #不相等则保存1，相等则保存0\n",
    "\n",
    "    error =np.dot(w_data, miss) #累加识别错误的样本权重，得到分类器误差\n",
    "    print('第 %d 个模型的误差是：%.2f'%(i,error))\n",
    "    #求第i个弱分类器权值，保存到w_model中\n",
    "    #a = 1/2 * log(1-e/e) + log(k-1),当k = 2 时为二分类更新权值公式不用修改，否则为多分类，算法准确率大于1/k即可\n",
    "    #该函数若准确率大于0.5（1-error）则值为正，否则为负值，越大说明模型分类越好\n",
    "    w_model[i] = 0.5 * np.log((1-error)/error) #+ np.log(k - 1)  \n",
    "    \n",
    "    # 更新数据权重 \n",
    "    #分类结果和真实的结果一致，那么结果是−w_model[m]，是一个负值，\n",
    "    #那么exp(-w_model[m]*train_y[i]*y_pred_train[i]) 结果小于1。也就是说该数据集的样本权重降低。否则该数据样本的权重增高。\n",
    "    #通过这种计算就可以让那些容易分错的样本的权重升高，容易分对的样本权重降低。继续迭代就会导致对难分的样本能分对的模型的权重上涨。\n",
    "    #最终，达到一个强分类器的目的。\n",
    "    #注意，这里只适合二分类【1，-1】\n",
    "    #多分类公式修改 wt = wt-1 * exp(at * (y_true!=y_pred)) \n",
    "    miss1 = np.array(miss)\n",
    "    miss1 = w_model[i]*miss1\n",
    "    for j in range(n_train):\n",
    "        w_data[j] = w_data[j] * np.exp(miss1[j])  #*train_y[i]*y_pred_train[i] #二分类时用这个\n",
    "    \n",
    "    #正则化数据权值\n",
    "    Z = np.sum(w_data)\n",
    "    for j in range(n_train):\n",
    "        w_data[j] /= Z\n",
    "\n",
    "#结果这个模块以后将得到每个模型的权值，保存在w_model中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4807478, 0.2664828, 0.1659931, 0.0867763])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#弱分类器权值归一化\n",
    "w_model = np.array(w_model / np.sum(w_model))\n",
    "w_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08250022,  0.08645892,  0.10790467, ...,  0.08255142,\n",
       "         0.09326959, -0.0051859 ],\n",
       "       [ 0.10105741,  0.08951402,  0.12675339, ...,  0.03912383,\n",
       "         0.04952657, -0.00444472],\n",
       "       [-0.00310087, -0.00373697,  0.04094768, ...,  0.00784969,\n",
       "        -0.0022049 , -0.00838351],\n",
       "       [ 0.03403616,  0.03602433,  0.04278827, ..., -0.45337248,\n",
       "        -0.52279925, -0.07016611]], dtype=float32)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_pre_ds = np.array([Models[m].predict(preX) for m in range(M)])\n",
    "models_pre_ds = np.squeeze(models_pre_ds)\n",
    "models_pre_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 288)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_pre_ds.shape  #4个模型得到的结果为4,288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = w_model[0]*models_pre_ds[0]\n",
    "for i in range(1,M):\n",
    "    result1 += w_model[i] * models_pre_ds[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测到数据后反归一化\n",
    "pre_four = result1.copy()\n",
    "pre_four = np.squeeze(pre_four)\n",
    "pre_four = np.array(pre_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_four = pre_four*COmax      #！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "pre_four = pre_four+COmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#添加一次预测结果，得到最终结果\n",
    "result = pre_four + CO  #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "result1 = result[0:24]\n",
    "result2 = result[24:48]\n",
    "result3 = result[48:72]\n",
    "result4 = result[72:96]\n",
    "result5 = result[96:120]\n",
    "result6 = result[120:144]\n",
    "result7 = result[144:168]\n",
    "result8 = result[168:192]\n",
    "result9 = result[192:216]\n",
    "result10 = result[216:240]\n",
    "result11 = result[240:264]\n",
    "result12 = result[264:]\n",
    "len(result1),len(result2),len(result3),len(result4),len(result5),len(result6),len(result7),len(result8),len(result9),len(result10),len(result11),len(result12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38076466, 0.36098663, 0.38832157, 0.41650182, 0.39947067,\n",
       "       0.37440705, 0.40671382, 0.38020724, 0.31541508, 0.30926008,\n",
       "       0.23466842, 0.16703126, 0.28113074, 0.20918232, 0.1463911 ,\n",
       "       0.16048486, 0.12547045, 0.0057676 , 0.07146281, 0.06527643,\n",
       "       0.14859846, 0.22207444])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去负值，对最终预测数据进行处理\n",
    "index1 = []\n",
    "index2 = []\n",
    "index3 = []\n",
    "index4 = []\n",
    "index5 = []\n",
    "index6 = []\n",
    "index7 = []\n",
    "index8 = []\n",
    "index9 = []\n",
    "index10 = []\n",
    "index11 = []\n",
    "index12 = []\n",
    "for i in range(0,len(result1)):\n",
    "    if result1[i] <= 0 :\n",
    "        index1.append(i)\n",
    "for i in range(0,len(result2)):\n",
    "    if result2[i] <= 0 :\n",
    "        index2.append(i)\n",
    "for i in range(0,len(result3)):\n",
    "    if result3[i] <= 0 :\n",
    "        index3.append(i)\n",
    "for i in range(0,len(result4)):\n",
    "    if result4[i] <= 0 :\n",
    "        index4.append(i)\n",
    "for i in range(0,len(result5)):\n",
    "    if result5[i] <= 0 :\n",
    "        index5.append(i)\n",
    "for i in range(0,len(result6)):\n",
    "    if result6[i] <= 0 :\n",
    "        index6.append(i)\n",
    "for i in range(0,len(result7)):\n",
    "    if result7[i] <= 0 :\n",
    "        index7.append(i)\n",
    "for i in range(0,len(result8)):\n",
    "    if result8[i] <= 0 :\n",
    "        index8.append(i)\n",
    "for i in range(0,len(result9)):\n",
    "    if result9[i] <= 0 :\n",
    "        index9.append(i)\n",
    "for i in range(0,len(result10)):\n",
    "    if result10[i] <= 0 :\n",
    "        index10.append(i)\n",
    "for i in range(0,len(result11)):\n",
    "    if result11[i] <= 0 :\n",
    "        index11.append(i)\n",
    "for i in range(0,len(result12)):\n",
    "    if result12[i] <= 0 :\n",
    "        index12.append(i)\n",
    "\n",
    "result1 = np.delete(result1, index1)\n",
    "result2 = np.delete(result2, index2)\n",
    "result3 = np.delete(result3,index3)\n",
    "result4 = np.delete(result4,index4)\n",
    "result5 = np.delete(result5,index5)\n",
    "result6 = np.delete(result6,index6)\n",
    "result7 = np.delete(result7, index7)\n",
    "result8 = np.delete(result8, index8)\n",
    "result9 = np.delete(result9,index9)\n",
    "result10 = np.delete(result10,index10)\n",
    "result11 = np.delete(result11,index11)\n",
    "result12 = np.delete(result12,index12)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #去除3个最大最小值\n",
    "\n",
    "result1 = np.sort(result1)[3:-3]\n",
    "result2 = np.sort(result2)[3:-3]\n",
    "result3 = np.sort(result3)[3:-3]\n",
    "result4 = np.sort(result4)[3:-3]\n",
    "result5 = np.sort(result5)[3:-3]\n",
    "result6 = np.sort(result6)[3:-3]\n",
    "result7 = np.sort(result7)[3:-3]\n",
    "result8 = np.sort(result8)[3:-3]\n",
    "result9 = np.sort(result9)[3:-3]\n",
    "result10 = np.sort(result10)[3:-3]\n",
    "result11 = np.sort(result11)[3:-3]\n",
    "result12 = np.sort(result12)[3:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 18, 18, 13, 17, 17, 14, 18, 18, 17, 18, 18)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result1),len(result2),len(result3),len(result4),len(result5),len(result6),len(result7),len(result8),len(result9),len(result10),len(result11),len(result12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.262774647243645, 0.2397340638278988, 0.3011235684811605)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result1)/len(result1),sum(result2)/len(result2),sum(result3)/len(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1566768095512482, 0.15220374159089256, 0.22283635961128684)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result4)/len(result4),sum(result5)/len(result5),sum(result6)/len(result6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1767446566340753, 0.1868472521765265, 0.24974752671153677)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result7)/len(result7),sum(result8)/len(result8),sum(result9)/len(result9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2543463481168396, 0.24592837615107005, 0.33233820744997267)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result10)/len(result10),sum(result11)/len(result11),sum(result12)/len(result12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
